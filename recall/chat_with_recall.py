import torch
import torch.nn.functional as F
import os
import numpy as np
from modelscope import AutoModelForCausalLM, AutoTokenizer
from model_utils import forward_backbone, ensure_last_hidden_state, build_causal_lm_output

class MemoryVectorDB:
    """è®°å¿†å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨PyTorchå®ç°ï¼ˆæ— éœ€faissï¼‰"""
    
    def __init__(self, embedding_dim=4096, device="cpu"):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        self.embedding_dim = embedding_dim
        self.embeddings = None  # å­˜å‚¨æ‰€æœ‰å‘é‡çš„tensor
        self.texts = []
        self.device = device  # æ·»åŠ è®¾å¤‡å±æ€§
        
    def add_vectors(self, embeddings, texts=None):
        """æ·»åŠ å‘é‡åˆ°æ•°æ®åº“"""
        if isinstance(embeddings, np.ndarray):
            embeddings = torch.from_numpy(embeddings)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºbfloat16å¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡ä¸Š
        embeddings = embeddings.to(dtype=torch.bfloat16, device=self.device)
        
        # å½’ä¸€åŒ–å‘é‡ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦
        embeddings = F.normalize(embeddings, p=2, dim=-1)
        
        if self.embeddings is None:
            self.embeddings = embeddings
        else:
            self.embeddings = torch.cat([self.embeddings, embeddings], dim=0)
        
        if texts:
            self.texts.extend(texts)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æ–‡æœ¬ï¼Œä½¿ç”¨å ä½ç¬¦
            self.texts.extend([f"Memory_{i}" for i in range(len(self.texts), len(self.texts) + embeddings.shape[0])])
        
        print(f"å‘é‡æ•°æ®åº“ç°æœ‰ {len(self.texts)} æ¡è®°å¿†")
    
    def search(self, query_embedding, top_k=5, debug=False):
        """æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡ï¼Œå¢å¼ºç‰ˆ"""
        if self.embeddings is None or len(self.embeddings) == 0:
            return []
        
        # ç¡®ä¿æŸ¥è¯¢å‘é‡å’Œå­˜å‚¨å‘é‡åœ¨åŒä¸€è®¾å¤‡ä¸Šä¸”ä¸ºç›¸åŒæ•°æ®ç±»å‹
        if isinstance(query_embedding, np.ndarray):
            query_embedding = torch.from_numpy(query_embedding)
        
        original_device = query_embedding.device
        original_dtype = query_embedding.dtype
        
        if debug:
            print(f"\n[è°ƒè¯•] æŸ¥è¯¢å‘é‡åŸå§‹ä¿¡æ¯:")
            print(f"  - è®¾å¤‡: {original_device}")
            print(f"  - æ•°æ®ç±»å‹: {original_dtype}")
            print(f"  - ç»´åº¦: {query_embedding.shape}")
            print(f"  - èŒƒæ•°: {torch.norm(query_embedding).item():.4f}")
            print(f"  - å‡å€¼: {torch.mean(query_embedding).item():.4f}")
            print(f"  - æ ‡å‡†å·®: {torch.std(query_embedding).item():.4f}")
            print(f"  - æœ€å¤§å€¼: {torch.max(query_embedding).item():.4f}")
            print(f"  - æœ€å°å€¼: {torch.min(query_embedding).item():.4f}")
        
        # ç§»åŠ¨æŸ¥è¯¢å‘é‡åˆ°ä¸å­˜å‚¨å‘é‡ç›¸åŒçš„è®¾å¤‡ä¸Š
        query_embedding = query_embedding.to(dtype=torch.bfloat16, device=self.device)
        
        # ç¡®ä¿æŸ¥è¯¢å‘é‡æœ‰æ­£ç¡®çš„ç»´åº¦
        if query_embedding.dim() == 1:
            # å¦‚æœæ˜¯å•ä¸ªå‘é‡ [embed_dim]ï¼Œæ·»åŠ æ‰¹æ¬¡ç»´åº¦
            query_embedding = query_embedding.unsqueeze(0)  # [1, embed_dim]
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        query_embedding_normalized = F.normalize(query_embedding, p=2, dim=-1)
        
        if debug:
            print(f"\n[è°ƒè¯•] å½’ä¸€åŒ–åæŸ¥è¯¢å‘é‡ä¿¡æ¯:")
            print(f"  - èŒƒæ•°: {torch.norm(query_embedding_normalized).item():.4f}")
            print(f"  - å‡å€¼: {torch.mean(query_embedding_normalized).item():.4f}")
            print(f"  - æ ‡å‡†å·®: {torch.std(query_embedding_normalized).item():.4f}")
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = torch.matmul(query_embedding_normalized, self.embeddings.t())
        
        if debug:
            # æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†å¸ƒä¿¡æ¯
            sim_mean = torch.mean(similarities).item()
            sim_std = torch.std(similarities).item()
            sim_max = torch.max(similarities).item()
            sim_min = torch.min(similarities).item()
            print(f"\n[è°ƒè¯•] ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
            print(f"  - å¹³å‡ç›¸ä¼¼åº¦: {sim_mean:.4f}")
            print(f"  - æ ‡å‡†å·®: {sim_std:.4f}")
            print(f"  - æœ€å¤§ç›¸ä¼¼åº¦: {sim_max:.4f}")
            print(f"  - æœ€å°ç›¸ä¼¼åº¦: {sim_min:.4f}")
            
            # è®¡ç®—ç›¸ä¼¼åº¦ç›´æ–¹å›¾ - ä¿®å¤ bfloat16 è½¬ numpy çš„é—®é¢˜
            sim_flat = similarities.flatten().cpu().to(torch.float32).numpy()  # å…ˆè½¬æ¢ä¸º float32
            hist_counts = np.histogram(sim_flat, bins=10, range=(float(sim_min), float(sim_max)))[0]
            hist_edges = np.linspace(float(sim_min), float(sim_max), 11)
            print(f"\n[è°ƒè¯•] ç›¸ä¼¼åº¦ç›´æ–¹å›¾:")
            for i in range(10):
                bar_len = int(hist_counts[i] / len(sim_flat) * 50)
                print(f"  {hist_edges[i]:.2f}-{hist_edges[i+1]:.2f}: {'#' * bar_len} ({hist_counts[i]})")
        
        # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
        top_k = min(top_k, len(self.embeddings))
        top_scores, top_indices = torch.topk(similarities, top_k, largest=True)
        
        # å¤„ç†ç»´åº¦ï¼Œç¡®ä¿ç»“æœå¯è¿­ä»£
        if top_scores.dim() == 1:
            top_scores = top_scores.unsqueeze(0)
            top_indices = top_indices.unsqueeze(0)
        
        results = []
        for i, (score, idx) in enumerate(zip(top_scores[0], top_indices[0])):
            memory_text = self.texts[idx.item()] if idx.item() < len(self.texts) else "Unknown memory"
            preview_text = memory_text[:100] + "..." if len(memory_text) > 100 else memory_text
            
            result = {
                'text': memory_text,
                'preview': preview_text,
                'embedding': self.embeddings[idx.item()].clone(),
                'score': float(score.item()),
                'index': int(idx.item())
            }
            results.append(result)
            
            if debug:
                print(f"\n[è°ƒè¯•] åŒ¹é…ç»“æœ #{i+1}:")
                print(f"  - ç›¸ä¼¼åº¦: {score.item():.4f}")
                print(f"  - ç´¢å¼•: {idx.item()}")
                print(f"  - é¢„è§ˆ: {preview_text}")
        
        return results
    
    def load_from_pt(self, pt_file_path):
        """ä».ptæ–‡ä»¶åŠ è½½å‘é‡æ•°æ®"""
        print(f"ä» {pt_file_path} åŠ è½½è®°å¿†æ•°æ®...")
        data = torch.load(pt_file_path, map_location='cpu')
        
        if isinstance(data, dict):
            if 'embeddings' in data and 'texts' in data:
                embeddings = data['embeddings']
                texts = data['texts']
            else:
                # å°è¯•æ¨æ–­é”®å
                embedding_keys = [k for k in data.keys() if 'embed' in k.lower()]
                text_keys = [k for k in data.keys() if 'text' in k.lower()]
                
                if embedding_keys and text_keys:
                    embeddings = data[embedding_keys[0]]
                    texts = data[text_keys[0]]
                else:
                    raise ValueError(f"æ— æ³•ä»æ•°æ®ä¸­è¯†åˆ«åµŒå…¥å‘é‡å’Œæ–‡æœ¬å­—æ®µ: {list(data.keys())}")
        else:
            # å‡è®¾æ˜¯ç›´æ¥çš„åµŒå…¥å‘é‡
            embeddings = data
            texts = [f"Memory_{i}" for i in range(embeddings.shape[0])]
        
        self.add_vectors(embeddings, texts)
        print(f"æˆåŠŸåŠ è½½ {len(texts)} æ¡è®°å¿†")


class MemoryRecallChat:
    """å…·æœ‰è®°å¿†å›æº¯åŠŸèƒ½çš„èŠå¤©æ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, model_name, memory_path=None, device=None):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.model_name = model_name
        
        # è®¾ç½®è®¾å¤‡
        if device is None:
            self.device = "auto"
        else:
            self.device = device
            
        print(f"ğŸ¤– åˆå§‹åŒ–è®°å¿†å›æº¯èŠå¤©æ¨¡å‹...")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.conversation_history = []
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()
        
        # æ£€æŸ¥ç‰¹æ®Štoken
        self._check_special_tokens()
        
        # åŠ è½½è®°å¿†æ•°æ® - ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
        self.memory_db = MemoryVectorDB(device=self.actual_device)  # ä¼ å…¥æ¨¡å‹çš„å®é™…è®¾å¤‡
        if memory_path:
            self.memory_db.load_from_pt(memory_path)
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map=self.device,
            trust_remote_code=True
        )
        
        # è·å–æ¨¡å‹å®é™…è®¾å¤‡
        self.actual_device = next(self.model.parameters()).device
        print(f"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.actual_device}")

    def _forward_with_backbone(self, **forward_inputs):
        local_inputs = dict(forward_inputs)
        use_cache_flag = local_inputs.pop("use_cache", True)
        backbone_outputs = forward_backbone(
            self.model,
            use_cache=use_cache_flag,
            output_hidden_states=False,
            return_dict=True,
            **local_inputs,
        )
        outputs = build_causal_lm_output(self.model, backbone_outputs)
        outputs.last_hidden_state = ensure_last_hidden_state(backbone_outputs)
        return outputs
    
    def _check_special_tokens(self):
        """æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦å­˜åœ¨"""
        self.special_tokens = {
            'recall_start': '<recall>',
            'recall': '<|recall|>',  # è¿™ä¸ªtokenå¯èƒ½ä»ç„¶å­˜åœ¨ï¼Œä¿ç•™
            'recall_end': '</recall>'
        }
        
        self.special_token_ids = {}
        missing_tokens = []
        
        for name, token in self.special_tokens.items():
            token_id = self.tokenizer.convert_tokens_to_ids(token)
            if token_id == self.tokenizer.unk_token_id:
                missing_tokens.append(token)
            else:
                self.special_token_ids[name] = token_id
                print(f"æ‰¾åˆ°ç‰¹æ®Štoken: {token} (ID: {token_id})")
        
        if missing_tokens:
            raise ValueError(f"ä»¥ä¸‹ç‰¹æ®Štokenä¸å­˜åœ¨: {missing_tokens}")
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.conversation_history = []
        print("å¯¹è¯å†å²å·²é‡ç½®")
    
    def chat(self, user_message, system_prompt=None, max_new_tokens=2000, temperature=0.7, top_p=0.9, stream=True, force_recall=False):
        """ä¼˜åŒ–çš„è®°å¿†å›æº¯èŠå¤© - ä¿ç•™KVç¼“å­˜ï¼Œæ”¯æŒå¯¹è¯å†å²
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            force_recall: æ˜¯å¦å¼ºåˆ¶ç¬¬ä¸€ä¸ªtokenä¸º<|recall_start|>
        """
        
        # 1. å¤„ç†ç³»ç»Ÿæç¤ºå’Œå¯¹è¯å†å²
        # å¦‚æœæ˜¯æ–°å¯¹è¯ï¼ˆæ²¡æœ‰å†å²ï¼‰ä¸”æä¾›äº†ç³»ç»Ÿæç¤ºï¼Œåˆ™æ·»åŠ ç³»ç»Ÿæç¤º
        if not self.conversation_history and system_prompt:
            self.conversation_history.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        self.conversation_history.append({"role": "user", "content": user_message})
        
        # 2. ä½¿ç”¨åˆ†è¯å™¨çš„chatæ¨¡æ¿åº”ç”¨æ•´ä¸ªå¯¹è¯å†å²
        chat_text = self.tokenizer.apply_chat_template(
            self.conversation_history,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # 3. ç¼–ç è¾“å…¥
        inputs = self.tokenizer(chat_text, return_tensors="pt").to(self.actual_device)
        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask
        
        print(f"\nğŸ§  å¼€å§‹ç”Ÿæˆå›ç­”..." + (" (å¼ºåˆ¶ä»¥å›å¿†å¼€å§‹)" if force_recall else ""))
        
        # 4. ç”Ÿæˆç»“æœè®°å½•
        generated_ids = []
        past_key_values = None
        in_recall_mode = False
        
        # 5. ç”Ÿæˆå¾ªç¯
        for i in range(max_new_tokens):
            # ç¡®å®šå½“å‰å¤„ç†çš„token
            current_input = input_ids[:, -1:] if past_key_values is not None else input_ids
            
            # æ¨¡å‹å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self._forward_with_backbone(
                    input_ids=current_input,
                    attention_mask=attention_mask,
                    past_key_values=past_key_values,
                    use_cache=True,
                )
            
            # æ›´æ–°KVç¼“å­˜
            past_key_values = outputs.past_key_values
            
            # è·å–é¢„æµ‹ç»“æœ
            logits = outputs.logits[:, -1, :]
            
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªtokenä¸”å¯ç”¨äº†å¼ºåˆ¶å›å¿†ï¼Œåˆ™å°†<|recall_start|>çš„æ¦‚ç‡è®¾ä¸ºæœ€é«˜
            if len(generated_ids) == 0 and force_recall:
                recall_start_id = self.special_token_ids.get('recall_start')
                # åˆ›å»ºä¸€ä¸ªæ–°çš„logits tensorï¼Œå°†æ‰€æœ‰å€¼è®¾ä¸ºä¸€ä¸ªéå¸¸å°çš„å€¼
                new_logits = torch.full_like(logits, -10000.0)
                # å°†<|recall_start|>çš„logitè®¾ä¸ºä¸€ä¸ªå¾ˆå¤§çš„å€¼
                new_logits[0, recall_start_id] = 10000.0
                logits = new_logits
                print("å¼ºåˆ¶ç”Ÿæˆ<|recall_start|>ä½œä¸ºç¬¬ä¸€ä¸ªtoken")
            
            # æ ¹æ®æ˜¯å¦åœ¨å›å¿†æ¨¡å¼é€‰æ‹©è§£ç ç­–ç•¥
            if in_recall_mode:
                # å›å¿†æ¨¡å¼ä½¿ç”¨è´ªå©ªè§£ç 
                next_token_id = torch.argmax(logits, dim=-1).item()
            else:
                # æ­£å¸¸æ¨¡å¼ä½¿ç”¨æ¸©åº¦é‡‡æ ·
                if temperature > 0:
                    logits = logits / temperature
                    
                    if top_p < 1.0:
                        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                        
                        # ç§»é™¤ä½æ¦‚ç‡token
                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                        sorted_indices_to_remove[..., 0] = 0
                        
                        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                        logits[indices_to_remove] = -float('inf')
                    
                    # é‡‡æ ·
                    probs = F.softmax(logits, dim=-1)
                    next_token_id = torch.multinomial(probs, num_samples=1).item()
                else:
                    # è´ªå©ªè§£ç 
                    next_token_id = torch.argmax(logits, dim=-1).item()
            
            # åˆ¤æ–­æ˜¯å¦ç”Ÿæˆäº†<|recall_start|>
            if next_token_id == self.special_token_ids.get('recall_start') and not in_recall_mode:
                # è¿›å…¥è®°å¿†å›æº¯æ¨¡å¼
                in_recall_mode = True
                
                # 1. å…ˆå°†<|recall_start|>æ·»åŠ åˆ°ç”Ÿæˆç»“æœ
                generated_ids.append(next_token_id)
                
                # æµå¼è¾“å‡ºå½“å‰token
                if stream:
                    token_text = self.tokenizer.decode([next_token_id])
                    print(token_text, end="", flush=True)
                
                # 2. å°†<|recall_start|>è¾“å…¥æ¨¡å‹è·å–éšè—çŠ¶æ€
                recall_start_input = torch.tensor([[next_token_id]], device=self.actual_device)
                attention_mask = torch.cat([
                    attention_mask, 
                    torch.ones(1, 1, device=self.actual_device)
                ], dim=1)
                
                # ä½¿ç”¨KVç¼“å­˜è¿›è¡Œæœ‰æ•ˆè®¡ç®—
                with torch.no_grad():
                    recall_outputs = self._forward_with_backbone(
                        input_ids=recall_start_input,
                        attention_mask=attention_mask[:, -1:],
                        past_key_values=past_key_values,
                        use_cache=True,
                    )
                
                # 3. è·å–<|recall_start|>çš„éšè—çŠ¶æ€ä½œä¸ºæŸ¥è¯¢å‘é‡
                query_vector = recall_outputs.last_hidden_state[0, -1]
                
                # 4. æ›´æ–°KVç¼“å­˜
                past_key_values = recall_outputs.past_key_values
                
                # 5. ä½¿ç”¨æŸ¥è¯¢å‘é‡åœ¨è®°å¿†æ•°æ®åº“ä¸­æ£€ç´¢
                if len(self.memory_db.texts) > 0:
                    # å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè¿”å›æ›´å¤šç»“æœ
                    query_vector = query_vector.to(dtype=torch.bfloat16)
                    
                    print("\n[ç³»ç»Ÿ: å¼€å§‹è®°å¿†æ£€ç´¢...]")
                    search_results = self.memory_db.search(query_vector, top_k=5, debug=True)
                    
                    if search_results:
                        print("\n[ç³»ç»Ÿ: æ‰¾åˆ°ä»¥ä¸‹è®°å¿†åŒ¹é…ç»“æœ]")
                        for i, result in enumerate(search_results):
                            print(f"  #{i+1} ç›¸ä¼¼åº¦: {result['score']:.4f} | {result['preview']}")
                        
                        # ä½¿ç”¨æœ€åŒ¹é…çš„ç»“æœç»§ç»­
                        result = search_results[0]
                        memory_embedding = result['embedding'].to(self.actual_device)
                        memory_text = result['text']
                        
                        print(f"\n[ç³»ç»Ÿ: ä½¿ç”¨æœ€ä½³åŒ¹é… (ç›¸ä¼¼åº¦: {result['score']:.4f})]")
                        
                        # 6. åˆ›å»ºåµŒå…¥å±‚å¼•ç”¨
                        embedding_layer = self.model.get_input_embeddings()
                        
                        # 7. å°†è®°å¿†å‘é‡ç›´æ¥è¾“å…¥æ¨¡å‹ï¼ˆè·³è¿‡åµŒå…¥å±‚ï¼‰
                        memory_embed = memory_embedding.unsqueeze(0).unsqueeze(0)
                        
                        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
                        memory_dtype = next(self.model.parameters()).dtype
                        memory_embed = memory_embed.to(memory_dtype)
                        
                        # ä½¿ç”¨KVç¼“å­˜è¿›è¡Œå‰å‘ä¼ æ’­ - è´ªå©ªè§£ç 
                        with torch.no_grad():
                            memory_outputs = self._forward_with_backbone(
                                inputs_embeds=memory_embed,
                                attention_mask=torch.ones(1, 1, device=self.actual_device),
                                past_key_values=past_key_values,
                                use_cache=True,
                            )
                        
                        # 8. æ›´æ–°KVç¼“å­˜
                        past_key_values = memory_outputs.past_key_values
                        
                        # 9. æ›´æ–°æ³¨æ„åŠ›æ©ç 
                        attention_mask = torch.cat([
                            attention_mask,
                            torch.ones(1, 1, device=self.actual_device)
                        ], dim=1)
                        
                        # 10. è¾“å…¥<|recall|> token
                        recall_token_id = self.special_token_ids['recall']
                        recall_input = torch.tensor([[recall_token_id]], device=self.actual_device)
                        
                        with torch.no_grad():
                            recall_outputs = self._forward_with_backbone(
                                input_ids=recall_input,
                                attention_mask=torch.ones(1, 1, device=self.actual_device),
                                past_key_values=past_key_values,
                                use_cache=True,
                            )
                        
                        # 11. æ›´æ–°KVç¼“å­˜å’Œæ³¨æ„åŠ›æ©ç 
                        past_key_values = recall_outputs.past_key_values
                        attention_mask = torch.cat([
                            attention_mask,
                            torch.ones(1, 1, device=self.actual_device)
                        ], dim=1)
                        
                        # 12. æ·»åŠ <|recall|> tokenåˆ°ç”Ÿæˆç»“æœ
                        generated_ids.append(recall_token_id)
                        
                        # æµå¼è¾“å‡ºå½“å‰token
                        if stream:
                            token_text = self.tokenizer.decode([recall_token_id])
                            print(token_text, end="", flush=True)
                        
                        # å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£çš„è¾“å…¥
                        input_ids = recall_input
                        continue
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®°å¿†æˆ–è®°å¿†æ•°æ®åº“ä¸ºç©º
                print("\n[ç³»ç»Ÿ: æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†]")
                input_ids = recall_start_input
                continue
            
            # å¤„ç†<|recall_end|>
            elif next_token_id == self.special_token_ids.get('recall_end') and in_recall_mode:
                in_recall_mode = False
            
            # æ·»åŠ tokenåˆ°ç”Ÿæˆç»“æœ
            generated_ids.append(next_token_id)
            
            # æµå¼è¾“å‡ºå½“å‰token
            if stream:
                token_text = self.tokenizer.decode([next_token_id])
                print(token_text, end="", flush=True)
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†EOS
            if next_token_id == self.tokenizer.eos_token_id:
                break
            
            # å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£çš„è¾“å…¥
            input_ids = torch.tensor([[next_token_id]], device=self.actual_device)
            
            # æ›´æ–°æ³¨æ„åŠ›æ©ç 
            attention_mask = torch.cat([
                attention_mask,
                torch.ones(1, 1, device=self.actual_device)
            ], dim=1)
        
        # è§£ç æœ€ç»ˆç»“æœ
        generated_text = self.tokenizer.decode(generated_ids)
        print("\n")
        
        # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
        self.conversation_history.append({"role": "assistant", "content": generated_text})
        
        return generated_text

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  ä¼˜åŒ–çš„è®°å¿†å›æº¯èŠå¤©ç³»ç»Ÿ")
    print("=" * 50)
    
    # æ¨¡å‹å’Œè®°å¿†è·¯å¾„
    MODEL_PATH = "./training_workspace/model_cycle_2"
    MEMORY_PATH = "./training_workspace/embeddings/text_embeddings.pt"
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        return
    
    if not os.path.exists(MEMORY_PATH):
        print(f"âŒ è®°å¿†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {MEMORY_PATH}")
        return
    
    # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
    chat_model = MemoryRecallChat(
        model_name=MODEL_PATH,
        memory_path=MEMORY_PATH,
        device="cuda:0"  # æˆ–æŒ‡å®šè®¾å¤‡ï¼Œå¦‚"cuda:0"
    )
    
    print("\nğŸ¤– è®°å¿†å›æº¯èŠå¤©å·²å‡†å¤‡å°±ç»ªï¼")
    print("è¾“å…¥ 'exit' é€€å‡ºèŠå¤©, è¾“å…¥ 'reset' é‡ç½®å¯¹è¯å†å²")
    print("è¾“å…¥ 'force-recall' å¼ºåˆ¶æ¨¡å‹ä»¥å›å¿†æ¨¡å¼å¼€å§‹å›ç­”")
    print("=" * 50)
    
    # è®¾ç½®é»˜è®¤ç³»ç»Ÿæç¤ºè¯
    default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœ‰è®°å¿†èƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚ä½ éœ€è¦æ ¹æ®å›å¿†å‡ºçš„å†…å®¹å›ç­”é—®é¢˜ã€‚"""
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯
    print(f"\nå½“å‰ç³»ç»Ÿæç¤ºè¯:\n{default_system_prompt}")
    change_prompt = input("\næ˜¯å¦ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯? (y/n): ").strip().lower()
    
    if change_prompt == 'y':
        system_prompt = input("è¯·è¾“å…¥æ–°çš„ç³»ç»Ÿæç¤ºè¯:\n")
    else:
        system_prompt = default_system_prompt
    
    # é»˜è®¤ä¸å¼ºåˆ¶å›å¿†
    force_recall_mode = True
    
    # èŠå¤©å¾ªç¯
    while True:
        user_input = input("\nç”¨æˆ·: ")
        
        if user_input.lower() in ['exit', 'quit', 'q']:
            break
        elif user_input.lower() == 'reset':
            chat_model.reset_conversation()
            continue
        elif user_input.lower() == 'force-recall':
            force_recall_mode = not force_recall_mode
            print(f"å¼ºåˆ¶å›å¿†æ¨¡å¼: {'å¼€å¯' if force_recall_mode else 'å…³é—­'}")
            continue
        
        try:
            # é¦–æ¬¡å¯¹è¯ä¼ å…¥ç³»ç»Ÿæç¤ºè¯ï¼Œåç»­å¯¹è¯ä¸éœ€è¦
            if len(chat_model.conversation_history) == 0:
                response = chat_model.chat(
                    user_message=user_input,
                    system_prompt=system_prompt,
                    max_new_tokens=500,
                    temperature=0.7,
                    top_p=0.9,
                    stream=True,
                    force_recall=force_recall_mode
                )
            else:
                response = chat_model.chat(
                    user_message=user_input,
                    max_new_tokens=500,
                    temperature=0.7,
                    top_p=0.9,
                    stream=True,
                    force_recall=force_recall_mode
                )
            
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()