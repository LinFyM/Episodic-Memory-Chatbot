import os
import random
import torch
import json
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Any
import torch.distributed as dist

# å¯¼å…¥å„ä¸ªè®­ç»ƒç»„ä»¶
from add_special_tokens_wrapper import SpecialTokensManager
from get_text_embedding import TextEmbeddingExtractor
from text_embedding_train import RecallMemoryTrainer
from text_memory_train import EnhancedTextMemoryTrainer
from normal_text_train import NormalTextTrainer

class IntegratedTrainingPipeline:
    """é›†æˆè®­ç»ƒæµæ°´çº¿ - è‡ªåŠ¨åŒ–å®Œæˆè®°å¿†è®­ç»ƒå…¨æµç¨‹"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒå‚æ•°
        """
        self.config = config
        self.device = config.get('device', 'cuda:0')
        self.ddp_enabled = False
        self.local_rank = None
        # è‹¥ç”± torchrun å¯åŠ¨ï¼Œå¯ç”¨DDP
        if 'LOCAL_RANK' in os.environ:
            self.local_rank = int(os.environ['LOCAL_RANK'])
            os.environ.setdefault('RANK', os.environ.get('RANK', '0'))
            os.environ.setdefault('WORLD_SIZE', os.environ.get('WORLD_SIZE', '1'))
            torch.cuda.set_device(self.local_rank)
            # å¯èƒ½è¢«å¤šå¤„åˆå§‹åŒ–ï¼Œåšå¹‚ç­‰ä¿æŠ¤
            if not dist.is_available() or (dist.is_available() and not dist.is_initialized()):
                dist.init_process_group(backend='nccl', timeout=timedelta(minutes=60))
            self.ddp_enabled = True
            # åœ¨DDPä¸‹å›ºå®šæ¯è¿›ç¨‹å•å¡
            self.device = f"cuda:{self.local_rank}"
            self.config['device'] = self.device
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        self.work_dir = config.get('work_dir', './training_workspace')
        if self.is_main_process():
            os.makedirs(self.work_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        self._save_config()
        
        if self.is_main_process():
            print("ğŸš€ é›†æˆè®­ç»ƒæµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
            print(f"   å·¥ä½œç›®å½•: {self.work_dir}")
            print(f"   è®¾å¤‡: {self.device}")
    
    def is_main_process(self) -> bool:
        return (not self.ddp_enabled) or (dist.get_rank() == 0)
        
    def _save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_path = os.path.join(self.work_dir, 'training_config.json')
        if self.is_main_process():
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“ é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
        
    def load_and_split_dataset(self, dataset_path: str) -> Tuple[List[str], List[str]]:
        """
        åŠ è½½æ•°æ®é›†å¹¶æŒ‰æ¯”ä¾‹åˆ†å‰² - æ”¯æŒJSONå’ŒCSVæ ¼å¼
        """
        if self.is_main_process():
            print(f"ğŸ“– åŠ è½½æ•°æ®é›†: {dataset_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        
        # æ ¹æ®æ–‡ä»¶æ ¼å¼åŠ è½½æ•°æ®
        texts = []
        
        if dataset_path.endswith('.csv'):
            # CSVæ ¼å¼æ”¯æŒ - åªè¯»å–text1å¹¶æ¸…æ´—
            import pandas as pd
            if self.is_main_process():
                print("ğŸ“Š æ£€æµ‹åˆ°CSVæ ¼å¼ï¼Œæ­£åœ¨è§£æ...")
            
            try:
                # è¯»å–CSVæ–‡ä»¶
                df = pd.read_csv(dataset_path)
                if self.is_main_process():
                    print(f"   CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
                    print(f"   CSVæ–‡ä»¶è¡Œæ•°: {len(df)}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰text1åˆ—
                if 'text1' not in df.columns:
                    raise ValueError(f"CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°'text1'åˆ—ï¼Œç°æœ‰åˆ—: {list(df.columns)}")
                
                print("   åªè¯»å–text1åˆ—ï¼Œæ¸…æ´—[SEP]å‰çš„ç­”æ¡ˆéƒ¨åˆ†...")
                
                for _, row in df.iterrows():
                    text1 = str(row['text1']).strip()
                    
                    # å¤„ç†text1ï¼šå»æ‰[SEP]å‰çš„ç­”æ¡ˆï¼Œåªä¿ç•™åé¢çš„ä¸Šä¸‹æ–‡
                    if '[SEP]' in text1:
                        # åˆ†å‰²å¹¶åªå–[SEP]åé¢çš„éƒ¨åˆ†
                        parts = text1.split('[SEP]', 1)
                        if len(parts) == 2:
                            context = parts[1].strip()  # åªè¦[SEP]åé¢çš„ä¸Šä¸‹æ–‡
                            if context:  # ç¡®ä¿ä¸Šä¸‹æ–‡ä¸ä¸ºç©º
                                texts.append(context)
                    else:
                        # å¦‚æœæ²¡æœ‰[SEP]ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªtext1
                        if text1:
                            texts.append(text1)
                            
            except Exception as e:
                print(f"âŒ CSVè§£æå¤±è´¥: {e}")
                # å°è¯•æ‰‹åŠ¨è§£æ
                if self.is_main_process():
                    print("ğŸ”„ å°è¯•æ‰‹åŠ¨è§£æCSV...")
                texts = self._manual_parse_csv_text1_only(dataset_path)
                
        elif dataset_path.endswith('.txt'):
            # åŸæœ‰çš„TXTæ ¼å¼æ”¯æŒ
            with open(dataset_path, 'r', encoding='utf-8') as f:
                texts = [line.strip() for line in f if line.strip()]
                
        elif dataset_path.endswith('.json'):
            # åŸæœ‰çš„JSONæ ¼å¼æ”¯æŒ
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                if isinstance(data, list):
                    texts = data
                elif isinstance(data, dict):
                    if 'data' in data:
                        data_items = data['data']
                        if isinstance(data_items, list):
                            texts = []
                            for item in data_items:
                                if isinstance(item, dict) and 'text' in item:
                                    texts.append(item['text'])
                                elif isinstance(item, str):
                                    texts.append(item)
                    elif 'texts' in data:
                        texts = data['texts']
                    else:
                        texts = list(data.values())[0] if data else []
                else:
                    raise ValueError(f"JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {dataset_path}")
        
        if self.is_main_process():
            print(f"   æˆåŠŸè§£ææ–‡æœ¬æ•°é‡: {len(texts)}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥å’Œè¿‡æ»¤
        original_count = len(texts)
        texts = [text for text in texts if text and len(text.strip()) > 20]  # æœ€å°‘20ä¸ªå­—ç¬¦
        filtered_count = len(texts)
        
        if self.is_main_process():
            print(f"   è¿‡æ»¤åæ–‡æœ¬æ•°é‡: {filtered_count} (è¿‡æ»¤æ‰ {original_count - filtered_count} æ¡)")
        
        if len(texts) == 0:
            raise ValueError("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")
        
        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        if self.is_main_process():
            print("\nğŸ“ æ¸…æ´—åçš„æ•°æ®æ ·æœ¬é¢„è§ˆ:")
            for i, text in enumerate(texts[:1]):
                preview = text[:200] + "..." if len(text) > 200 else text
                print(f"   æ ·æœ¬ {i+1}: {preview}")
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²
        memory_ratio = self.config.get('memory_ratio', 0.3)
        random.shuffle(texts)
        
        split_idx = int(len(texts) * memory_ratio)
        if split_idx == 0:
            split_idx = 1
        
        memory_texts = texts[:split_idx]
        normal_texts = texts[split_idx:]
        
        if self.is_main_process():
            print(f"\nğŸ“Š æ•°æ®åˆ†å‰²ç»“æœ:")
            print(f"   è®°å¿†è®­ç»ƒæ–‡æœ¬: {len(memory_texts)} ({len(memory_texts)/len(texts)*100:.1f}%)")
            print(f"   æ™®é€šè®­ç»ƒæ–‡æœ¬: {len(normal_texts)} ({len(normal_texts)/len(texts)*100:.1f}%)")
        
        return memory_texts, normal_texts

    def _manual_parse_csv_text1_only(self, dataset_path: str) -> List[str]:
        """æ‰‹åŠ¨è§£æCSVæ–‡ä»¶ï¼Œåªæå–text1çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†"""
        texts = []
        
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                import csv
                reader = csv.reader(f)
                header = next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
                
                print(f"   CSVæ ‡é¢˜è¡Œ: {header}")
                
                # æ‰¾åˆ°text1åˆ—çš„ç´¢å¼•
                text1_index = None
                for i, col in enumerate(header):
                    if col.strip().lower() == 'text1':
                        text1_index = i
                        break
                
                if text1_index is None:
                    raise ValueError("æœªæ‰¾åˆ°text1åˆ—")
                
                for row in reader:
                    if len(row) > text1_index:
                        text1 = row[text1_index].strip().strip('"')
                        
                        # å¤„ç†[SEP]åˆ†éš”ç¬¦
                        if '[SEP]' in text1:
                            parts = text1.split('[SEP]', 1)
                            if len(parts) == 2:
                                context = parts[1].strip()  # åªè¦[SEP]åé¢çš„éƒ¨åˆ†
                                if context:
                                    texts.append(context)
                        else:
                            # æ²¡æœ‰[SEP]å°±ç›´æ¥ä½¿ç”¨
                            if text1:
                                texts.append(text1)
                            
        except Exception as e:
            print(f"âŒ æ‰‹åŠ¨CSVè§£æå¤±è´¥: {e}")
            raise ValueError(f"æ— æ³•è§£æCSVæ–‡ä»¶: {dataset_path}")
        
        return texts
        
    def step1_extract_embeddings(self, memory_texts: List[str]) -> str:
        """æ­¥éª¤1: ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹æå–è®°å¿†æ–‡æœ¬çš„ç‰¹å¾å‘é‡"""
        print(f"ğŸ¯ ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹: {self.config['original_model_path']}")
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")

        save_dir = os.path.join(self.work_dir, 'embeddings')

        if self.ddp_enabled:
            # 1) å„rankåˆ†ç‰‡å¤„ç†
            world_size = dist.get_world_size()
            rank = dist.get_rank()
            total = len(memory_texts)
            shard_size = (total + world_size - 1) // world_size
            start = rank * shard_size
            end = min(total, (rank + 1) * shard_size)
            shard_texts = memory_texts[start:end]

            print(f"[rank{rank}] æå–åˆ†ç‰‡: {start}:{end} / {total}")
            extractor = TextEmbeddingExtractor(self.config['original_model_path'], device=self.device)
            data = extractor.extract_embeddings(shard_texts)
            os.makedirs(save_dir, exist_ok=True)
            partial_path = os.path.join(save_dir, f'text_embeddings_rank{rank}.pt')
            # ç”¨å·²æœ‰çš„ä¿å­˜å‡½æ•°ä½†æŒ‡å®šæ–‡ä»¶å
            extractor.save_embeddings(data, save_dir=save_dir, filename=f'text_embeddings_rank{rank}.pt')
            del extractor
            import gc
            gc.collect()
            torch.cuda.empty_cache()

            dist.barrier()

            # 2) rank0 åˆå¹¶
            final_path = os.path.join(save_dir, 'text_embeddings.pt')
            if self.is_main_process():
                print("ğŸ”— åˆå¹¶å„rankæå–çš„åµŒå…¥...")
                # æ”¶é›†æ‰€æœ‰åˆ†ç‰‡
                texts_all = []
                embeds_all = []
                prompts_all = []
                tokens_all = []
                for r in range(world_size):
                    p = os.path.join(save_dir, f'text_embeddings_rank{r}.pt')
                    part = torch.load(p, map_location='cpu')
                    texts_all.extend(part.get('texts', []))
                    if 'embeddings' in part:
                        embeds = part['embeddings']
                        if isinstance(embeds, torch.Tensor):
                            embeds_all.append(embeds.cpu())
                    prompts_all.extend(part.get('prompts', []))
                    tokens_all.extend(part.get('predicted_tokens', []))

                if embeds_all:
                    embeddings_cat = torch.cat(embeds_all, dim=0)
                else:
                    raise ValueError("æœªå‘ç°ä»»ä½•åˆ†ç‰‡çš„embeddings")

                merged = {
                    'texts': texts_all,
                    'embeddings': embeddings_cat,
                    'prompts': prompts_all,
                    'predicted_tokens': tokens_all,
                    'metadata': {
                        'model_name': self.config['original_model_path'],
                        'embedding_dim': embeddings_cat.shape[-1],
                        'num_samples': len(texts_all),
                        'dtype': str(embeddings_cat.dtype),
                        'created_date': datetime.now().isoformat(),
                        'device': str(self.device),
                        'device_config': f'ddp world_size={world_size}'
                    }
                }
                torch.save(merged, final_path)
                print(f"ğŸ’¾ å·²åˆå¹¶ä¿å­˜åˆ°: {final_path} (æ€»æ ·æœ¬: {len(texts_all)})")

                # æ¸…ç†åˆ†ç‰‡æ–‡ä»¶
                for r in range(world_size):
                    p = os.path.join(save_dir, f'text_embeddings_rank{r}.pt')
                    try:
                        os.remove(p)
                    except OSError:
                        pass

            dist.barrier()
            return final_path

        else:
            extractor = TextEmbeddingExtractor(self.config['original_model_path'], device=self.device)
            data = extractor.extract_embeddings(memory_texts)
            save_path = extractor.save_embeddings(data, save_dir)
            del extractor
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            print("ğŸ§¹ å·²æ¸…ç†ç‰¹å¾æå–å™¨å†…å­˜")
            print("âœ… æ­¥éª¤1å®Œæˆ: ç‰¹å¾å‘é‡æå–æˆåŠŸ")
            return save_path

    def step2_add_special_tokens(self) -> str:
        """æ­¥éª¤2: æ·»åŠ ç‰¹æ®Štoken"""
        if self.is_main_process():
            print("\n" + "="*60)
            print("ğŸ“ æ­¥éª¤ 2/6: æ·»åŠ è®°å¿†ç›¸å…³ç‰¹æ®Štoken")
            print("="*60)
            original_model = self.config['original_model_path']
            token_manager = SpecialTokensManager(original_model, self.device)
            save_path = os.path.join(self.work_dir, 'model_with_special_tokens')
            model_path, token_ids = token_manager.process(
                save_path, 
                self.config.get('token_perturbation_std', 0.02)
            )
            del token_manager
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            print("ğŸ§¹ å·²æ¸…ç†tokenç®¡ç†å™¨å†…å­˜")
            token_info = {
                'token_ids': token_ids,
                'model_path': model_path,
                'step_completed': 'add_special_tokens'
            }
            with open(os.path.join(self.work_dir, 'token_info.json'), 'w') as f:
                json.dump(token_info, f, indent=2)
            print("âœ… æ­¥éª¤2å®Œæˆ: ç‰¹æ®Štokenæ·»åŠ æˆåŠŸ")
        else:
            model_path = os.path.join(self.work_dir, 'model_with_special_tokens')
        if self.ddp_enabled:
            dist.barrier()
        return model_path
        
    def step3_embedding_training(self, embedding_path: str, model_path: str) -> str:
        """æ­¥éª¤3: åµŒå…¥å‘é‡è®­ç»ƒï¼ˆ<recall> tokenï¼‰"""
        print("\n" + "="*60)
        print("ğŸ“ æ­¥éª¤ 3/6: è®­ç»ƒ <recall> token")
        print("="*60)
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        trainer = RecallMemoryTrainer(model_path, self.device)
        
        embedding_config = self.config.get('embedding_training', {})
        save_path = os.path.join(self.work_dir, 'model_embedding_trained')
        
        trainer.train(
            pt_file_path=embedding_path,
            num_epochs=embedding_config.get('num_epochs', 30),
            batch_size=embedding_config.get('batch_size', 4),
            learning_rate=embedding_config.get('learning_rate', 1e-4),
            save_path=save_path
        )
        
        # æ¸…ç†trainerå ç”¨çš„å†…å­˜
        del trainer
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        print("ğŸ§¹ å·²æ¸…ç†åµŒå…¥è®­ç»ƒå™¨å†…å­˜")
        
        print("âœ… æ­¥éª¤3å®Œæˆ: åµŒå…¥å‘é‡è®­ç»ƒæˆåŠŸ")
        return save_path

    def step4_memory_training(self, embedding_path: str, model_path: str) -> str:
        """æ­¥éª¤4: è®°å¿†è®­ç»ƒï¼ˆè¡¨å¾å‘é‡è§£ç ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ“ æ­¥éª¤ 4/6: è®­ç»ƒè¡¨å¾å‘é‡è§£ç èƒ½åŠ›")
        print("="*60)
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        trainer = EnhancedTextMemoryTrainer(model_path, self.device)
        
        memory_config = self.config.get('memory_training', {})
        save_path = os.path.join(self.work_dir, 'model_memory_trained')
        
        trainer.train(
            pt_file_path=embedding_path,
            num_epochs=memory_config.get('num_epochs', 20),
            batch_size=memory_config.get('batch_size', 4),
            learning_rate=memory_config.get('learning_rate', 1e-4),
            noise_std=memory_config.get('noise_std', 0.0),
            save_path=save_path
        )
        
        # æ¸…ç†trainerå ç”¨çš„å†…å­˜
        del trainer
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        print("ğŸ§¹ å·²æ¸…ç†è®°å¿†è®­ç»ƒå™¨å†…å­˜")
        
        print("âœ… æ­¥éª¤4å®Œæˆ: è®°å¿†è®­ç»ƒæˆåŠŸ")
        return save_path

    def step5_normal_training(self, normal_texts: List[str], model_path: str) -> str:
        """æ­¥éª¤5: æ™®é€šæ–‡æœ¬è®­ç»ƒ"""
        print("\n" + "="*60)
        print("ğŸ“ æ­¥éª¤ 5/6: æ™®é€šæ–‡æœ¬è®­ç»ƒ")
        print("="*60)
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # NormalTextTrainer æœªæ”¹é€ DDPï¼Œè¿™é‡Œä»…åœ¨ä¸»è¿›ç¨‹è¿è¡Œï¼Œé¿å…é‡å¤è®­ç»ƒ
        if self.is_main_process():
            trainer = NormalTextTrainer(model_path, self.device)
            normal_config = self.config.get('normal_training', {})
            save_path = os.path.join(self.work_dir, 'model_normal_trained')
            trainer.train(
                texts=normal_texts,
                num_epochs=normal_config.get('num_epochs', 5),
                batch_size=normal_config.get('batch_size', 4),
                learning_rate=normal_config.get('learning_rate', 1e-4),
                save_path=save_path
            )
            del trainer
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            print("ğŸ§¹ å·²æ¸…ç†æ™®é€šè®­ç»ƒå™¨å†…å­˜")
            print("âœ… æ­¥éª¤5å®Œæˆ: æ™®é€šæ–‡æœ¬è®­ç»ƒæˆåŠŸ")
        else:
            save_path = os.path.join(self.work_dir, 'model_normal_trained')
        if self.ddp_enabled:
            dist.barrier()
        return save_path
        
    def step6_final_integration(self, final_model_path: str) -> str:
        """æ­¥éª¤6: æœ€ç»ˆæ•´åˆå’ŒéªŒè¯"""
        print("\n" + "="*60)
        print("ğŸ“ æ­¥éª¤ 6/6: æœ€ç»ˆæ•´åˆå’ŒéªŒè¯")
        print("="*60)
        
        # ä»…ä¸»è¿›ç¨‹æ‰§è¡Œæœ€ç»ˆæ•´åˆ
        if self.is_main_process():
            final_save_path = os.path.join(self.work_dir, 'final_model')
            import shutil
            if os.path.exists(final_save_path):
                shutil.rmtree(final_save_path)
            shutil.copytree(final_model_path, final_save_path)
            self._generate_training_report(final_save_path)
            print("âœ… æ­¥éª¤6å®Œæˆ: æœ€ç»ˆæ•´åˆæˆåŠŸ")
        else:
            final_save_path = os.path.join(self.work_dir, 'final_model')
        if self.ddp_enabled:
            dist.barrier()
        return final_save_path
        
    def _generate_training_report(self, final_model_path: str):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report = {
            'training_completed': datetime.now().isoformat(),
            'final_model_path': final_model_path,
            'config': self.config,
            'steps_completed': [
                'add_special_tokens',
                'extract_embeddings', 
                'embedding_training',
                'memory_training',
                'normal_training',
                'final_integration'
            ]
        }
        
        report_path = os.path.join(self.work_dir, 'training_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ“Š è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
    def run_training_cycle(self, embedding_path: str, normal_texts: List[str], 
                        current_model_path: str, cycle_num: int) -> str:
        """è¿è¡Œä¸€ä¸ªè®­ç»ƒå‘¨æœŸ"""
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒå‘¨æœŸ {cycle_num}")
        
        # åµŒå…¥å‘é‡è®­ç»ƒ
        model_path = self.step3_embedding_training(embedding_path, current_model_path)
        
        # è®°å¿†è®­ç»ƒ  
        model_path = self.step4_memory_training(embedding_path, model_path)
        
        # æ™®é€šæ–‡æœ¬è®­ç»ƒ
        model_path = self.step5_normal_training(normal_texts, model_path)
        
        print(f"âœ… è®­ç»ƒå‘¨æœŸ {cycle_num} å®Œæˆ")
        return model_path
        
    def run_full_pipeline(self, dataset_path: str):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿"""
        if self.is_main_process():
            print("ğŸš€ å¼€å§‹é›†æˆè®­ç»ƒæµæ°´çº¿")
            print("="*80)
        
        start_time = datetime.now()
        
        try:
            # åŠ è½½å’Œåˆ†å‰²æ•°æ®é›†
            memory_texts, normal_texts = self.load_and_split_dataset(dataset_path)
            
            # æ­¥éª¤1: ä½¿ç”¨åŸå§‹æ¨¡å‹æå–ç‰¹å¾å‘é‡ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
            if self.is_main_process():
                print("\n" + "="*60)
                print("ğŸ“ æ­¥éª¤ 1/6: ä½¿ç”¨åŸå§‹æ¨¡å‹æå–ç‰¹å¾å‘é‡")
                print("="*60)
            embedding_path = self.step1_extract_embeddings(memory_texts)
            
            # æ­¥éª¤2: æ·»åŠ ç‰¹æ®Štoken
            model_path = self.step2_add_special_tokens()
            
            # å¤šè½®è®­ç»ƒå¾ªç¯
            num_cycles = self.config.get('num_training_cycles', 3)
            if self.is_main_process():
                print(f"\nğŸ”„ å°†è¿›è¡Œ {num_cycles} ä¸ªè®­ç»ƒå‘¨æœŸ")
            
            for cycle in range(1, num_cycles + 1):
                model_path = self.run_training_cycle(
                    embedding_path, normal_texts, model_path, cycle  # ä¼ é€’embedding_pathè€Œä¸æ˜¯memory_texts
                )
                
                # æ¯ä¸ªå‘¨æœŸåä¿å­˜ä¸­é—´æ¨¡å‹
                if self.is_main_process():
                    cycle_save_path = os.path.join(self.work_dir, f'model_cycle_{cycle}')
                    import shutil
                    if os.path.exists(cycle_save_path):
                        shutil.rmtree(cycle_save_path)
                    shutil.copytree(model_path, cycle_save_path)
                    print(f"ğŸ’¾ å‘¨æœŸ {cycle} æ¨¡å‹å·²ä¿å­˜: {cycle_save_path}")
                if self.ddp_enabled:
                    dist.barrier()
            
            # æ­¥éª¤6: æœ€ç»ˆæ•´åˆ
            final_model_path = self.step6_final_integration(model_path)
            
            # è®¡ç®—æ€»æ—¶é—´
            end_time = datetime.now()
            total_time = end_time - start_time
            
            if self.is_main_process():
                print("\n" + "="*80)
                print("ğŸ‰ é›†æˆè®­ç»ƒæµæ°´çº¿å®Œæˆ!")
                print("="*80)
                print(f"   æ€»è€—æ—¶: {total_time}")
                print(f"   æœ€ç»ˆæ¨¡å‹: {final_model_path}")
                print(f"   å·¥ä½œç›®å½•: {self.work_dir}")
                print(f"   è®­ç»ƒå‘¨æœŸ: {num_cycles}")
                print("="*80)
            
            return final_model_path
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒæµæ°´çº¿å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            raise

def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    project_root = os.path.dirname(os.path.abspath(__file__))
    return {
        # åŸºç¡€é…ç½®
        'original_model_path': os.path.join(project_root, 'Qwen2.5-7B-Instruct'),
        'device': 'cuda:0',
        'work_dir': os.path.join(project_root, 'training_workspace'),
        'memory_ratio': 0.6,  # è®°å¿†è®­ç»ƒæ•°æ®å æ¯”
        'num_training_cycles': 3,  # è®­ç»ƒå‘¨æœŸæ•°
        
        # ç‰¹æ®Štokené…ç½®
        'token_perturbation_std': 0.02,
        
        # åµŒå…¥è®­ç»ƒé…ç½®
        'embedding_training': {
            'num_epochs': 2,
            'batch_size': 2,
            'learning_rate': 1e-4
        },
        
        # è®°å¿†è®­ç»ƒé…ç½®
        'memory_training': {
            'num_epochs': 2,
            'batch_size': 2,
            'learning_rate': 1e-4,
            'noise_std': 0.0
        },
        
        # æ™®é€šè®­ç»ƒé…ç½®
        'normal_training': {
            'num_epochs': 2,
            'batch_size': 2,
            'learning_rate': 1e-4
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®
    config = create_default_config()
    
    # ä¿®æ”¹é…ç½®
    config['device'] = ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']
    config['memory_ratio'] = 0.6  # 30%ç”¨äºè®°å¿†è®­ç»ƒ
    config['num_training_cycles'] = 8
    # é™å†…å­˜ï¼šStep3æ‰¹æ¬¡è®¾ä¸º1ï¼ˆDDPä¸‹æ¯å¡1æ ·æœ¬ï¼‰
    config['embedding_training']['batch_size'] = 2
    
    # ä¿®æ”¹æ•°æ®é›†è·¯å¾„ä¸ºCSVæ–‡ä»¶
    project_root = os.path.dirname(os.path.abspath(__file__))
    dataset_path = os.path.join(project_root, "DuReader_robust-QG", "train.csv")  # ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„å¤±æ•ˆ
    # dataset_path = "./datasets/rich_text_dataset.json"
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        print("è¯·ç¡®ä¿CSVæ–‡ä»¶å­˜åœ¨ï¼Œæˆ–è€…ä¿®æ”¹dataset_pathå˜é‡")
        return
    
    # æ·»åŠ pandasä¾èµ–æ£€æŸ¥
    try:
        import pandas as pd
        print("âœ… pandaså¯ç”¨ï¼Œæ”¯æŒCSVè§£æ")
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…pandas: pip install pandas")
        return
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œæµæ°´çº¿
        pipeline = IntegratedTrainingPipeline(config)
        final_model_path = pipeline.run_full_pipeline(dataset_path)
        
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ¨¡å‹ä½äº: {final_model_path}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # ä¼˜é›…å…³é—­è¿›ç¨‹ç»„ï¼Œæ¶ˆé™¤NCCLèµ„æºå‘Šè­¦
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                dist.barrier()
                dist.destroy_process_group()
        except Exception:
            pass

if __name__ == "__main__":
    main()