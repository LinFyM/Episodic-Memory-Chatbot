import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
from tqdm import tqdm
import json
from peft import LoraConfig, get_peft_model, TaskType
from modelscope import AutoModelForCausalLM, AutoTokenizer

class NormalTextDataset(Dataset):
    """æ™®é€šæ–‡æœ¬è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, texts, tokenizer, max_length=3000):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
        
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        
        # æ ‡ç­¾å°±æ˜¯input_idsï¼Œå‘å³ç§»ä½åœ¨æŸå¤±è®¡ç®—æ—¶å¤„ç†
        labels = input_ids.clone()
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

class NormalTextTrainer:
    """æ™®é€šæ–‡æœ¬è®­ç»ƒå™¨ - æ”¯æŒå¤šGPUé…ç½®"""
    
    def __init__(self, model_name, device=None):
        self.model_name = model_name
        self.specified_device = device
        
        # è®¾å¤‡å¤„ç†é€»è¾‘ - ä¸å…¶ä»–è®­ç»ƒå™¨ä¿æŒä¸€è‡´
        if device is None:
            self.use_auto_device = False
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        elif isinstance(device, list):
            # å¤„ç†GPUåˆ—è¡¨
            if len(device) > 0:
                self.use_auto_device = False
                self.primary_device = torch.device(device[0])
                self.multi_gpu_list = device
                print(f"   ä½¿ç”¨å¤šGPUåˆ—è¡¨: {device}ï¼Œä¸»è®¾å¤‡: {device[0]}")
            else:
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
        elif isinstance(device, str):
            if device == "auto":
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
            elif device.startswith('cuda:'):
                # å•GPUé…ç½® - æ£€æŸ¥CUDA_VISIBLE_DEVICES
                cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
                if cuda_visible and cuda_visible.strip():
                    # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„è®¾å¤‡
                    self.primary_device = torch.device("cuda:0")
                    print(f"   CUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ cuda:0ï¼ˆå¯¹åº”ç‰©ç†GPU {device}ï¼‰")
                else:
                    # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç›´æ¥ä½¿ç”¨ç‰©ç†è®¾å¤‡
                    self.primary_device = torch.device(device)
                    print(f"   ä½¿ç”¨è®¾å¤‡ {device}")
                self.use_auto_device = False
                self.multi_gpu_list = None
            else:
                self.use_auto_device = False
                self.primary_device = torch.device(device)
                self.multi_gpu_list = None
        else:
            self.use_auto_device = False
            self.primary_device = device
            self.multi_gpu_list = None
            
        print(f"ğŸ¤– åˆå§‹åŒ–æ™®é€šæ–‡æœ¬è®­ç»ƒå™¨...")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   è®¾å¤‡: {device}")
        
        self._load_model()
        
        # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
        first_param = next(self.model.parameters())
        self.actual_device = first_param.device
        print(f"   å®é™…æ¨¡å‹è®¾å¤‡: {self.actual_device}")
        
        self._setup_lora()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - æ”¯æŒå¤šGPUé…ç½®"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        try:
            # æ ¹æ®è®¾å¤‡é…ç½®é€‰æ‹©device_map
            if self.use_auto_device:
                device_map = "auto"
                print("   ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")
            elif hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                # å¤šGPUé…ç½®
                device_map = "auto"
                print(f"   ä½¿ç”¨å¤šGPUè‡ªåŠ¨åˆ†é…: {self.multi_gpu_list}")
                
                # è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶å¯è§GPU
                import os
                if 'CUDA_VISIBLE_DEVICES' not in os.environ:
                    gpu_indices = [gpu.split(':')[1] for gpu in self.multi_gpu_list if gpu.startswith('cuda:')]
                    if gpu_indices:
                        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_indices)
                        print(f"   è®¾ç½®CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
            elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                # å•GPUæŒ‡å®š
                device_index = int(self.specified_device.split(':')[1])
                device_map = {"": device_index}
                print(f"   ä½¿ç”¨æŒ‡å®šå•GPU: {self.specified_device}")
            elif self.specified_device == "cpu":
                device_map = {"": "cpu"}
                print(f"   ä½¿ç”¨CPUè®¾å¤‡")
            else:
                # é»˜è®¤æƒ…å†µ
                if hasattr(self, 'primary_device') and self.primary_device.type == 'cuda':
                    device_map = {"": self.primary_device.index}
                else:
                    device_map = "auto"
                print(f"   ä½¿ç”¨é»˜è®¤è®¾å¤‡æ˜ å°„: {device_map}")
            
            print(f"   å®é™…ä½¿ç”¨è®¾å¤‡æ˜ å°„: {device_map}")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map=device_map,
                trust_remote_code=True
            )
            
            # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
            first_param = next(self.model.parameters())
            model_dtype = first_param.dtype
            model_device = first_param.device
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å®é™…è®¾å¤‡: {model_device}")
            print(f"   æ•°æ®ç±»å‹: {model_dtype}")
            
            # æ˜¾ç¤ºè®¾å¤‡æ˜ å°„ä¿¡æ¯
            if hasattr(self.model, 'hf_device_map'):
                print(f"   è®¾å¤‡æ˜ å°„è¯¦æƒ…: {self.model.hf_device_map}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€ç­–ç•¥
            print("ğŸ”„ å°è¯•å›é€€åˆ°å•GPUæ¨¡å¼...")
            
            try:
                # ç¡®å®šå›é€€è®¾å¤‡
                if hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                    fallback_device = self.multi_gpu_list[0]
                elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                    fallback_device = self.specified_device
                else:
                    fallback_device = 'cuda:0'
                
                # æå–è®¾å¤‡ç´¢å¼•
                if fallback_device.startswith('cuda:'):
                    device_index = int(fallback_device.split(':')[1])
                    device_map = {"": device_index}
                else:
                    device_map = {"": "cpu"}
                
                print(f"   å›é€€è®¾å¤‡æ˜ å°„: {device_map}")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype="auto",
                    device_map=device_map,
                    trust_remote_code=True
                )
                
                first_param = next(self.model.parameters())
                print(f"âœ… ä½¿ç”¨å›é€€è®¾å¤‡åŠ è½½æˆåŠŸ: {first_param.device}")
                
            except Exception as fallback_error:
                print(f"âŒ å›é€€åŠ è½½ä¹Ÿå¤±è´¥: {fallback_error}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: åŸé”™è¯¯={e}, å›é€€é”™è¯¯={fallback_error}")
        
    def _setup_lora(self):
        """è®¾ç½®LoRA"""
        print("âš¡ é…ç½®LoRA...")
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=[
                "q_proj", "v_proj", "k_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
        )
        
        self.model = get_peft_model(self.model, lora_config)
        print("âœ… LoRAé…ç½®å®Œæˆ")
        
    def create_dataloader(self, texts, batch_size=4, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        dataset = NormalTextDataset(texts, self.tokenizer)
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
        
    def train_epoch(self, dataloader, optimizer):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(dataloader, desc="æ™®é€šæ–‡æœ¬è®­ç»ƒ")
        
        for batch in progress_bar:
            # ä½¿ç”¨å®é™…è®¾å¤‡
            input_ids = batch['input_ids'].to(self.actual_device)
            attention_mask = batch['attention_mask'].to(self.actual_device)
            labels = batch['labels'].to(self.actual_device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.6f}'})
            
        return total_loss / len(dataloader)
        
    def merge_and_save_model(self, save_path):
        """åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜æ¨¡å‹"""
        print("ğŸ”„ åˆå¹¶LoRAæƒé‡...")
        
        merged_model = self.model.merge_and_unload()
        
        os.makedirs(save_path, exist_ok=True)
        merged_model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        return merged_model
        
    def train(self, texts, num_epochs=5, batch_size=4, learning_rate=1e-4, save_path=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹æ™®é€šæ–‡æœ¬è®­ç»ƒ")
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   å®é™…è®¾å¤‡: {self.actual_device}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self.create_dataloader(texts, batch_size, True)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=learning_rate,
            weight_decay=0.01
        )
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            
            avg_loss = self.train_epoch(train_loader, optimizer)
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.6f}")
            
        # ä¿å­˜æ¨¡å‹
        if save_path:
            final_model = self.merge_and_save_model(save_path)
            
        print("ğŸ‰ æ™®é€šæ–‡æœ¬è®­ç»ƒå®Œæˆ!")
        return avg_loss

def main():
    """æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æµ‹è¯•æ™®é€šæ–‡æœ¬è®­ç»ƒå™¨...")
    
    # æµ‹è¯•å¤šGPUé…ç½®
    trainer = NormalTextTrainer(
        "./Qwen2.5-7B-Instruct", 
        device=['cuda:1', 'cuda:2', 'cuda:3']  # å¤šGPUé…ç½®
    )
    
    # æµ‹è¯•æ•°æ®
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ™®é€šæ–‡æœ¬è®­ç»ƒåŠŸèƒ½ã€‚",
        "å¤šGPUè®­ç»ƒå¯ä»¥åŠ é€Ÿå¤§æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚",
        "LoRAæ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚"
    ]
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        texts=test_texts,
        num_epochs=2,
        batch_size=2,
        learning_rate=1e-4,
        save_path="./test_normal_trained"
    )
    
    print("âœ… æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()