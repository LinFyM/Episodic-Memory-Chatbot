import torch
import os
from tqdm import tqdm
from datetime import datetime
from create_text_dataset import load_text_dataset, get_dataset_paths
from modelscope import AutoModelForCausalLM, AutoTokenizer
from recall.model_utils import forward_backbone, ensure_last_hidden_state, build_causal_lm_output

def extract_last_token_embedding(model, tokenizer, text, device):
    """æå–æ–‡æœ¬æœ€åä¸€ä¸ªtokençš„åµŒå…¥å‘é‡"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    
    with torch.no_grad():
        backbone_outputs = forward_backbone(
            model,
            input_ids=input_ids,
            attention_mask=attention_mask,
            use_cache=False,
            output_hidden_states=False,
            return_dict=True,
        )
        last_hidden_states = ensure_last_hidden_state(backbone_outputs)
        last_token_embedding = last_hidden_states[0, -1, :]
        
        # è·å–é¢„æµ‹tokenï¼ˆåªä¸ºè¿™ä¸ªæ“ä½œè½¬æ¢ç±»å‹ï¼‰
        causal_outputs = build_causal_lm_output(model, backbone_outputs)
        logits = causal_outputs.logits[0, -1, :]
        predicted_token_id = torch.argmax(logits.float()).item()
        
    return last_token_embedding, predicted_token_id

class TextEmbeddingExtractor:
    def __init__(self, model_name="./Qwen2.5-7B-Instruct", device=None, verbose=False):
        """
        åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æå–å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡é…ç½®ï¼Œæ”¯æŒï¼š
                   - å­—ç¬¦ä¸²: 'cuda:0', 'auto', 'cpu'
                   - åˆ—è¡¨: ['cuda:0', 'cuda:1', ...]
                   - None: ä½¿ç”¨é»˜è®¤è®¾å¤‡cuda:5
        """
        self.model_name = model_name
        self.specified_device = device
        self.model = None
        self.tokenizer = None
        self.device = None
        self.verbose = verbose
        
        # å¤„ç†å¤šç§è®¾å¤‡é…ç½®
        if device is None:
            # ä¿æŒåŸæœ‰é»˜è®¤è¡Œä¸º
            self.use_auto_device = False
            self.primary_device = torch.device('cuda:5')
            self.multi_gpu_list = None
        elif isinstance(device, list):
            # å¤„ç†GPUåˆ—è¡¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºä¸»è®¾å¤‡
            if len(device) > 0:
                self.use_auto_device = False
                self.primary_device = torch.device(device[0])  # ä½¿ç”¨åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªGPU
                self.multi_gpu_list = device
                print(f"   ä½¿ç”¨å¤šGPUåˆ—è¡¨: {device}ï¼Œä¸»è®¾å¤‡: {device[0]}")
            else:
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
        elif isinstance(device, str):
            if device == "auto":
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
            else:
                self.use_auto_device = False
                self.primary_device = torch.device(device)
                self.multi_gpu_list = None
        else:
            self.use_auto_device = False
            self.primary_device = device
            self.multi_gpu_list = None
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - æ”¯æŒå¤šGPUé…ç½®"""
        if self.verbose:
            print("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹...")
            print(f"ğŸ¯ æŒ‡å®šè®¾å¤‡: {self.specified_device}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        )
        
        # æ ¹æ®è®¾å¤‡é…ç½®é€‰æ‹©device_map
        try:
            if self.use_auto_device:
                device_map = "auto"
                if self.verbose:
                    print("   ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")
            elif hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                # ä¸ºå¤šGPUåˆ›å»ºè®¾å¤‡æ˜ å°„
                device_map = "auto"  # è®©transformersè‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨GPU
                if self.verbose:
                    print(f"   ä½¿ç”¨å¤šGPUè‡ªåŠ¨åˆ†é…: {self.multi_gpu_list}")
                
                # å¯é€‰ï¼šè®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶å¯è§GPU
                import os
                if 'CUDA_VISIBLE_DEVICES' not in os.environ:
                    gpu_indices = [gpu.split(':')[1] for gpu in self.multi_gpu_list if gpu.startswith('cuda:')]
                    if gpu_indices:
                        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_indices)
                        if self.verbose:
                            print(f"   è®¾ç½®CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
                        
            elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                # å•GPUæŒ‡å®š
                device_index = int(self.specified_device.split(':')[1])
                device_map = {"": device_index}
                if self.verbose:
                    print(f"   ä½¿ç”¨æŒ‡å®šå•GPU: {self.specified_device}")
            elif self.specified_device == "cpu":
                # CPUè®¾å¤‡
                device_map = {"": "cpu"}
                if self.verbose:
                    print(f"   ä½¿ç”¨CPUè®¾å¤‡")
            else:
                # é»˜è®¤æƒ…å†µæˆ–å…¶ä»–è®¾å¤‡å­—ç¬¦ä¸²
                if hasattr(self, 'primary_device') and self.primary_device.type == 'cuda':
                    device_map = {"": self.primary_device.index}
                else:
                    device_map = "cuda:5"  # ä¿æŒåŸæœ‰é»˜è®¤å€¼
                if self.verbose:
                    print(f"   ä½¿ç”¨é»˜è®¤è®¾å¤‡æ˜ å°„: {device_map}")
            
            if self.verbose:
                print(f"   å®é™…ä½¿ç”¨è®¾å¤‡æ˜ å°„: {device_map}")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map=device_map,
                trust_remote_code=True
            )
            
            # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
            self.device = next(self.model.parameters()).device
            model_dtype = next(self.model.parameters()).dtype
            
            if self.verbose:
                print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                print(f"   å®é™…è®¾å¤‡: {self.device}")
                print(f"   æ•°æ®ç±»å‹: {model_dtype}")
            
            # æ˜¾ç¤ºè®¾å¤‡æ˜ å°„ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self.model, 'hf_device_map'):
                if self.verbose:
                    print(f"   è®¾å¤‡æ˜ å°„è¯¦æƒ…: {self.model.hf_device_map}")
                
        except Exception as e:
            if self.verbose:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•å›é€€åˆ°å•GPUæ¨¡å¼...")
            
            try:
                # ç¡®å®šå›é€€è®¾å¤‡
                if hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                    fallback_device = self.multi_gpu_list[0]
                elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                    fallback_device = self.specified_device
                else:
                    fallback_device = 'cuda:5'  # åŸæœ‰é»˜è®¤å€¼
                
                # æå–è®¾å¤‡ç´¢å¼•
                if fallback_device.startswith('cuda:'):
                    device_index = int(fallback_device.split(':')[1])
                    device_map = {"": device_index}
                else:
                    device_map = {"": "cpu"}
                
                if self.verbose:
                    print(f"   å›é€€è®¾å¤‡æ˜ å°„: {device_map}")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype="auto",
                    device_map=device_map,
                    trust_remote_code=True
                )
                
                self.device = next(self.model.parameters()).device
                if self.verbose:
                    print(f"âœ… ä½¿ç”¨å›é€€è®¾å¤‡åŠ è½½æˆåŠŸ: {self.device}")
                
            except Exception as fallback_error:
                if self.verbose:
                    print(f"âŒ å›é€€åŠ è½½ä¹Ÿå¤±è´¥: {fallback_error}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: åŸé”™è¯¯={e}, å›é€€é”™è¯¯={fallback_error}")
    
    def create_prompt(self, text):
        """åˆ›å»ºæç¤ºè¯"""
        return f'è¯·ç”¨ä¸€ä¸ªTokenè¡¨å¾"{text}"è¿™å¥è¯ï¼š'
    
    def extract_embeddings(self, texts):
        """æ‰¹é‡æå–åµŒå…¥å‘é‡"""
        if not texts:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•åµŒå…¥å‘é‡")
        
        if self.verbose:
            print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(texts)} æ¡æ–‡æœ¬...")
        
        embeddings_list = []
        texts_list = []
        prompts_list = []
        tokens_list = []
        
        with torch.no_grad():
            for i, text in enumerate(tqdm(texts, desc="æå–åµŒå…¥å‘é‡")):
                try:
                    # åˆ›å»ºæç¤ºè¯
                    prompt = self.create_prompt(text)
                    
                    # æå–åµŒå…¥å‘é‡
                    embedding, predicted_token_id = extract_last_token_embedding(
                        self.model, self.tokenizer, prompt, self.device
                    )
                    
                    # è§£ç é¢„æµ‹token
                    predicted_token = self.tokenizer.decode([predicted_token_id])
                    
                    # æ”¶é›†ç»“æœ
                    embeddings_list.append(embedding)
                    texts_list.append(text)
                    prompts_list.append(prompt)
                    tokens_list.append(predicted_token)
                        
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ–‡æœ¬ {i} æ—¶å‡ºé”™: {e}")
                    continue
        
        if not embeddings_list:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•åµŒå…¥å‘é‡")
        
        # å †å æ‰€æœ‰embeddings
        embeddings_tensor = torch.stack(embeddings_list)
        
        if self.verbose:
            print(f"âœ… æˆåŠŸæå– {len(embeddings_list)} ä¸ªåµŒå…¥å‘é‡")
            print(f"   å‘é‡å½¢çŠ¶: {embeddings_tensor.shape}")
            print(f"   æ•°æ®ç±»å‹: {embeddings_tensor.dtype}")
        
        # è¿”å›ç¬¦åˆtext_memory_train.pyæœŸæœ›çš„æ ¼å¼
        data = {
            'texts': texts_list,        # å¿…é¡»æœ‰è¿™ä¸ªå­—æ®µ
            'embeddings': embeddings_tensor, # å¿…é¡»æœ‰è¿™ä¸ªå­—æ®µ
            'prompts': prompts_list,
            'predicted_tokens': tokens_list,
            'metadata': {
                'model_name': self.model_name,
                'embedding_dim': embeddings_tensor.shape[-1],
                'num_samples': len(texts_list),
                'dtype': str(embeddings_tensor.dtype),
                'created_date': datetime.now().isoformat(),
                'device': str(self.device),
                'device_config': str(self.specified_device)
            }
        }
        
        return data
    
    def save_embeddings(self, data, save_dir="./embeddings", filename="text_embeddings.pt"):
        """ä¿å­˜embeddingsæ•°æ® - ç¡®ä¿æ ¼å¼åŒ¹é…"""
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, filename)
        
        # ç¡®ä¿ä¿å­˜çš„æ ¼å¼ä¸text_memory_train.pyæœŸæœ›çš„ä¸€è‡´
        save_data = {
            'texts': data['texts'],           # text_memory_train.pyéœ€è¦è¿™ä¸ªå­—æ®µ
            'embeddings': data['embeddings'], # text_memory_train.pyéœ€è¦è¿™ä¸ªå­—æ®µ
            'prompts': data.get('prompts', []),
            'predicted_tokens': data.get('predicted_tokens', []),
            'metadata': data.get('metadata', {}),
            'created_date': datetime.now().isoformat()
        }
        
        torch.save(save_data, save_path)
        if self.verbose:
            print(f"ğŸ’¾ Embeddingså·²ä¿å­˜åˆ°: {save_path}")
            print(f"   æ–‡æœ¬æ•°é‡: {len(save_data['texts'])}")
            print(f"   å‘é‡å½¢çŠ¶: {save_data['embeddings'].shape}")
        
        return save_path
    
    def load_embeddings(self, file_path=None):
        """åŠ è½½åµŒå…¥å‘é‡"""
        if file_path is None:
            paths = get_dataset_paths()
            file_path = os.path.join(paths['embeddings_dir'], 'text_embeddings.pt')
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if self.verbose:
            print(f"ğŸ“– åŠ è½½åµŒå…¥å‘é‡: {file_path}")
        data = torch.load(file_path, map_location='cpu')  # å…ˆåŠ è½½åˆ°CPU
        
        if self.verbose:
            print(f"âœ… åŠ è½½æˆåŠŸ:")
            print(f"   æ ·æœ¬æ•°é‡: {data['metadata']['num_samples']}")
            print(f"   åµŒå…¥ç»´åº¦: {data['metadata']['embedding_dim']}")
            print(f"   æ•°æ®ç±»å‹: {data['metadata']['dtype']}")
        
        return data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ–‡æœ¬åµŒå…¥å‘é‡æå–...")
    
    # 1. åŠ è½½æ–‡æœ¬æ•°æ®
    try:
        texts = load_text_dataset()
        print(f"ğŸ“– åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
    except FileNotFoundError:
        print("âŒ è¯·å…ˆè¿è¡Œ create_text_dataset.py åˆ›å»ºæ•°æ®é›†")
        return
    
    # 2. åˆå§‹åŒ–æå–å™¨ - å¯ä»¥æµ‹è¯•ä¸åŒçš„è®¾å¤‡é…ç½®
    # extractor = TextEmbeddingExtractor()  # é»˜è®¤è®¾å¤‡
    # extractor = TextEmbeddingExtractor(device='auto')  # è‡ªåŠ¨åˆ†é…
    extractor = TextEmbeddingExtractor(device=['cuda:5', 'cuda:6', 'cuda:7'])  # å¤šGPU
    
    # 3. æå–åµŒå…¥å‘é‡
    try:
        data = extractor.extract_embeddings(texts)
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        return
    
    # 4. ä¿å­˜æ•°æ®
    save_path = extractor.save_embeddings(data)
    
    # 5. éªŒè¯åŠ è½½
    print("\nğŸ”¬ éªŒè¯åŠ è½½...")
    try:
        loaded_data = extractor.load_embeddings()
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        original_shape = data['embeddings'].shape
        loaded_shape = loaded_data['embeddings'].shape
        
        print(f"   åŸå§‹å½¢çŠ¶: {original_shape}")
        print(f"   åŠ è½½å½¢çŠ¶: {loaded_shape}")
        print(f"   æ•°æ®ä¸€è‡´: {original_shape == loaded_shape}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
        print(f"\nğŸ“ ç¬¬ä¸€ä¸ªæ ·æœ¬:")
        print(f"   æ–‡æœ¬: {loaded_data['texts'][0][:100]}...")
        print(f"   é¢„æµ‹token: '{loaded_data['predicted_tokens'][0]}'")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
    
    print("\nğŸ‰ å®Œæˆ!")

if __name__ == "__main__":
    main()