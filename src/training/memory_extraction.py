# -*- coding: utf-8 -*-
"""
è®°å¿†æ¡ç›®/å‘é‡æå–ç›¸å…³çš„è¾…åŠ©å‡½æ•°ã€‚
ä»åŸ MemoryTrainingService ä¸­æ‹†åˆ†ï¼Œæ–¹ä¾¿ç‹¬ç«‹ç»´æŠ¤ä¸æµ‹è¯•ã€‚
"""

from __future__ import annotations

import logging
import os
import json
import random
import re
import gc
from typing import List, Dict, Any, Tuple, Optional

import requests
from PIL import UnidentifiedImageError
import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
from training.model_utils import forward_backbone, ensure_last_hidden_state

_log = logging.getLogger(__name__)


def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """
    ä»last_hidden_statesä¸­æå–æœ€åä¸€ä¸ªtokençš„hidden state
    å‚è€ƒQwen3-Embeddingçš„å®˜æ–¹å®ç°
    """
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]


class EmbeddingModelContext:
    """Embeddingæ¨¡å‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ç®¡ç†embeddingæ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸ"""
    
    def __init__(self, model_path: str, device, multi_gpu_config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–embeddingæ¨¡å‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            model_path: embeddingæ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆå¦‚ "cuda:0" æˆ– ["cuda:0", "cuda:1"]ï¼‰
            multi_gpu_config: å¤šGPUé…ç½®ï¼ˆå¯é€‰ï¼‰
        """
        self.model_path = model_path
        self.device = device
        self.multi_gpu_config = multi_gpu_config or {}
        self.model = None
        self.tokenizer = None
    
    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ï¼ŒåŠ è½½embeddingæ¨¡å‹"""
        _log.info(f"åŠ è½½embeddingæ¨¡å‹: {self.model_path}")
        self.model, self.tokenizer = self._load_embedding_model()
        return self.model, self.tokenizer
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ï¼Œå½»åº•æ¸…ç†æ¨¡å‹"""
        _log.info("æ¸…ç†embeddingæ¨¡å‹ä¸Šä¸‹æ–‡...")
        
        try:
            # æ¸…ç†æ¨¡å‹
            if self.model is not None:
                try:
                    self.model.cpu()
                except Exception as e:
                    _log.warning(f"ç§»åŠ¨embeddingæ¨¡å‹åˆ°CPUæ—¶å‡ºç°è­¦å‘Š: {e}")
                del self.model
                self.model = None
            
            # æ¸…ç†tokenizer
            if self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶å’Œæ˜¾å­˜æ¸…ç†
            for _ in range(3):
                gc.collect()
            
            # æ¸…ç†GPUæ˜¾å­˜
            if torch.cuda.is_available():
                if isinstance(self.device, str) and self.device.startswith("cuda:"):
                    try:
                        device_idx = int(self.device.split(":")[1])
                        with torch.cuda.device(device_idx):
                            torch.cuda.synchronize()
                            torch.cuda.empty_cache()
                    except Exception:
                        pass
                torch.cuda.empty_cache()
            
            _log.info("âœ… embeddingæ¨¡å‹ä¸Šä¸‹æ–‡æ¸…ç†å®Œæˆ")
        
        except Exception as cleanup_error:
            _log.warning(f"embeddingæ¨¡å‹ä¸Šä¸‹æ–‡æ¸…ç†æ—¶å‡ºç°é”™è¯¯: {cleanup_error}")
        
        return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸
    
    def _load_embedding_model(self):
        """åŠ è½½embeddingæ¨¡å‹ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰- å‚è€ƒè®­ç»ƒæ¨¡å‹çš„åŠ è½½æ–¹å¼"""
        # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(self.model_path):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(script_dir))
            model_path = os.path.abspath(os.path.join(project_root, self.model_path))
        else:
            model_path = self.model_path
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        is_local_path = os.path.exists(model_path) and os.path.isdir(model_path)
        
        _log.info(f"ä»è·¯å¾„åŠ è½½embeddingæ¨¡å‹: {model_path} (æœ¬åœ°: {is_local_path})")
        
        # åŠ è½½tokenizerï¼ˆéœ€è¦è®¾ç½®padding_side='left'ï¼‰
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=is_local_path,
            padding_side='left'
        )
        
        # å‡†å¤‡åŠ è½½å‚æ•°ï¼ˆå‚è€ƒè®­ç»ƒæ¨¡å‹çš„åŠ è½½æ–¹å¼ï¼‰
        load_kwargs = {
            "trust_remote_code": True,
            "local_files_only": is_local_path,
            "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
        }
        
        # æ£€æŸ¥CUDA_VISIBLE_DEVICESè®¾ç½®çŠ¶æ€
        cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
        cuda_visible_set = bool(cuda_visible)
        
        # æ ¹æ®è®¾å¤‡é…ç½®å†³å®šdevice_mapï¼ˆå‚è€ƒè®­ç»ƒæ¨¡å‹çš„é€»è¾‘ï¼‰
        multi_gpu_enabled = self.multi_gpu_config.get("enabled", False)
        
        if isinstance(self.device, list) and multi_gpu_enabled:
            # å¤šGPUé…ç½®ï¼ˆembeddingæ¨¡å‹é€šå¸¸ä¸éœ€è¦å¤šGPUï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
            from training.training_service import _optimize_multi_gpu_allocation
            max_memory_config = self.multi_gpu_config.get("max_memory", {})
            allocation = _optimize_multi_gpu_allocation(self.device, max_memory_config, cuda_visible_set=cuda_visible_set)
            load_kwargs["device_map"] = allocation["device_map"]
            if allocation["max_memory"]:
                load_kwargs["max_memory"] = allocation["max_memory"]
            _log.info(f"ğŸ”§ embeddingæ¨¡å‹: å¤šGPUæ¨¡å¼ï¼Œè®¾å¤‡ {self.device}")
        elif isinstance(self.device, str) and self.device.startswith("cuda"):
            # å•GPUé…ç½®ï¼ˆå‚è€ƒè®­ç»ƒæ¨¡å‹çš„é€»è¾‘ï¼‰
            if cuda_visible_set and cuda_visible:
                # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                device_map_device = "cuda:0"
                _log.info(f"ğŸ”§ embeddingæ¨¡å‹: å•GPUæ¨¡å¼ï¼ŒCUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {device_map_device}ï¼ˆå¯¹åº”ç‰©ç†GPU {self.device}ï¼‰")
            else:
                # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç›´æ¥ä½¿ç”¨ç‰©ç†è®¾å¤‡
                device_map_device = self.device
                _log.info(f"ğŸ”§ embeddingæ¨¡å‹: å•GPUæ¨¡å¼ï¼Œè®¾å¤‡æ˜ å°„åˆ° {self.device}")
            load_kwargs["device_map"] = {"": device_map_device}
        else:
            # é»˜è®¤ä½¿ç”¨auto
            load_kwargs["device_map"] = "auto"
            _log.info("ğŸ”§ embeddingæ¨¡å‹: ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModel.from_pretrained(
            model_path,
            **load_kwargs
        )
        
        model.eval()
        actual_device = next(model.parameters()).device
        _log.info(f"âœ… embeddingæ¨¡å‹åŠ è½½å®Œæˆï¼Œå®é™…è®¾å¤‡: {actual_device}")
        
        return model, tokenizer


def _strip_formal_reply(generated_text: str) -> Tuple[str, bool, Optional[str]]:
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­æˆªå–æœ€åä¸€ä¸ª </think>/<thinking> æ ‡ç­¾ä¹‹åçš„æ­£å¼å›ç­”ã€‚
    è¿”å› (trimmed_text, trimmed_flag, matched_tag)ã€‚
    """
    if not generated_text:
        return generated_text, False, None

    thinking_patterns = [
        r"</think\s*>",
        r"</thinking\s*>",
    ]
    last_match = None
    for pattern in thinking_patterns:
        matches = list(re.finditer(pattern, generated_text, flags=re.IGNORECASE))
        if matches:
            candidate = matches[-1]
            if last_match is None or candidate.end() > last_match.end():
                last_match = candidate

    if last_match:
        trimmed = generated_text[last_match.end():].strip()
        return trimmed, True, last_match.group(0).strip()

    return generated_text, False, None


def extract_memory_entries(
    service,
    chat_messages: List[Dict[str, Any]],
    model=None,
    processor=None,
) -> Optional[str]:
    """
    æå–è®°å¿†æ¡ç›®å¹¶ç”Ÿæˆç›‘ç£å‘é‡ï¼Œç›´æ¥ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    
    æ³¨æ„ï¼šç¬¬ä¸€é˜¶æ®µï¼ˆæå–è®°å¿†æ–‡æœ¬ï¼‰ä»ä½¿ç”¨æ™®é€šæ¨¡å‹ï¼Œç¬¬äºŒé˜¶æ®µï¼ˆæå–å‘é‡ï¼‰ä½¿ç”¨embeddingæ¨¡å‹

    Args:
        service: MemoryTrainingService å®ä¾‹
        chat_messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
        model: æ™®é€šæ¨¡å‹ï¼ˆç”¨äºç¬¬ä¸€é˜¶æ®µæå–è®°å¿†æ–‡æœ¬ï¼‰
        processor: processorï¼ˆç”¨äºç¬¬ä¸€é˜¶æ®µæå–è®°å¿†æ–‡æœ¬ï¼‰

    Returns:
        ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    """
    self = service
    _log.info("å¼€å§‹æå–è®°å¿†æ¡ç›®...")

    # æŒ‰èŠå¤©åˆ†ç»„
    chat_groups = {}
    for msg_data in chat_messages:
        chat_type = msg_data.get("chat_type", "unknown")
        chat_id = msg_data.get("chat_id", "unknown")
        message = msg_data.get("message", {})

        key = f"{chat_type}_{chat_id}"
        if key not in chat_groups:
            chat_groups[key] = []
        chat_groups[key].append(message)

    _log.info(f"å…± {len(chat_groups)} ä¸ªèŠå¤©ç»„")

    # ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼ˆåªåŒ…å«è®°å¿†æ¡ç›®æ–‡æœ¬ï¼‰
    temp_texts_path = os.path.join(self.memory_db_dir, "temp_memory_texts.pt")
    _log.debug(f"è®°å¿†æ¡ç›®å°†ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_texts_path}")

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æå–å¥½çš„è®°å¿†æ¡ç›®æ–‡ä»¶
    skip_stage1 = False
    if os.path.exists(temp_texts_path):
        all_memory_texts = _load_memory_texts_from_file(self, temp_texts_path)
        if all_memory_texts:
            _log.info(f"âœ… æ£€æµ‹åˆ°å·²æœ‰çš„è®°å¿†æ¡ç›®æ–‡ä»¶ï¼ŒåŒ…å« {len(all_memory_texts)} ä¸ªè®°å¿†æ¡ç›®")
            _log.info("â­ï¸  è·³è¿‡ç¬¬ä¸€é˜¶æ®µï¼ˆè®°å¿†æ¡ç›®æ–‡æœ¬æå–ï¼‰ï¼Œç›´æ¥è¿›å…¥ç¬¬äºŒé˜¶æ®µï¼ˆå‘é‡æå–ï¼‰")
            skip_stage1 = True
        else:
            _log.info("âš ï¸ æ£€æµ‹åˆ°è®°å¿†æ¡ç›®æ–‡ä»¶ä½†å†…å®¹ä¸ºç©ºï¼Œå°†é‡æ–°æå–")
            skip_stage1 = False

    # ä»é…ç½®ä¸­è·å–æœ€å¤§tokené™åˆ¶ï¼ˆç”¨äºæ‰¹é‡æå–å‘é‡æ—¶çš„æˆªæ–­ï¼‰
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å€¼ 35000
    max_tokens = self.training_config.get("max_tokens_for_embedding", 35000)
    _log.debug(f"ä½¿ç”¨æœ€å¤§tokené™åˆ¶: {max_tokens}ï¼ˆç”¨äºæ‰¹é‡æå–å‘é‡æ—¶çš„æˆªæ–­ï¼‰")

    if not skip_stage1:
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ¨¡å‹å’Œprocessorï¼ˆç”¨äºç¬¬ä¸€é˜¶æ®µï¼‰
        if model is None or processor is None:
            _log.error("âŒ extract_memory_entrieséœ€è¦æä¾›modelå’Œprocessorå‚æ•°ï¼ˆç”¨äºç¬¬ä¸€é˜¶æ®µæå–è®°å¿†æ–‡æœ¬ï¼‰")
            return None

        _log.info("ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œè®°å¿†æ–‡æœ¬æå–")

        # è§’è‰²è®¾å®šï¼ˆç”¨äºè®°å¿†æå–æ—¶æé†’æ¨¡å‹è‡ªå·±çš„èº«ä»½ï¼‰
        role_playing_prompt = ""
        extraction_prompts = getattr(self, "memory_extraction_prompts", {}) or {}
        try:
            role_playing_prompt = self.config.get("prompt", {}).get("role_playing", "")
            if role_playing_prompt:
                role_playing_prompt = role_playing_prompt.strip()
        except Exception:
            role_playing_prompt = ""

    try:
        if not skip_stage1:
            # å¯¹æ¯ä¸ªèŠå¤©ç»„è¿›è¡Œæ€»ç»“ï¼ˆé€’å½’å¤„ç†ï¼Œæ”¯æŒå¯¹åŠåˆ†ï¼‰
            def process_chat_group(messages: List[Dict[str, Any]], chat_key: str, depth: int = 0):
                """
                å¤„ç†å•ä¸ªèŠå¤©ç»„ï¼ˆé€’å½’å‡½æ•°ï¼Œæ”¯æŒå¯¹åŠåˆ†ï¼‰
                """
                if not messages:
                    return

                # æ„å»ºæ ‡å‡†æ ¼å¼çš„èŠå¤©å†å²ï¼ˆä¿ç•™å¤šæ¨¡æ€ä¿¡æ¯ï¼‰
                chat_messages_for_extraction = []
                for msg in messages:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")  # é»˜è®¤å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä¸æ—§ç‰ˆæœ¬ä¿æŒä¸€è‡´

                    # ä¿æŒåŸå§‹contentæ ¼å¼ï¼ˆå¯èƒ½æ˜¯listï¼ŒåŒ…å«å›¾ç‰‡ä¿¡æ¯ï¼‰
                    if isinstance(content, list):
                        _log.debug(f"ğŸ” èŠå¤©ç»„ {chat_key} æ¶ˆæ¯ {role} çš„contentæ˜¯åˆ—è¡¨ï¼ŒåŒ…å« {len(content)} é¡¹")
                        # å¤šæ¨¡æ€å†…å®¹ï¼Œéœ€è¦éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ
                        filtered_content = []
                        image_count = 0
                        valid_image_count = 0
                        for item in content:
                            if item.get("type") == "text":
                                filtered_content.append(item)
                            elif item.get("type") == "image":
                                image_url = item.get("image", "")
                                image_count += 1
                                if image_url:
                                    if image_url.startswith('http://') or image_url.startswith('https://'):
                                        filtered_content.append(item)
                                        valid_image_count += 1
                                    else:
                                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å›¾ç‰‡URLæ ¼å¼æ— æ•ˆï¼Œè·³è¿‡")
                                else:
                                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å‘ç°æ— æ•ˆçš„å›¾ç‰‡é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                            elif item.get("type") == "video":
                                video_url = item.get("video") or item.get("url")
                                if not video_url:
                                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å‘ç°æ— æ•ˆçš„è§†é¢‘é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                                    continue

                                is_local_server_url = (
                                    video_url.startswith('http://127.0.0.1:9999/static/videos/') or
                                    video_url.startswith('http://localhost:9999/static/videos/') or
                                    (self.server_base_url and video_url.startswith(f"{self.server_base_url.rstrip('/')}/static/videos/"))
                                )
                                is_local_file = os.path.exists(video_url) and os.path.isfile(video_url)
                                is_file_url = video_url.startswith('file://') and os.path.exists(video_url[7:])

                                _log.debug(f"ğŸ” è§†é¢‘URLæ£€æŸ¥: {video_url}")
                                _log.debug(f"  is_local_server_url: {is_local_server_url}")
                                _log.debug(f"  is_local_file: {is_local_file} (æ–‡ä»¶å­˜åœ¨: {os.path.exists(video_url) if video_url else False})")
                                _log.debug(f"  is_file_url: {is_file_url}")
                                _log.debug(f"  is_http: {video_url.startswith('http://') or video_url.startswith('https://') if video_url else False}")

                                if is_local_server_url or is_local_file or is_file_url or video_url.startswith('http://') or video_url.startswith('https://'):
                                    filtered_content.append({
                                        "type": "video",
                                        "video": video_url
                                    })
                                    _log.info(f"âœ… ä¿ç•™è§†é¢‘: {video_url}")
                                else:
                                    _log.warning(f"âš ï¸ ç§»é™¤æ— æ•ˆè§†é¢‘URL: {video_url}")

                        if filtered_content:
                            img_count = sum(1 for item in filtered_content if item.get("type") == "image")
                            vid_count = sum(1 for item in filtered_content if item.get("type") == "video")
                            if img_count > 0 or vid_count > 0:
                                _log.info(f"ğŸ“Š èŠå¤©ç»„ {chat_key} æ¶ˆæ¯åŒ…å« {img_count} å¼ å›¾ç‰‡å’Œ {vid_count} ä¸ªè§†é¢‘")
                                for item in filtered_content:
                                    if item.get("type") == "image":
                                        _log.info(f"   ğŸ“· å›¾ç‰‡URL: {item.get('image', '')}")
                                    elif item.get("type") == "video":
                                        _log.info(f"   ğŸ¥ è§†é¢‘URL: {item.get('video', '')}")
                            chat_messages_for_extraction.append({
                                "role": role,
                                "content": filtered_content
                            })
                        else:
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯è¿‡æ»¤åæ— å†…å®¹ï¼Œè·³è¿‡è¯¥æ¶ˆæ¯")
                    elif isinstance(content, str):
                        chat_messages_for_extraction.append({
                            "role": role,
                            "content": [{"type": "text", "text": content}]
                        })
                    else:
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯contentæ ¼å¼æœªçŸ¥: {type(content)}ï¼Œè·³è¿‡")

                extraction_system_prompt = _build_extraction_prompt(role_playing_prompt, extraction_prompts)

                if not chat_messages_for_extraction:
                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å¤„ç†åæ— æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡")
                    return

                if len(chat_messages_for_extraction) > self.training_config.get("max_messages_per_group", 80):
                    mid = len(messages) // 2
                    _log.debug(f"âœ‚ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯è¿‡é•¿ï¼Œæ‹†åˆ†ä¸ºä¸¤ä¸ªå­ç»„ï¼ˆæ·±åº¦ {depth + 1}ï¼‰")
                    process_chat_group(messages[:mid], f"{chat_key}_part1", depth + 1)
                    process_chat_group(messages[mid:], f"{chat_key}_part2", depth + 1)
                    return

                try:
                    _log.info(f"ğŸ§  å¼€å§‹å¤„ç†èŠå¤©ç»„ {chat_key}ï¼ˆæ·±åº¦ {depth}ï¼‰...")
                    _log.info(f"   æ¶ˆæ¯æ¡æ•°: {len(chat_messages_for_extraction)}")

                    if depth > 0:
                        child_prompt = self.training_config.get("child_depth_prompt")
                        if not child_prompt:
                            child_prompt = extraction_prompts.get("child_depth_prompt")
                        if child_prompt:
                            extraction_system_prompt += "\n\n" + child_prompt
                    
                    media_instruction = extraction_prompts.get("media_instruction")
                    if media_instruction:
                        extraction_system_prompt += f"\n\n{media_instruction}"
                    
                    activation_instruction = self.training_config.get("memory_activation_prompt")
                    if not activation_instruction:
                        activation_instruction = extraction_prompts.get("memory_activation_prompt")
                    if activation_instruction:
                        extraction_system_prompt += f"\n\nè®°å¿†æ¿€æ´»æç¤ºï¼š{activation_instruction}"
                    
                    user_prompt = extraction_prompts.get("user_prompt", "è¯·å¼€å§‹æå–è®°å¿†æ¡ç›®ã€‚")

                    full_messages = [
                        {"role": "system", "content": [{"type": "text", "text": extraction_system_prompt}]}
                    ]
                    full_messages.extend(chat_messages_for_extraction)
                    full_messages.append({
                        "role": "user",
                        "content": [{"type": "text", "text": user_prompt}]
                    })

                    try:
                        inputs = processor.apply_chat_template(
                            full_messages,
                            tokenize=True,
                            add_generation_prompt=True,
                            return_dict=True,
                            return_tensors="pt"
                        )

                        input_ids_text = processor.batch_decode(
                            inputs["input_ids"],
                            skip_special_tokens=False,
                            clean_up_tokenization_spaces=False
                        )
                        _log.info("=" * 80)
                        _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
                        _log.info(input_ids_text[0])
                        _log.info("=" * 80)
                    except (UnidentifiedImageError, OSError, requests.RequestException, Exception) as media_error:
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å›¾ç‰‡/è§†é¢‘å¤„ç†å¤±è´¥: {media_error}")
                        _log.warning(f"   é”™è¯¯ç±»å‹: {type(media_error).__name__}", exc_info=True)
                        _log.info("   ğŸ”„ è‡ªåŠ¨é™çº§ï¼šç§»é™¤æ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘ï¼Œåªä½¿ç”¨æ–‡æœ¬å†…å®¹è¿›è¡Œè®°å¿†æå–...")

                        text_only_messages = []
                        for msg in full_messages:
                            msg_content = msg.get("content", [])
                            if isinstance(msg_content, list):
                                text_items = [item for item in msg_content if item.get("type") == "text"]
                                if text_items:
                                    text_only_messages.append({
                                        "role": msg.get("role", "user"),
                                        "content": text_items
                                    })
                            else:
                                text_only_messages.append(msg)

                        if not text_only_messages:
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} ç§»é™¤å¤šæ¨¡æ€å†…å®¹åæ— æœ‰æ•ˆæ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                            return

                        inputs = processor.apply_chat_template(
                            text_only_messages,
                            tokenize=True,
                            add_generation_prompt=True,
                            return_dict=True,
                            return_tensors="pt"
                        )

                    input_length = inputs["input_ids"].shape[-1]
                    _log.info(f"ğŸ“Š èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) è¾“å…¥tokené•¿åº¦: {input_length}, æœ€å¤§é™åˆ¶: {max_tokens}")

                    if input_length > max_tokens:
                        if len(messages) <= 1:
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} ä»…ä¸€æ¡æ¶ˆæ¯ä½†tokené•¿åº¦ {input_length} è¶…è¿‡é™åˆ¶ {max_tokens}ï¼Œè·³è¿‡å¤„ç†")
                            return

                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) è¾“å…¥tokené•¿åº¦è¶…é™ï¼Œæ‹†åˆ†åŸå§‹æ¶ˆæ¯é‡æ–°å¤„ç†")
                        half_point = len(messages) // 2
                        process_chat_group(messages[:half_point], f"{chat_key}_part1", depth + 1)
                        process_chat_group(messages[half_point:], f"{chat_key}_part2", depth + 1)
                        return

                    inputs = {k: v.to(model.device) if hasattr(v, "to") else v for k, v in inputs.items()}

                    gen_config = self.config.get("generation", {})
                    max_new_tokens = gen_config.get("max_new_tokens", 1000)
                    temperature = gen_config.get("temperature", 1.0)
                    top_p = gen_config.get("top_p", 0.95)
                    top_k = gen_config.get("top_k", 20)
                    repetition_penalty = gen_config.get("repetition_penalty", 1.0)

                    _log.info(
                        "ğŸ¯ è®°å¿†æå–ç”Ÿæˆå‚æ•°: max_new_tokens=%s, temperature=%s, top_p=%s, top_k=%s, repetition_penalty=%s",
                        max_new_tokens,
                        temperature,
                        top_p,
                        top_k,
                        repetition_penalty,
                    )

                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=max_new_tokens,
                            temperature=temperature,
                            top_p=top_p,
                            top_k=top_k,
                            repetition_penalty=repetition_penalty,
                            do_sample=True,
                        )

                    generated_text = processor.batch_decode(
                        outputs[:, inputs["input_ids"].shape[1]:],
                        skip_special_tokens=False,
                        clean_up_tokenization_spaces=False
                    )[0]
                    generated_text = generated_text.strip()

                    generated_text, trimmed, matched_tag = _strip_formal_reply(generated_text)
                    if trimmed:
                        tag_text = matched_tag or "</think>"
                        _log.info(f"âœ… ä»æ¨¡å‹è¾“å‡ºä¸­æå–åˆ°æ­£å¼å›ç­”ï¼ˆæˆªå–æœ€åä¸€ä¸ª{tag_text}ä¹‹åçš„å†…å®¹ï¼‰")
                    else:
                        _log.warning("âš ï¸ æœªæ‰¾åˆ°</think>æˆ–</thinking>æ ‡ç­¾ï¼Œä½¿ç”¨å®Œæ•´è¾“å‡º")

                    _log.info(f"ğŸ“ èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) æ¨¡å‹æ­£å¼å›ç­”ï¼ˆé•¿åº¦ {len(generated_text)}ï¼‰:")
                    _log.info(generated_text)

                    memory_texts = _parse_memory_entries(self, generated_text)

                    _log.info(f"ğŸ“Š è§£æåæå–åˆ° {len(memory_texts)} ä¸ªè®°å¿†æ¡ç›®")
                    if memory_texts:
                        for i, mem_text in enumerate(memory_texts, 1):
                            _log.info(f"   è®°å¿†æ¡ç›® {i}: {mem_text[:100]}...")

                    for memory_text in memory_texts:
                        _append_memory_text_to_file(self, memory_text, temp_texts_path)
                        _log.info(f"âœ… æå–è®°å¿†æ¡ç›®æ–‡æœ¬ (æ·±åº¦ {depth}): {memory_text[:80]}...")

                except Exception as e:
                    _log.warning(f"å¤„ç†èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) æ—¶å‡ºé”™: {e}", exc_info=True)
                    return

            for chat_key, messages in chat_groups.items():
                process_chat_group(messages, chat_key)

            if not os.path.exists(temp_texts_path):
                _log.warning("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•è®°å¿†æ¡ç›®")
                return None

            all_memory_texts = _load_memory_texts_from_file(self, temp_texts_path)
            if not all_memory_texts:
                _log.warning("âš ï¸ ä¸´æ—¶æ–‡ä»¶ä¸­æ²¡æœ‰è®°å¿†æ¡ç›®")
                return None
        else:
            # ä½¿ç”¨å·²æœ‰çš„è®°å¿†æ¡ç›®æ–‡ä»¶
            all_memory_texts = _load_memory_texts_from_file(self, temp_texts_path)

        _log.info(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼šå…±æå– {len(all_memory_texts)} ä¸ªè®°å¿†æ¡ç›®æ–‡æœ¬")
        _log.info("=" * 60)
        _log.info("å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡æå–è®°å¿†æ¡ç›®å‘é‡ï¼ˆä½¿ç”¨Qwen3-Embeddingæ¨¡å‹ï¼‰")
        _log.info("=" * 60)

        # è·å–embeddingæ¨¡å‹è·¯å¾„å’Œè®¾å¤‡
        embedding_model_path = self.memory_config.get("embedding_model_path")
        if not embedding_model_path:
            _log.error("âŒ æœªé…ç½®embedding_model_pathï¼Œæ— æ³•æå–å‘é‡")
            return None
        
        # è·å–å¤šGPUé…ç½®ï¼ˆç”¨äºembeddingæ¨¡å‹åŠ è½½ï¼‰
        multi_gpu_config = self.config.get("model", {}).get("multi_gpu", {})
        
        # ä½¿ç”¨embeddingæ¨¡å‹ä¸Šä¸‹æ–‡æå–å‘é‡ï¼ˆå‚è€ƒè®­ç»ƒæ¨¡å‹çš„åŠ è½½æ–¹å¼ï¼‰
        with EmbeddingModelContext(embedding_model_path, self.device, multi_gpu_config) as (embedding_model, embedding_tokenizer):
            all_texts, all_embeddings = _batch_extract_embeddings(
                self, all_memory_texts, embedding_model, embedding_tokenizer, max_tokens
            )

        if all_texts and all_embeddings:
            _save_training_data_batch(self, all_texts, all_embeddings)
            _log.info(f"âœ… æˆåŠŸä¿å­˜ {len(all_texts)} ä¸ªè®°å¿†æ¡ç›®åŠå…¶å‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶")

            try:
                if os.path.exists(temp_texts_path):
                    os.remove(temp_texts_path)
                    _log.info(f"âœ… å·²åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶: temp_memory_texts.pt")
            except Exception as e:
                _log.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
        else:
            _log.warning("âŒ æ²¡æœ‰æˆåŠŸæå–åˆ°å‘é‡")
            return None

        temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
        if os.path.exists(temp_data_path):
            data = torch.load(temp_data_path, map_location='cpu')
            total_entries = len(data.get('texts', []))
            _log.info(f"âœ… æˆåŠŸæå–å¹¶ä¿å­˜ {total_entries} ä¸ªè®°å¿†æ¡ç›®åˆ°ä¸´æ—¶æ–‡ä»¶")
        else:
            _log.warning("âŒ æ²¡æœ‰ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶")
            return None

        return temp_data_path

    finally:
        if 'all_texts' in locals():
            del all_texts
        if 'all_embeddings' in locals():
            del all_embeddings

        temp_texts_path = os.path.join(self.memory_db_dir, "temp_memory_texts.pt")
        if os.path.exists(temp_texts_path):
            try:
                temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
                if os.path.exists(temp_data_path):
                    os.remove(temp_texts_path)
                    _log.debug(f"æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶: temp_memory_texts.pt")
            except Exception as e:
                _log.debug(f"æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰: {e}")

        _log.info("âœ… è®°å¿†æå–å®Œæˆï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹ï¼‰")


def extract_sft_vectors_for_recall_training(
    service,
    num_memory_entries: int,
    model=None,
    processor=None
) -> Optional[str]:
    """
    æå–ç­‰é‡çš„SFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒ
    
    æ³¨æ„ï¼šç°åœ¨ä½¿ç”¨Qwen3-Embeddingæ¨¡å‹æå–å‘é‡ï¼Œä½†æå–SFTæ€è€ƒå†…å®¹æ—¶ä»éœ€è¦processoræ¥æ ¼å¼åŒ–æ¶ˆæ¯
    """
    self = service
    try:
        if not self.sft_enabled or not self.sft_path:
            _log.info("â„¹ï¸ SFTæœªå¯ç”¨æˆ–æœªé…ç½®ï¼Œè·³è¿‡SFTå‘é‡æå–")
            return None

        required_sft_count = int(num_memory_entries * 1.5)
        _log.info(f"ğŸ§ª å¼€å§‹æå– {required_sft_count} ä¸ªSFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒï¼ˆè®°å¿†æ¡ç›®æ•°: {num_memory_entries}ï¼‰")

        sft_samples = _load_sft_dataset(self)
        if not sft_samples:
            _log.warning("âš ï¸ æ— æ³•åŠ è½½SFTæ•°æ®é›†ï¼Œè·³è¿‡SFTå‘é‡æå–")
            return None

        # å¦‚æœæ²¡æœ‰ä¼ é€’processorï¼Œä»base_model_pathåŠ è½½processorï¼ˆåªåŠ è½½processorï¼Œä¸åŠ è½½æ¨¡å‹ï¼‰
        if processor is None:
            from transformers import AutoProcessor
            _log.info("ğŸ“¦ åŠ è½½processorç”¨äºSFTæ¶ˆæ¯æ ¼å¼åŒ–...")
            base_model_path = self.base_model_path
            if not os.path.isabs(base_model_path):
                script_dir = os.path.dirname(os.path.abspath(__file__))
                project_root = os.path.dirname(os.path.dirname(script_dir))
                base_model_path = os.path.abspath(os.path.join(project_root, base_model_path))
            
            is_local_path = os.path.exists(base_model_path) and os.path.isdir(base_model_path)
            processor = AutoProcessor.from_pretrained(
                base_model_path,
                trust_remote_code=True,
                local_files_only=is_local_path
            )
            _log.info("âœ… processoråŠ è½½å®Œæˆ")

        max_tokens = int(service.training_config.get("sft_max_tokens") or 0)
        tokenizer = service._get_base_tokenizer(processor)
        sft_thinking_texts = []
        collected = 0
        random.shuffle(sft_samples)
        processed = 0
        no_messages_count = 0
        no_tag_count = 0
        empty_content_count = 0
        token_exceeded_count = 0
        
        for sample in sft_samples:
            messages = _standardize_sft_messages(self, sample)
            if not messages:
                no_messages_count += 1
                continue
            processed += 1
            try:
                full_text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )

                start_tag = "<think>"
                end_tag = "</think>"
                start_idx = full_text.find(start_tag)
                end_idx = full_text.find(end_tag)

                if start_idx == -1 or end_idx == -1:
                    no_tag_count += 1
                    if processed <= 5:  # åªè®°å½•å‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                        _log.debug(f"æ ·æœ¬ {processed} æœªæ‰¾åˆ°æ ‡ç­¾: start_idx={start_idx}, end_idx={end_idx}")
                    continue
                
                thinking_content = full_text[start_idx + len(start_tag):end_idx].strip()
                if not thinking_content:
                    empty_content_count += 1
                    continue
                    
                if max_tokens:
                    encoded = tokenizer(
                        thinking_content,
                        return_tensors="pt",
                        add_special_tokens=True,
                        padding=False,
                        truncation=False
                    )
                    if encoded["input_ids"].shape[1] > max_tokens:
                        token_exceeded_count += 1
                        continue
                        
                sft_thinking_texts.append(thinking_content)
                collected += 1
            except Exception as e:
                _log.debug(f"å¤„ç†SFTæ ·æœ¬å¤±è´¥: {e}", exc_info=True)
                continue
            if collected >= required_sft_count:
                break

        _log.info(f"ğŸ“Š SFTæ ·æœ¬å¤„ç†ç»Ÿè®¡: å¤„ç†äº† {processed} ä¸ªæ ·æœ¬ï¼Œæ”¶é›†åˆ° {collected} ä¸ªæœ‰æ•ˆæ€è€ƒå†…å®¹")
        _log.info(f"   æ— æ¶ˆæ¯: {no_messages_count}, æ— æ ‡ç­¾: {no_tag_count}, ç©ºå†…å®¹: {empty_content_count}, tokenè¶…é™: {token_exceeded_count}")

        if not sft_thinking_texts:
            _log.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„SFTæ€è€ƒå†…å®¹ï¼Œè·³è¿‡SFTå‘é‡æå–")
            _log.warning(f"   å¤„ç†äº† {processed} ä¸ªæ ·æœ¬ï¼Œä½†æœªæ‰¾åˆ°åŒ…å« <think> æ ‡ç­¾çš„æœ‰æ•ˆå†…å®¹")
            return None

        if len(sft_thinking_texts) < required_sft_count:
            raise ValueError(
                f"SFTæ€è€ƒæ ·æœ¬ä¸è¶³ï¼šéœ€è¦ {required_sft_count} æ¡æ»¡è¶³tokené™åˆ¶çš„thinkingæ®µï¼Œ"
                f"ä½†ä»…æ”¶é›†åˆ° {len(sft_thinking_texts)} æ¡ã€‚"
            )

        _log.info(f"âœ… æå–åˆ° {len(sft_thinking_texts)} ä¸ªSFTæ€è€ƒå†…å®¹")

        # è·å–embeddingæ¨¡å‹è·¯å¾„å’Œè®¾å¤‡
        embedding_model_path = self.memory_config.get("embedding_model_path")
        if not embedding_model_path:
            _log.error("âŒ æœªé…ç½®embedding_model_pathï¼Œæ— æ³•æå–SFTå‘é‡")
            return None
        
        # è·å–å¤šGPUé…ç½®ï¼ˆç”¨äºembeddingæ¨¡å‹åŠ è½½ï¼‰
        multi_gpu_config = self.config.get("model", {}).get("multi_gpu", {})
        
        # ä½¿ç”¨embeddingæ¨¡å‹ä¸Šä¸‹æ–‡æå–å‘é‡ï¼ˆå‚è€ƒè®­ç»ƒæ¨¡å‹çš„åŠ è½½æ–¹å¼ï¼‰
        with EmbeddingModelContext(embedding_model_path, self.device, multi_gpu_config) as (embedding_model, embedding_tokenizer):
            sft_texts, sft_embeddings = _batch_extract_embeddings(
                self,
                sft_thinking_texts,
                embedding_model,
                embedding_tokenizer,
                self.training_config.get("max_tokens_for_embedding", 35000)
            )

        if not sft_embeddings or len(sft_embeddings) < required_sft_count:
            _log.warning("âš ï¸ SFTå‘é‡æå–å¤±è´¥")
            return None

        sft_vectors_path = os.path.join(self.memory_db_dir, "temp_sft_vectors.pt")
        torch.save({
            "texts": sft_texts,
            "embeddings": torch.stack(sft_embeddings)
        }, sft_vectors_path)

        _log.info(f"âœ… å·²ä¿å­˜ {len(sft_embeddings)} ä¸ªSFTå‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶: {sft_vectors_path}")
        return sft_vectors_path

    except Exception as e:
        _log.error(f"âŒ SFTå‘é‡æå–å¤±è´¥: {e}", exc_info=True)
        return None


# ----------------------- Helper Functions -----------------------

def get_detailed_instruct(task_description: str, text: str) -> str:
    """
    æ„å»ºå¸¦æŒ‡ä»¤çš„æ–‡æœ¬ï¼Œç”¨äºQwen3-Embeddingæ¨¡å‹
    å‚è€ƒæ ·ä¾‹è„šæœ¬ï¼šInstruct: {task_description}\nQuery:{text}
    
    Args:
        task_description: ä»»åŠ¡æè¿°ï¼ŒæŒ‡å¯¼æ¨¡å‹æå–ä»€ä¹ˆç±»å‹çš„åµŒå…¥
        text: åŸå§‹æ–‡æœ¬
    
    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼šInstruct: {task_description}\nQuery:{text}
    """
    return f'Instruct: {task_description}\nQuery:{text}'


def _batch_extract_embeddings(self, memory_texts, embedding_model, embedding_tokenizer, max_tokens):
    """
    ä½¿ç”¨Qwen3-Embeddingæ¨¡å‹æ‰¹é‡æå–å‘é‡
    
    æ ¹æ®Qwen3-Embeddingçš„è®¾è®¡ï¼Œå¯ä»¥é€šè¿‡Instructæ ¼å¼æŒ‡å¯¼æå–çš„åµŒå…¥ç±»å‹ï¼š
    - å¯¹äºéœ€è¦æŒ‡å¯¼çš„æ–‡æœ¬ï¼Œä½¿ç”¨æ ¼å¼ï¼šInstruct: {task_description}\nQuery:{text}
    - å¯¹äºæ™®é€šæ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬
    
    Args:
        memory_texts: è®°å¿†æ–‡æœ¬åˆ—è¡¨
        embedding_model: Qwen3-Embeddingæ¨¡å‹
        embedding_tokenizer: Qwen3-Embedding tokenizer
        max_tokens: æœ€å¤§tokené•¿åº¦
    
    Returns:
        (all_texts, all_embeddings): æ–‡æœ¬åˆ—è¡¨å’Œå¯¹åº”çš„embeddingåˆ—è¡¨
    """
    all_texts = []
    all_embeddings = []

    # è·å–æ¨¡å‹è®¾å¤‡
    model_device = next(embedding_model.parameters()).device
    _log.debug(f"ğŸ”§ æ‰¹é‡å‘é‡æå–: ä½¿ç”¨è®¾å¤‡ {model_device}")

    # ä¸ä½¿ç”¨Instructæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬æå–è¡¨å¾ï¼ˆç±»ä¼¼æ ·ä¾‹è„šæœ¬ä¸­çš„documentsï¼‰
    # å› ä¸ºæˆ‘ä»¬çš„è®°å¿†æ¡ç›®å’ŒSFTæ–‡æœ¬éƒ½æ˜¯å®Œæ•´çš„è¯­ä¹‰å•å…ƒï¼Œä¸éœ€è¦æŒ‡å¯¼
    use_instruct = False
    _log.info("ğŸ“ ç›´æ¥ä½¿ç”¨æ–‡æœ¬æå–åµŒå…¥ï¼ˆä¸ä½¿ç”¨Instructæ ¼å¼ï¼Œç±»ä¼¼æ ·ä¾‹è„šæœ¬ä¸­çš„documentsï¼‰")

    batch_size = self.training_config.get("embedding_batch_size", 8)
    _log.info(f"ğŸ“¦ ä½¿ç”¨batch_size={batch_size}è¿›è¡Œæ‰¹é‡å‘é‡æå–ï¼ˆä½¿ç”¨Qwen3-Embeddingæ¨¡å‹ï¼‰")

    total_batches = (len(memory_texts) + batch_size - 1) // batch_size
    _log.info(f"ğŸ“Š å…± {len(memory_texts)} ä¸ªè®°å¿†æ¡ç›®ï¼Œåˆ†ä¸º {total_batches} ä¸ªbatchå¤„ç†")
    # ç«‹å³åˆ·æ–°æ—¥å¿—ï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°è¿›åº¦ä¿¡æ¯
    for handler in _log.handlers:
        if hasattr(handler, 'flush'):
            handler.flush()

    for batch_idx in range(0, len(memory_texts), batch_size):
        batch_num = batch_idx // batch_size + 1
        batch_texts = memory_texts[batch_idx:batch_idx + batch_size]

        _log.info(f"ğŸ”„ å¼€å§‹å¤„ç† Batch {batch_num}/{total_batches} (æ¡ç›® {batch_idx + 1}-{min(batch_idx + batch_size, len(memory_texts))}/{len(memory_texts)})")
        # ç«‹å³åˆ·æ–°æ—¥å¿—
        for handler in _log.handlers:
            if hasattr(handler, 'flush'):
                handler.flush()

        try:
            # ç›´æ¥ä½¿ç”¨æ–‡æœ¬æå–è¡¨å¾ï¼ˆç±»ä¼¼æ ·ä¾‹è„šæœ¬ä¸­çš„documentsï¼‰
            # ä¸ä½¿ç”¨Instructæ ¼å¼ï¼Œå› ä¸ºè®°å¿†æ¡ç›®å’ŒSFTæ–‡æœ¬éƒ½æ˜¯å®Œæ•´çš„è¯­ä¹‰å•å…ƒ
            batch_input_texts = batch_texts
            
            # ä½¿ç”¨Qwen3-Embeddingçš„æ–¹å¼ï¼štokenizeæ–‡æœ¬
            batch_dict = embedding_tokenizer(
                batch_input_texts,
                padding=True,
                truncation=True,
                max_length=max_tokens,
                return_tensors="pt",
            )
            batch_dict = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v for k, v in batch_dict.items()}
            
            with torch.no_grad():
                outputs = embedding_model(**batch_dict)
                # ä½¿ç”¨last_token_poolæå–æœ€åä¸€ä¸ªtokençš„hidden state
                embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
                # å½’ä¸€åŒ–embeddings
                embeddings = F.normalize(embeddings, p=2, dim=1)
            
            # ä¿å­˜ç»“æœ
            for i in range(len(batch_texts)):
                all_texts.append(batch_texts[i])
                all_embeddings.append(embeddings[i].detach().cpu())

            processed = min(batch_idx + batch_size, len(memory_texts))
            _log.info(f"âœ… Batch {batch_num}/{total_batches} å®Œæˆ: å·²å¤„ç† {processed}/{len(memory_texts)} ä¸ªæ¡ç›®")
            # ç«‹å³åˆ·æ–°æ—¥å¿—
            for handler in _log.handlers:
                if hasattr(handler, 'flush'):
                    handler.flush()

            if (batch_idx // batch_size + 1) % 10 == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
                _log.debug(f"ğŸ§¹ å·²æ¸…ç†GPUæ˜¾å­˜ï¼ˆå¤„ç†äº† {processed} ä¸ªæ¡ç›®ï¼‰")

        except Exception as e:
            _log.error(f"âŒ Batch {batch_num} å¤„ç†å¤±è´¥: {e}", exc_info=True)
            _log.warning(f"ğŸ”„ å°è¯•é€ä¸ªå¤„ç†è¯¥batchä¸­çš„æ¡ç›®...")
            for i, memory_text in enumerate(batch_texts):
                try:
                    # å•ä¸ªå¤„ç†ï¼šç›´æ¥ä½¿ç”¨æ–‡æœ¬ï¼ˆä¸ä½¿ç”¨Instructæ ¼å¼ï¼‰
                    single_dict = embedding_tokenizer(
                        memory_text,
                        padding=True,
                        truncation=True,
                        max_length=max_tokens,
                        return_tensors="pt",
                    )
                    single_dict = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v for k, v in single_dict.items()}
                    
                    with torch.no_grad():
                        outputs = embedding_model(**single_dict)
                        embedding = last_token_pool(outputs.last_hidden_state, single_dict['attention_mask'])
                        embedding = F.normalize(embedding, p=2, dim=1)
                    
                    all_texts.append(memory_text)
                    all_embeddings.append(embedding[0].detach().cpu())
                except Exception as single_e:
                    _log.warning(f"âš ï¸ å•ä¸ªæ¡ç›®å¤„ç†ä¹Ÿå¤±è´¥: {memory_text[:50]}... é”™è¯¯: {single_e}")
                    continue

    _log.info(f"âœ… æ‰¹é‡å‘é‡æå–å®Œæˆï¼šæˆåŠŸæå– {len(all_embeddings)}/{len(memory_texts)} ä¸ªå‘é‡")
    return all_texts, all_embeddings


def _append_memory_text_to_file(self, memory_text: str, file_path: str):
    try:
        if os.path.exists(file_path):
            existing_data = torch.load(file_path, map_location='cpu')
            existing_texts = existing_data.get('texts', [])
            existing_texts.append(memory_text)
        else:
            existing_texts = [memory_text]

        torch.save({"texts": existing_texts}, file_path)
    except Exception as e:
        _log.warning(f"è¿½åŠ è®°å¿†æ¡ç›®æ–‡æœ¬åˆ°æ–‡ä»¶å¤±è´¥: {e}")


def _load_memory_texts_from_file(self, file_path: str) -> List[str]:
    try:
        if not os.path.exists(file_path):
            return []

        data = torch.load(file_path, map_location='cpu')
        texts = data.get('texts', [])
        return texts
    except Exception as e:
        _log.error(f"ä»æ–‡ä»¶åŠ è½½è®°å¿†æ¡ç›®æ–‡æœ¬å¤±è´¥: {e}")
        return []


def _save_training_data_batch(self, texts: List[str], embeddings: List[torch.Tensor]):
    temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")

    if not texts:
        return

    try:
        embeddings_tensor = torch.stack(embeddings)

        if os.path.exists(temp_data_path):
            existing_data = torch.load(temp_data_path, map_location='cpu')
            existing_texts = existing_data.get('texts', [])
            existing_embeddings = existing_data.get('embeddings')
            all_texts = existing_texts + texts
            all_embeddings = torch.cat([existing_embeddings, embeddings_tensor], dim=0)
        else:
            all_texts = texts
            all_embeddings = embeddings_tensor

        torch.save({
            "texts": all_texts,
            "embeddings": all_embeddings
        }, temp_data_path)

        _log.info(f"ä¿å­˜äº† {len(texts)} ä¸ªæ¡ç›®çš„è®­ç»ƒæ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆæ€»è®¡ {len(all_texts)} ä¸ªæ¡ç›®ï¼‰")

    except Exception as e:
        _log.error(f"ä¿å­˜è®­ç»ƒæ•°æ®æ‰¹æ¬¡å¤±è´¥: {e}")
        raise


def _load_sft_dataset(self) -> List[Dict[str, Any]]:
    dataset_path = self.sft_path
    if not dataset_path:
        _log.warning("âš ï¸ æœªé…ç½®SFTæ•°æ®é›†è·¯å¾„")
        return []
    if not os.path.isabs(dataset_path):
        dataset_path = os.path.abspath(os.path.join(self._project_root, dataset_path))
    if not os.path.exists(dataset_path):
        _log.warning(f"âš ï¸ SFTæ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        return []
    samples = []
    try:
        with open(dataset_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    samples.append(obj)
                except Exception:
                    continue
    except Exception as e:
        _log.warning(f"åŠ è½½SFTæ•°æ®é›†å¤±è´¥: {e}")
        return []
    _log.info(f"âœ… åŠ è½½SFTæ ·æœ¬: {len(samples)}")
    return samples


def _standardize_sft_messages(self, sample: Dict[str, Any]) -> List[Dict[str, Any]]:
    msgs = sample.get("messages")
    if isinstance(msgs, list) and msgs:
        std = []
        for m in msgs:
            role = m.get("role", "user")
            content = m.get("content") or m.get("text") or ""
            if isinstance(content, str):
                std.append({"role": role, "content": [{"type": "text", "text": content}]})
            elif isinstance(content, list):
                text_join = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_join += item.get("text", "")
                    elif isinstance(item, str):
                        text_join += item
                std.append({"role": role, "content": [{"type": "text", "text": text_join}]})
        if std:
            return std
    inst = sample.get("instruction") or sample.get("input")
    out = sample.get("output") or sample.get("answer")
    if isinstance(inst, str) and isinstance(out, str):
        return [
            {"role": "user", "content": [{"type": "text", "text": inst}]},
            {"role": "assistant", "content": [{"type": "text", "text": out}]},
        ]
    q = sample.get("query") or sample.get("question")
    a = sample.get("response") or sample.get("answer")
    if isinstance(q, str) and isinstance(a, str):
        return [
            {"role": "user", "content": [{"type": "text", "text": q}]},
            {"role": "assistant", "content": [{"type": "text", "text": a}]},
        ]
    return []


def _parse_memory_entries(self, generated_text: str) -> List[str]:
    """
    è§£ææ¨¡å‹è¾“å‡ºï¼Œæå–æ ¼å¼åŒ–çš„è®°å¿†æ¡ç›®ã€‚ä¸¥æ ¼æŒ‰ç…§ v1.2 çš„è§£æé€»è¾‘ï¼Œé¿å…æŠŠæ€è€ƒå†…å®¹æˆ–æŒ‡ä»¤åŸæ–‡å½“æˆè®°å¿†ã€‚
    """
    if not generated_text or not generated_text.strip():
        return []

    text = generated_text.strip()
    text = re.sub(r"<\|[^>]+?\|>", "", text)
    text = re.sub(r"</?think>", "", text, flags=re.IGNORECASE)
    text = re.sub(r"</?thinking>", "", text, flags=re.IGNORECASE)
    text = re.sub(r"</?analysis>", "", text, flags=re.IGNORECASE)
    text = re.sub(r"</?reflect>", "", text, flags=re.IGNORECASE)
    text = text.replace("<no_reply>", "").replace("</no_reply>", "")
    # æ˜¾å¼æ‹’ç»â€œæ— è®°å¿†â€æç¤º
    if "æ— è®°å¿†æ¡ç›®" in text or "æ— è®°å¿†" in text[:20]:
        return []

    entries: List[str] = []

    # æ–¹æ³•1ï¼šé€è¡ŒåŒ¹é…â€œæ¡ç›®/ç¼–å·â€æ ¼å¼
    for line in text.splitlines():
        stripped = line.strip()
        if not stripped:
            continue
        match = re.match(r'^(?:æ¡ç›®\s*\d+|[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[\.ã€]|[-*])\s*[:ï¼š]?\s*(.+)', stripped)
        if match:
            candidate = match.group(1).strip()
            if candidate and len(candidate) > 3:
                entries.append(candidate)
        elif len(stripped) > 10:
            # å¤‡ç”¨ï¼šæ²¡æœ‰æ˜æ˜¾ç¼–å·ä½†çœ‹èµ·æ¥åƒäº‹å®é™ˆè¿°
            if any(keyword in stripped for keyword in ["å–œæ¬¢", "æ˜¯", "åœ¨", "æœ‰", "çš„", "äº†", "ä¼š", "è¦", "å»", "æ¥"]):
                entries.append(stripped)

    # æ–¹æ³•2ï¼šè‹¥ä»æ— æ¡ç›®ï¼ŒæŒ‰å¥å­åˆ‡åˆ†å¹¶è¿‡æ»¤æ‰æŒ‡ä»¤/æ€è€ƒæè¿°
    if not entries:
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) <= 5:
                continue
            if any(keyword in sentence for keyword in ["è¯·", "åˆ†æ", "å¯¹è¯", "å†…å®¹", "è®°å¿†æ¡ç›®", "ä»¥ä¸‹", "æ€è€ƒ", "æ­¥éª¤"]):
                continue
            entries.append(sentence)

    # å»é‡ã€å»å™ª
    seen = set()
    cleaned: List[str] = []
    for entry in entries:
        normalized = " ".join(entry.split())
        if len(normalized) < 3:
            continue
        if normalized in seen:
            continue
        if any(token in normalized for token in ["å¯¹è¯å†…å®¹", "æå–", "è¯·å¼€å§‹", "æ€è€ƒå†…å®¹"]):
            continue
        seen.add(normalized)
        cleaned.append(normalized)

    _log.debug(f"è§£æåçš„è®°å¿†æ¡ç›®: {cleaned}")
    return cleaned


def _build_extraction_prompt(role_prompt: str, extraction_prompts: Dict[str, Any]) -> str:
    prompts_cfg = extraction_prompts or {}
    base_prompt = prompts_cfg.get("system_prompt")
    if not base_prompt:
        raise ValueError("memory_extraction.system_prompt æœªé…ç½®ï¼Œè¯·åœ¨ prompts.yaml ä¸­è®¾ç½®")
    wrapper = prompts_cfg.get("role_prompt_wrapper")
    if role_prompt:
        if wrapper and "{role_prompt}" in wrapper:
            base_prompt += "\n\n" + wrapper.replace("{role_prompt}", role_prompt)
        else:
            base_prompt += f"\n\n{role_prompt}"
    return base_prompt.strip()

