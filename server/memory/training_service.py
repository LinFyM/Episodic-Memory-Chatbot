#! -*- coding: utf-8 -*-
"""
è®°å¿†è®­ç»ƒæœåŠ¡
è´Ÿè´£æ•´åˆèŠå¤©è®°å½•ã€æå–è®°å¿†æ¡ç›®ã€è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜
ï¼ˆä» server/memory_training_service.py è¿ç§»ï¼‰
"""

import os
import json
import shutil
import torch
import torch.nn as nn
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import PIL
import requests
from transformers import AutoProcessor
import random
import logging as _logging

# å¯¼å…¥è®­ç»ƒç›¸å…³ç»„ä»¶
import sys
# training_service.py åœ¨ server/memory/ ç›®å½•ä¸‹ï¼Œéœ€è¦å¾€ä¸Š3å±‚åˆ°é¡¹ç›®æ ¹ç›®å½•
# __file__ = server/memory/training_service.py
# dirname(__file__) = server/memory/
# dirname(dirname(__file__)) = server/
# dirname(dirname(dirname(__file__))) = é¡¹ç›®æ ¹ç›®å½•/
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
recall_dir = os.path.join(project_root, 'recall')
# ç¡®ä¿recallç›®å½•åœ¨sys.pathçš„æœ€å‰é¢
if recall_dir in sys.path:
    sys.path.remove(recall_dir)
sys.path.insert(0, recall_dir)

from recall.model_utils import forward_backbone, ensure_last_hidden_state

# å»¶è¿Ÿå¯¼å…¥è®­ç»ƒå™¨ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
# æ³¨æ„ï¼šè¿™é‡Œä¸é¢„å…ˆå¯¼å…¥ï¼Œè®©_ensure_training_modules_loaded()å‡½æ•°å¤„ç†
TRAINING_MODULES_AVAILABLE = False


def _try_prepare_recall_paths() -> bool:
    """
    å°è¯•å°†å¯èƒ½å­˜åœ¨çš„ recall ç›®å½•åŠ å…¥ sys.pathï¼š
    - <project_root>/recall
    - <project_root>/èåœå­v2.0/recall
    è¿”å›æ˜¯å¦è‡³å°‘å­˜åœ¨ä¸€ä¸ªç›®å½•ã€‚
    """
    try:
        import sys as _sys, os as _os
        # training_service.py åœ¨ server/memory/ ç›®å½•ä¸‹ï¼Œéœ€è¦å¾€ä¸Š3å±‚åˆ°é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # qqbot_new/server/memory -> qqbot_new
        candidates = [
            os.path.join(project_root, "recall"),
            os.path.join(project_root, "èåœå­v2.0", "recall"),
        ]
        found = False
        for p in candidates:
            if os.path.isdir(p):
                found = True
                if p not in _sys.path:
                    _sys.path.insert(0, p)
        return found
    except Exception:
        return False


def _ensure_training_modules_loaded() -> bool:
    """
    ç¡®ä¿è®­ç»ƒä¾èµ–å¯å¯¼å…¥ã€‚è‹¥å¯å¯¼å…¥åˆ™è¿”å›Trueã€‚
    """
    global TRAINING_MODULES_AVAILABLE
    if TRAINING_MODULES_AVAILABLE:
        return True
    
    # è·å–loggerï¼ˆå¦‚æœ_logè¿˜æ²¡æœ‰å®šä¹‰ï¼Œå°±åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ï¼‰
    try:
        logger = _log
    except NameError:
        logger = logging.getLogger(__name__)
    
    # å°è¯•å‡†å¤‡è·¯å¾„å¹¶å¯¼å…¥
    _try_prepare_recall_paths()
    # ç¡®ä¿recallç›®å½•åœ¨sys.pathä¸­ï¼ˆå³ä½¿_try_prepare_recall_pathså·²ç»æ·»åŠ ï¼Œä¹Ÿè¦ç¡®ä¿ï¼‰
    import sys
    # training_service.py åœ¨ server/memory/ ç›®å½•ä¸‹ï¼Œéœ€è¦å¾€ä¸Š3å±‚åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    recall_dir = os.path.join(project_root, 'recall')
    
    # ç¡®ä¿recallç›®å½•å­˜åœ¨ä¸”å·²æ·»åŠ åˆ°sys.pathï¼ˆç¡®ä¿åœ¨æœ€å‰é¢ï¼‰
    if os.path.isdir(recall_dir):
        if recall_dir in sys.path:
            # å¦‚æœå·²ç»åœ¨sys.pathä¸­ï¼Œå…ˆç§»é™¤å†é‡æ–°æ’å…¥åˆ°æœ€å‰é¢
            sys.path.remove(recall_dir)
            logger.debug(f"ä»sys.pathä¸­ç§»é™¤recallç›®å½•: {recall_dir}")
        # æ’å…¥åˆ°sys.pathçš„æœ€å‰é¢ï¼Œç¡®ä¿ä¼˜å…ˆä½¿ç”¨
        sys.path.insert(0, recall_dir)
        logger.debug(f"å·²å°†recallç›®å½•æ’å…¥åˆ°sys.pathæœ€å‰é¢: {recall_dir}")
    else:
        logger.error(f"recallç›®å½•ä¸å­˜åœ¨: {recall_dir}")
    
    try:
        global RecallMemoryTrainer, EnhancedTextMemoryTrainer, extract_last_token_embedding  # type: ignore
        # å°è¯•å¯¼å…¥è®­ç»ƒæ¨¡å—
        logger.info(f"å°è¯•å¯¼å…¥è®­ç»ƒæ¨¡å—ï¼Œå½“å‰sys.pathå‰3é¡¹: {sys.path[:3]}")
        logger.info(f"recallç›®å½•: {recall_dir}")
        logger.info(f"recallç›®å½•å­˜åœ¨: {os.path.isdir(recall_dir)}")
        logger.info(f"Python executable: {sys.executable}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        files_to_check = [
            'text_embedding_train.py',
            'text_memory_train.py',
            'get_text_embedding.py'
        ]
        for file_name in files_to_check:
            file_path = os.path.join(recall_dir, file_name)
            exists = os.path.isfile(file_path)
            logger.info(f"  {file_name}: {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")

        # å…ˆç¡®ä¿torchå¯ç”¨
        try:
            import torch
            logger.info(f"torchç‰ˆæœ¬: {torch.__version__}, CUDAå¯ç”¨: {torch.cuda.is_available()}")
        except ImportError as e:
            logger.error(f"torchä¸å¯ç”¨: {e}ã€‚è¯·ç¡®ä¿torchå·²æ­£ç¡®å®‰è£…ã€‚")
            TRAINING_MODULES_AVAILABLE = False
            return False

        # æ£€æŸ¥è®­ç»ƒæ¨¡å—çš„è¯­æ³•
        try:
            import importlib.util
            for file_name in files_to_check:
                file_path = os.path.join(recall_dir, file_name)
                if os.path.isfile(file_path):
                    spec = importlib.util.spec_from_file_location(file_name[:-3], file_path)  # ç§»é™¤.pyæ‰©å±•å
                    if spec is None:
                        logger.warning(f"æ— æ³•åˆ›å»ºspec for {file_name}")
                    else:
                        logger.info(f"spec for {file_name} åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.warning(f"specæ£€æŸ¥å¤±è´¥: {e}")

        from text_embedding_train import RecallMemoryTrainer  # type: ignore
        from text_memory_train import EnhancedTextMemoryTrainer  # type: ignore
        from get_text_embedding import extract_last_token_embedding  # type: ignore
        TRAINING_MODULES_AVAILABLE = True
        logger.info("âœ… è®­ç»ƒæ¨¡å—åŠ è½½æˆåŠŸ")
        return True
    except ImportError as e:
        logger.warning(f"è®­ç»ƒä¾èµ–ä¸å¯ç”¨ï¼š{e}ã€‚å¦‚éœ€å¯ç”¨è®­ç»ƒï¼Œè¯·åœ¨é¡¹ç›®æ ¹å‡†å¤‡ recall/ è„šæœ¬æˆ– èåœå­v2.0/recall/")
        logger.debug(f"å½“å‰sys.pathä¸­çš„recallç›¸å…³è·¯å¾„: {[p for p in sys.path if 'recall' in p]}")
        logger.debug(f"recallç›®å½•ç»å¯¹è·¯å¾„: {recall_dir}")
        logger.debug(f"recallç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.isdir(recall_dir)}")
        if os.path.isdir(recall_dir):
            logger.debug(f"recallç›®å½•å†…å®¹: {os.listdir(recall_dir)[:10]}")
        TRAINING_MODULES_AVAILABLE = False
        return False
    except Exception as e:
        logger.error(f"å¯¼å…¥è®­ç»ƒæ¨¡å—æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        TRAINING_MODULES_AVAILABLE = False
        return False

from .vector_db import MemoryVectorDB

if 'TRAINING_MODULES_AVAILABLE' not in locals():
    TRAINING_MODULES_AVAILABLE = False

if '_log' not in locals():
    _log = logging.getLogger(__name__)


def _optimize_multi_gpu_allocation(device_list: List[str], max_memory_config: Dict[int, str] = None, cuda_visible_set: bool = False) -> Dict[str, Any]:
    """
    ä¼˜åŒ–å¤šGPUåˆ†é…ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹å’Œæ•°æ®æ›´å‡åŒ€åœ°åˆ†å¸ƒåœ¨å¤šå¼ GPUä¸Š
    
    Args:
        device_list: GPUè®¾å¤‡åˆ—è¡¨ï¼Œå¦‚ ["cuda:0", "cuda:1"] æˆ– ["cuda:6", "cuda:7"]
        max_memory_config: ç”¨æˆ·é…ç½®çš„max_memoryï¼Œæ ¼å¼å¦‚ {0: "20GB", 1: "20GB"}ï¼ˆç´¢å¼•æ˜¯å¯è§GPUçš„ç´¢å¼•ï¼Œä¸æ˜¯ç‰©ç†ç´¢å¼•ï¼‰
        cuda_visible_set: æ˜¯å¦å·²ç»è®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼ˆå¦‚æœå·²è®¾ç½®ï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•ï¼‰
    
    Returns:
        åŒ…å«ä¼˜åŒ–åçš„max_memoryå’Œdevice_mapçš„å­—å…¸
    """
    if not torch.cuda.is_available():
        return {"device_map": "cpu", "max_memory": None}
    
    num_gpus = len(device_list)
    if num_gpus == 0:
        return {"device_map": "cpu", "max_memory": None}
    
    # æ£€æµ‹æ¯å¼ GPUçš„å¯ç”¨æ˜¾å­˜
    gpu_memories = {}
    for i, device in enumerate(device_list):
        if device.startswith("cuda:"):
            try:
                physical_gpu_idx = int(device.split(":")[1])
                
                # å¦‚æœCUDA_VISIBLE_DEVICESå·²ç»è®¾ç½®ï¼Œtorchåªèƒ½çœ‹åˆ°é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                # æ­¤æ—¶éœ€è¦ä½¿ç”¨å¯è§GPUçš„ç´¢å¼•ï¼ˆ0, 1, 2...ï¼‰ï¼Œè€Œä¸æ˜¯ç‰©ç†ç´¢å¼•
                if cuda_visible_set:
                    # ä½¿ç”¨å¯è§GPUçš„ç´¢å¼•ï¼ˆiå°±æ˜¯é‡æ–°æ˜ å°„åçš„ç´¢å¼•ï¼‰
                    visible_gpu_idx = i
                    # è·å–GPUæ€»æ˜¾å­˜ï¼ˆMBï¼‰- ä½¿ç”¨å¯è§ç´¢å¼•
                    total_memory_mb = torch.cuda.get_device_properties(visible_gpu_idx).total_memory // (1024 * 1024)
                    # è·å–å½“å‰å·²ç”¨æ˜¾å­˜ï¼ˆMBï¼‰
                    torch.cuda.set_device(visible_gpu_idx)
                    allocated_mb = torch.cuda.memory_allocated(visible_gpu_idx) // (1024 * 1024)
                    reserved_mb = torch.cuda.memory_reserved(visible_gpu_idx) // (1024 * 1024)
                    available_mb = total_memory_mb - reserved_mb
                    _log.info(f"ğŸ” è®­ç»ƒæ¨¡å‹ GPU {i} (ç‰©ç†ç´¢å¼• {physical_gpu_idx}, å¯è§ç´¢å¼• {visible_gpu_idx}): æ€»æ˜¾å­˜={total_memory_mb}MB, å¯ç”¨={available_mb}MB, å·²ä¿ç•™={reserved_mb}MB")
                else:
                    # CUDA_VISIBLE_DEVICESæœªè®¾ç½®ï¼Œä½¿ç”¨ç‰©ç†ç´¢å¼•
                    # è·å–GPUæ€»æ˜¾å­˜ï¼ˆMBï¼‰
                    total_memory_mb = torch.cuda.get_device_properties(physical_gpu_idx).total_memory // (1024 * 1024)
                    # è·å–å½“å‰å·²ç”¨æ˜¾å­˜ï¼ˆMBï¼‰
                    torch.cuda.set_device(physical_gpu_idx)
                    allocated_mb = torch.cuda.memory_allocated(physical_gpu_idx) // (1024 * 1024)
                    reserved_mb = torch.cuda.memory_reserved(physical_gpu_idx) // (1024 * 1024)
                    available_mb = total_memory_mb - reserved_mb
                    _log.info(f"ğŸ” è®­ç»ƒæ¨¡å‹ GPU {i} (ç‰©ç†ç´¢å¼• {physical_gpu_idx}): æ€»æ˜¾å­˜={total_memory_mb}MB, å¯ç”¨={available_mb}MB, å·²ä¿ç•™={reserved_mb}MB")
                
                gpu_memories[i] = {
                    "total_mb": total_memory_mb,
                    "available_mb": available_mb,
                    "reserved_mb": reserved_mb,
                    "allocated_mb": allocated_mb
                }
            except Exception as e:
                _log.warning(f"âš ï¸ æ— æ³•æ£€æµ‹GPU {i}çš„æ˜¾å­˜: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                gpu_memories[i] = {"total_mb": 24000, "available_mb": 20000, "reserved_mb": 0, "allocated_mb": 0}
    
    # è®¡ç®—ä¼˜åŒ–çš„max_memoryé…ç½®
    optimized_max_memory = {}
    if max_memory_config:
        # å¦‚æœç”¨æˆ·æä¾›äº†é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œä½†ç¡®ä¿æ‰€æœ‰GPUéƒ½æœ‰é…ç½®
        for i in range(num_gpus):
            if i in max_memory_config:
                optimized_max_memory[i] = max_memory_config[i]
            else:
                # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨å¯ç”¨æ˜¾å­˜çš„90%ï¼ˆç•™10%ç»™ç³»ç»Ÿå’Œå…¶ä»–æ“ä½œï¼‰
                if i in gpu_memories:
                    available_gb = gpu_memories[i]["available_mb"] / 1024
                    optimized_max_memory[i] = f"{int(available_gb * 0.9)}GB"
                else:
                    optimized_max_memory[i] = "20GB"  # é»˜è®¤å€¼
    else:
        # å¦‚æœæ²¡æœ‰ç”¨æˆ·é…ç½®ï¼Œè‡ªåŠ¨è®¡ç®—ï¼šä½¿ç”¨æ¯å¼ GPUå¯ç”¨æ˜¾å­˜çš„90%
        for i in range(num_gpus):
            if i in gpu_memories:
                available_gb = gpu_memories[i]["available_mb"] / 1024
                optimized_max_memory[i] = f"{int(available_gb * 0.9)}GB"
            else:
                optimized_max_memory[i] = "20GB"  # é»˜è®¤å€¼
    
    _log.info(f"âœ… è®­ç»ƒæ¨¡å‹ä¼˜åŒ–çš„max_memoryé…ç½®: {optimized_max_memory}")
    
    # ä½¿ç”¨ "balanced" device_mapï¼Œå°½å¯èƒ½å‡åŒ€åœ°åˆ†é…æ¨¡å‹å±‚åˆ°æ‰€æœ‰GPU
    # è¿™æ ·å¯ä»¥æœ€å¤§åŒ–åˆ©ç”¨æ‰€æœ‰GPUçš„æ˜¾å­˜ï¼Œé¿å…å•å¼ GPUè¿‡è½½
    # æ³¨æ„ï¼šå¦‚æœé‡åˆ°OOMï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ "balanced_low_0" è®©cuda:0åˆ†é…æ›´å°‘
    # å‚è€ƒï¼šhttps://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained.device_map
    if num_gpus > 1:
        device_map_strategy = "balanced"
        _log.info(f"ğŸ”§ å¤šGPUæ¨¡å¼ï¼šä½¿ç”¨ device_map='balanced'ï¼Œå‡åŒ€åˆ†é…æ¨¡å‹å±‚åˆ°æ‰€æœ‰ {num_gpus} å¼ GPU")
    else:
        device_map_strategy = "auto"
        _log.info(f"ğŸ”§ å•GPUæ¨¡å¼ï¼šä½¿ç”¨ device_map='auto'")
    
    return {
        "device_map": device_map_strategy,
        "max_memory": optimized_max_memory
    }


class TrainingModelContext:
    """è®­ç»ƒæ¨¡å‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ç®¡ç†è®­ç»ƒæ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸ"""

    def __init__(self, model_path: str, device, multi_gpu_config: Dict[str, Any] = None, add_special_tokens: bool = True):
        """
        åˆå§‹åŒ–è®­ç»ƒæ¨¡å‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        """
        self.model_path = model_path
        self.device = device
        self.multi_gpu_config = multi_gpu_config or {}
        self.add_special_tokens = add_special_tokens
        self.model = None
        self.processor = None

    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ï¼ŒåŠ è½½è®­ç»ƒæ¨¡å‹"""
        _log.info(f"åŠ è½½è®­ç»ƒæ¨¡å‹ä¸Šä¸‹æ–‡: {self.model_path}")
        self.model, self.processor = self._load_training_model()
        return self.model, self.processor

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ï¼Œå½»åº•æ¸…ç†æ¨¡å‹ï¼ˆæ”¯æŒå¤šGPUï¼‰"""
        _log.info("æ¸…ç†è®­ç»ƒæ¨¡å‹ä¸Šä¸‹æ–‡...")

        try:
            # æ¸…ç†æ¨¡å‹ï¼ˆå¤šGPUæƒ…å†µä¸‹éœ€è¦æ›´å½»åº•çš„æ¸…ç†ï¼‰
            if self.model is not None:
                try:
                    # å¯¹äºå¤šGPUæ¨¡å‹ï¼Œéœ€è¦å…ˆå°è¯•ç§»åŠ¨åˆ°CPU
                    # å¦‚æœæ¨¡å‹ä½¿ç”¨äº†device_map="auto"ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                    if hasattr(self.model, 'hf_device_map') and self.model.hf_device_map:
                        # å¤šGPUæ¨¡å‹ï¼Œéœ€è¦é€ä¸ªè®¾å¤‡æ¸…ç†
                        _log.info("æ£€æµ‹åˆ°å¤šGPUæ¨¡å‹ï¼Œæ‰§è¡Œå½»åº•æ¸…ç†...")
                        # å…ˆå°è¯•ç§»åŠ¨åˆ°CPUï¼ˆå¯èƒ½éƒ¨åˆ†å±‚å·²ç»åœ¨CPUä¸Šï¼‰
                        try:
                            self.model.cpu()
                        except Exception as e:
                            _log.warning(f"ç§»åŠ¨æ¨¡å‹åˆ°CPUæ—¶å‡ºç°è­¦å‘Š: {e}")
                        
                        # å¦‚æœæ¨¡å‹æœ‰acceleratoråŒ…è£…ï¼Œéœ€è¦å…ˆæ¸…ç†accelerator
                        if hasattr(self.model, 'accelerator'):
                            try:
                                self.model.accelerator.free_memory()
                            except:
                                pass
                    else:
                        # å•GPUæ¨¡å‹ï¼Œç›´æ¥ç§»åŠ¨åˆ°CPU
                        try:
                            self.model.cpu()
                        except:
                            pass
                except Exception as e:
                    _log.warning(f"æ¸…ç†æ¨¡å‹æ—¶å‡ºç°è­¦å‘Š: {e}")
                
                # åˆ é™¤æ¨¡å‹å¼•ç”¨
                del self.model
                self.model = None

            # æ¸…ç†processor
            if self.processor is not None:
                del self.processor
                self.processor = None

            # å¼ºåˆ¶åƒåœ¾å›æ”¶å’Œæ˜¾å­˜æ¸…ç†ï¼ˆå¤šæ¬¡æ¸…ç†ç¡®ä¿å½»åº•ï¼‰
            import gc
            for _ in range(5):  # å¢åŠ æ¸…ç†æ¬¡æ•°
                gc.collect()

            # æ¸…ç†æ‰€æœ‰GPUçš„æ˜¾å­˜
            if torch.cuda.is_available():
                # åŒæ­¥æ‰€æœ‰GPU
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.synchronize()
                        torch.cuda.empty_cache()
                        torch.cuda.reset_peak_memory_stats()
                
                # å†æ¬¡æ¸…ç†æ‰€æœ‰GPU
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                
                _log.info(f"âœ… å·²æ¸…ç†æ‰€æœ‰ {torch.cuda.device_count()} å¼ GPUçš„æ˜¾å­˜")

            _log.info("âœ… è®­ç»ƒæ¨¡å‹ä¸Šä¸‹æ–‡æ¸…ç†å®Œæˆ")

        except Exception as cleanup_error:
            _log.warning(f"è®­ç»ƒæ¨¡å‹ä¸Šä¸‹æ–‡æ¸…ç†æ—¶å‡ºç°é”™è¯¯: {cleanup_error}")

        return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸

    def _load_training_model(self):
        """åŠ è½½è®­ç»ƒæ¨¡å‹ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        return self.load_training_model(self.model_path, self.device, self.multi_gpu_config, add_special_tokens=self.add_special_tokens)

    @staticmethod
    def load_training_model(model_path: str, device, multi_gpu_config: Dict[str, Any] = None, add_special_tokens: bool = True):
        """åŠ è½½ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹ï¼ˆé™æ€æ–¹æ³•ï¼‰"""
        # ä½¿ç”¨ä¸initialize_modelç›¸åŒçš„åŠ è½½é€»è¾‘
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM

        multi_gpu_config = multi_gpu_config or {}
        multi_gpu_enabled = multi_gpu_config.get("enabled", True)

        # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(model_path):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            model_path = os.path.abspath(os.path.join(project_root, model_path))

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        is_local_path = os.path.exists(model_path) and os.path.isdir(model_path)

        try:
            # åŠ è½½processorï¼ˆä½¿ç”¨AutoProcessorè€Œä¸æ˜¯AutoTokenizerï¼Œå› ä¸ºéœ€è¦å¤„ç†å›¾ç‰‡å’Œè§†é¢‘ï¼‰
            # æ­£å¸¸æ¨ç†æ—¶ä½¿ç”¨AutoProcessorï¼Œè®­ç»ƒæ—¶ä¹Ÿåº”è¯¥ä½¿ç”¨AutoProcessor
            processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=is_local_path
            )

            # å¤„ç†å¤šGPUè®¾å¤‡é…ç½®
            if device == "auto" and multi_gpu_enabled:
                # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨GPU
                if torch.cuda.is_available():
                    gpu_count = torch.cuda.device_count()
                    device = [f"cuda:{i}" for i in range(gpu_count)]
                    _log.info(f"ğŸ”§ è®­ç»ƒæ¨¡å‹: è‡ªåŠ¨æ£€æµ‹åˆ° {gpu_count} å¼ GPUï¼Œä½¿ç”¨å¤šGPUæ¨¡å¼")

            # åŠ è½½æ¨¡å‹ - æ”¯æŒå¤šGPU
            from transformers import Qwen3VLForConditionalGeneration
            load_kwargs = {
                "torch_dtype": torch.bfloat16,
                "trust_remote_code": True,
                "local_files_only": is_local_path
            }

            # æ£€æŸ¥CUDA_VISIBLE_DEVICESè®¾ç½®çŠ¶æ€ï¼ˆåœ¨æ‰€æœ‰è®¾å¤‡é…ç½®ä¹‹å‰ï¼‰
            cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
            cuda_visible_set = bool(cuda_visible)
            cuda_visible_devices = cuda_visible

            # æ ¹æ®è®¾å¤‡é…ç½®å†³å®šdevice_map
            if isinstance(device, list) and multi_gpu_enabled:
                # å¤šGPUé…ç½®
                # æ³¨æ„ï¼šCUDA_VISIBLE_DEVICESåº”è¯¥åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ï¼ˆåœ¨app.pyä¸­å·²è®¾ç½®ï¼‰
                # è¿™é‡Œåªéœ€è¦æ£€æŸ¥æ˜¯å¦å·²ç»è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™è®¾ç½®ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
                
                if cuda_visible:
                    _log.info(f"ğŸ”§ æ£€æµ‹åˆ°CUDA_VISIBLE_DEVICES={cuda_visible}ï¼ˆå·²åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ï¼‰")
                else:
                    # å¦‚æœæœªè®¾ç½®ï¼Œåˆ™åœ¨è¿™é‡Œè®¾ç½®ï¼ˆè™½ç„¶å¯èƒ½å·²ç»å¤ªæ™šäº†ï¼‰
                    gpu_indices = []
                    for gpu_device in device:
                        if gpu_device.startswith("cuda:"):
                            try:
                                gpu_idx = int(gpu_device.split(":")[1])
                                gpu_indices.append(str(gpu_idx))
                            except (ValueError, IndexError):
                                _log.warning(f"âš ï¸ æ— æ•ˆçš„GPUè®¾å¤‡åç§°: {gpu_device}ï¼Œè·³è¿‡")
                                continue
                    if gpu_indices:
                        cuda_visible_devices = ",".join(gpu_indices)
                        os.environ["CUDA_VISIBLE_DEVICES"] = cuda_visible_devices
                        _log.warning(f"âš ï¸ CUDA_VISIBLE_DEVICESæœªåœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ï¼Œç°åœ¨è®¾ç½®={cuda_visible_devices}ï¼ˆå¯èƒ½æ— æ•ˆï¼‰")
                        # æ³¨æ„ï¼šå¦‚æœåœ¨è¿™é‡Œè®¾ç½®ï¼Œtorchå¯èƒ½å·²ç»åˆå§‹åŒ–ï¼Œæ‰€ä»¥å¯èƒ½æ— æ•ˆ
                        # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä»ç„¶è®¾ç½®å®ƒ

                # ä½¿ç”¨ä¼˜åŒ–çš„å¤šGPUåˆ†é…ç­–ç•¥
                # æ³¨æ„ï¼šå¦‚æœCUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                max_memory_config = multi_gpu_config.get("max_memory", {})
                allocation = _optimize_multi_gpu_allocation(device, max_memory_config, cuda_visible_set=cuda_visible_set)
                load_kwargs["device_map"] = allocation["device_map"]
                if allocation["max_memory"]:
                    load_kwargs["max_memory"] = allocation["max_memory"]
                _log.info(f"ğŸ”§ è®­ç»ƒæ¨¡å‹: æŒ‡å®šè®¾å¤‡{device}ï¼Œä½¿ç”¨ä¼˜åŒ–çš„åˆ†é…ç­–ç•¥")
            elif isinstance(device, str) and device.startswith("cuda"):
                # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                if cuda_visible_set and cuda_visible_devices:
                    # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                    device_map_device = "cuda:0"
                    _log.info(f"ğŸ”§ è®­ç»ƒæ¨¡å‹: å•GPUæ¨¡å¼ï¼ŒCUDA_VISIBLE_DEVICES={cuda_visible_devices}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {device_map_device}ï¼ˆå¯¹åº”ç‰©ç†GPU {device}ï¼‰")
                else:
                    # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç›´æ¥ä½¿ç”¨ç‰©ç†è®¾å¤‡
                    device_map_device = device
                    _log.info(f"ğŸ”§ è®­ç»ƒæ¨¡å‹: å•GPUæ¨¡å¼ï¼Œè®¾å¤‡æ˜ å°„åˆ° {device}")
                load_kwargs["device_map"] = {"": device_map_device}
            else:
                load_kwargs["device_map"] = "auto"  # é»˜è®¤ä½¿ç”¨auto
                _log.info("ğŸ”§ è®­ç»ƒæ¨¡å‹: ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")

            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_path,
                **load_kwargs
            )

            if add_special_tokens:
                # æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
                # ä½¿ç”¨MemoryTokenManagerï¼Œä¸æ­£å¸¸æ¨ç†æ—¶ä¿æŒä¸€è‡´
                from memory.token_manager import MemoryTokenManager
                token_manager = MemoryTokenManager(model, processor.tokenizer)
                recall_token_ids = token_manager.check_and_add_tokens(perturbation_std=0.02)
                _log.info(f"âœ… ç‰¹æ®Štokenå¤„ç†å®Œæˆ: {recall_token_ids}")

            _log.info("âœ… è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, processor

        except Exception as e:
            _log.error(f"âŒ åŠ è½½è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            raise


def _resolve_path(path: Optional[str], project_root: str) -> Optional[str]:
    if not path:
        return path
    if os.path.isabs(path):
        return path
    return os.path.abspath(os.path.join(project_root, path))


class MemoryTrainingService:
    """è®°å¿†è®­ç»ƒæœåŠ¡"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒæœåŠ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«è®­ç»ƒç›¸å…³å‚æ•°
        """
        self.config = config
        self.memory_config = config.get("memory", {}).get("training", {})
        
        script_dir = os.path.dirname(os.path.abspath(__file__))  # server/memory
        server_dir = os.path.dirname(script_dir)                 # server
        project_root = os.path.dirname(server_dir)               # é¡¹ç›®æ ¹
        self._project_root = project_root
        
        # è·¯å¾„é…ç½®
        self.base_model_path = _resolve_path(self.memory_config.get("base_model_path"), project_root)
        self.trained_model_dir = _resolve_path(self.memory_config.get("trained_model_dir"), project_root)
        self.memory_db_dir = _resolve_path(self.memory_config.get("memory_db_dir"), project_root)
        self.chat_history_storage_dir = _resolve_path(self.memory_config.get("chat_history_storage_dir"), project_root)
        
        # è®­ç»ƒé…ç½®
        self.training_config = self.memory_config.get("training_config", {})
        self.lora_config = self.memory_config.get("lora_config", {})
        self.guides_config = self.memory_config.get("guides", {})
        
        # è®¾å¤‡é…ç½®ï¼ˆä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„è®¾å¤‡ï¼‰
        model_config = config.get("model", {})
        self.device = model_config.get("device", "cuda:0")
        # ä¿å­˜åŸå§‹è®¾å¤‡ä¿¡æ¯ï¼ˆç”¨äºè®­ç»ƒå™¨æ—¥å¿—æ˜¾ç¤ºï¼‰
        self.original_device = self.device
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.trained_model_dir, exist_ok=True)
        os.makedirs(self.memory_db_dir, exist_ok=True)
        
        if self.chat_history_storage_dir:
            os.makedirs(self.chat_history_storage_dir, exist_ok=True)
        
        _log.info("è®°å¿†è®­ç»ƒæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        _log.info(f"  åŸºç¡€æ¨¡å‹è·¯å¾„: {self.base_model_path}")
        _log.info(f"  è®­ç»ƒæ¨¡å‹ç›®å½•: {self.trained_model_dir}")
        _log.info(f"  è®°å¿†æ•°æ®åº“ç›®å½•: {self.memory_db_dir}")
        _log.info(f"  èŠå¤©è®°å½•ç›®å½•: {self.chat_history_storage_dir}")
        
        # SFTç›¸å…³é…ç½®
        sft_cfg = self.memory_config.get("sft", {})
        self.sft_enabled = bool(sft_cfg.get("enabled", False))
        self.sft_path = sft_cfg.get("dataset_path")
        self.sft_per_epoch = bool(sft_cfg.get("per_epoch", True))
        self.sft_max_per_epoch = sft_cfg.get("max_per_epoch") or None
        self.sft_seed = int(sft_cfg.get("seed", 42))
        export_cfg = self.memory_config.get("export", {})
        self.export_save_full_vl_assets = bool(export_cfg.get("save_full_vl_assets", True))
        self.export_merge_lora = bool(export_cfg.get("merge_lora", True))
        self._memory_entry_count = None
        self._current_epoch_sample_n = None
        self._saved_history_counts = {
            "group": {},
            "private": {}
        }

    # ä¸‹æ–¹ä¿ç•™ä¸æ—§å®ç°ä¸€è‡´çš„å¤§æ®µå‡½æ•°ï¼ˆæå–/ä¿å­˜/è®­ç»ƒ/æ¸…ç†ç­‰ï¼‰ï¼Œä¸ºèŠ‚çœç¯‡å¹…çœç•¥é‡å¤æ³¨é‡Š
    # ç”±äºå†…å®¹è¾ƒå¤šï¼Œè¿™é‡Œç›´æ¥ä»æ—§å®ç°å®Œå…¨è¿ç§»ï¼ˆé€»è¾‘ä¸å˜ï¼‰

    def _prepare_output_dir(self, path: str):
        if os.path.isdir(path):
            _log.info(f"æ¸…ç†å†å²æ¨¡å‹ç›®å½•: {path}")
            shutil.rmtree(path)
        os.makedirs(path, exist_ok=True)

    def _get_latest_trained_model_path(self) -> str:
        """
        è·å–æœ€æ–°çš„æ¨¡å‹è·¯å¾„
        ä¼˜å…ˆçº§ï¼šè®­ç»ƒåçš„æ¨¡å‹ > æ·»åŠ äº†tokençš„æ¨¡å‹ > åŸºç¡€æ¨¡å‹
        """
        # è·å–token_added_model_diré…ç½®
        token_added_model_dir = self.memory_config.get("token_added_model_dir", "./server/models/token_added")
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        server_dir = os.path.dirname(script_dir)
        project_root = os.path.dirname(server_dir)
        
        trained_model_dir = self.trained_model_dir
        if not os.path.isabs(trained_model_dir):
            trained_model_dir = os.path.abspath(os.path.join(project_root, trained_model_dir))
        if not os.path.isabs(token_added_model_dir):
            token_added_model_dir = os.path.abspath(os.path.join(project_root, token_added_model_dir))
        
        # 1. ä¼˜å…ˆæŸ¥æ‰¾è®­ç»ƒåçš„æ¨¡å‹
        if os.path.exists(trained_model_dir):
            model_dirs = [
                d for d in os.listdir(trained_model_dir)
                if os.path.isdir(os.path.join(trained_model_dir, d)) and d.startswith("model_")
            ]
            if model_dirs:
                model_dirs.sort(reverse=True)
                latest_model = os.path.join(trained_model_dir, model_dirs[0])
                _log.info(f"æ‰¾åˆ°æœ€æ–°è®­ç»ƒæ¨¡å‹: {latest_model}")
                return latest_model
        
        # 2. å¦‚æœæ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼ŒæŸ¥æ‰¾æ·»åŠ äº†tokençš„æ¨¡å‹
        if os.path.exists(token_added_model_dir):
            model_dirs = [
                d for d in os.listdir(token_added_model_dir)
                if os.path.isdir(os.path.join(token_added_model_dir, d)) and d.startswith("model_")
            ]
            if model_dirs:
                model_dirs.sort(reverse=True)
                latest_model = os.path.join(token_added_model_dir, model_dirs[0])
                _log.info(f"æ‰¾åˆ°æ·»åŠ äº†tokençš„æ¨¡å‹: {latest_model}")
                return latest_model
        
        # 3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹
        _log.info(f"æœªæ‰¾åˆ°è®­ç»ƒæ¨¡å‹æˆ–æ·»åŠ äº†tokençš„æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {self.base_model_path}")
        return self.base_model_path

    def _create_trained_model_path(self) -> str:
        """
        åˆ›å»ºæ–°çš„è®­ç»ƒæ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œä½¿ç”¨æ—¶é—´æˆ³å‘½åæ ¼å¼ï¼šmodel_YYYYMMDD_HHMMSS
        ç¡®ä¿ä¸åŠ è½½é€»è¾‘åŒ¹é…ï¼ˆæŒ‰æ—¶é—´æˆ³æ’åºé€‰æ‹©æœ€æ–°çš„ï¼‰
        """
        from datetime import datetime
        if not os.path.isabs(self.trained_model_dir):
            script_dir = os.path.dirname(os.path.abspath(__file__))  # memoryç›®å½•
            server_dir = os.path.dirname(script_dir)  # serverç›®å½•
            project_root = os.path.dirname(server_dir)  # é¡¹ç›®æ ¹ç›®å½•
            # è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œç›´æ¥æ‹¼æ¥
            trained_model_dir = os.path.abspath(os.path.join(project_root, self.trained_model_dir))
        else:
            trained_model_dir = self.trained_model_dir
        
        os.makedirs(trained_model_dir, exist_ok=True)
        
        # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºæ–°çš„æ¨¡å‹ç›®å½•åï¼šmodel_YYYYMMDD_HHMMSS
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir_name = f"model_{timestamp}"
        model_path = os.path.join(trained_model_dir, model_dir_name)
        
        _log.info(f"åˆ›å»ºæ–°çš„è®­ç»ƒæ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        return model_path

    def run_training(self, skip_memory_dump: bool = False) -> Optional[str]:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Returns:
            æœ€ç»ˆè®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        """
        _log.info("=" * 60)
        _log.info("å¼€å§‹è®°å¿†è®­ç»ƒæµç¨‹")
        _log.info("=" * 60)
        
        multi_gpu_config = self.config.get("model", {}).get("multi_gpu", {})

        # 0. ä¿å­˜å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°JSONæ–‡ä»¶ï¼ˆè®­ç»ƒå‰è°ƒç”¨ï¼‰
        if skip_memory_dump:
            _log.info("=" * 60)
            _log.info("æ­¥éª¤0: å·²åœ¨APIä¸­ä¿å­˜èŠå¤©è®°å½•ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            _log.info("=" * 60)
        else:
            _log.info("=" * 60)
            _log.info("æ­¥éª¤0: ä¿å­˜å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°JSONæ–‡ä»¶")
            _log.info("=" * 60)
            self.save_memory_chat_histories_to_storage()

        # 1. ä»JSONæ–‡ä»¶åŠ è½½èŠå¤©è®°å½•
        _log.info("=" * 60)
        _log.info("æ­¥éª¤1: åŠ è½½JSONæ–‡ä»¶ä¸­çš„èŠå¤©è®°å½•")
        _log.info("=" * 60)
        chat_messages = self.load_chat_histories_from_json_only()

        # chat_messages æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ {"chat_type": ..., "chat_id": ..., "message": ...}
        if len(chat_messages) == 0:
            _log.warning("âš ï¸ æ²¡æœ‰èŠå¤©è®°å½•å¯è®­ç»ƒï¼Œè·³è¿‡è®­ç»ƒ")
            _log.info("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
            _log.info("   - å†…å­˜ä¸­æ²¡æœ‰èŠå¤©è®°å½•")
            _log.info("   - chat_history_storage_dir ä¸­æ²¡æœ‰JSONæ–‡ä»¶")
            _log.info("   - è¯·æ£€æŸ¥èŠå¤©è®°å½•æ˜¯å¦è¢«æ­£ç¡®ä¿å­˜")
            return None

        # ç»Ÿè®¡èŠå¤©è®°å½•ä¿¡æ¯ï¼ˆæŒ‰èŠå¤©åˆ†ç»„ç»Ÿè®¡ï¼‰
        chat_groups = {}
        for msg_data in chat_messages:
            chat_type = msg_data.get("chat_type", "unknown")
            chat_id = msg_data.get("chat_id", "unknown")
            key = f"{chat_type}_{chat_id}"
            if key not in chat_groups:
                chat_groups[key] = 0
            chat_groups[key] += 1

        total_messages = len(chat_messages)
        _log.info(f"ğŸ“Š æ€»å…± {total_messages} æ¡æ¶ˆæ¯ï¼Œåˆ†å¸ƒåœ¨ {len(chat_groups)} ä¸ªèŠå¤©ç»„ä¸­")

        # 2. ä½¿ç”¨åŸºç¡€æ¨¡å‹æå–è®°å¿†æ¡ç›®å’Œç›‘ç£å‘é‡
        _log.info("=" * 60)
        _log.info("æ­¥éª¤2: ä½¿ç”¨åŸºç¡€æ¨¡å‹æå–è®°å¿†æ¡ç›®å’Œç›‘ç£å‘é‡")
        _log.info("=" * 60)
        _log.info(f"ä½¿ç”¨åŸºç¡€æ¨¡å‹: {self.base_model_path}")
        
        with TrainingModelContext(self.base_model_path, self.device, multi_gpu_config, add_special_tokens=False) as (base_model, base_processor):
            # æå–è®°å¿†æ¡ç›®å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼‰
            temp_training_data_path = self.extract_memory_entries(chat_messages, base_model, base_processor)

            if temp_training_data_path is None or not os.path.exists(temp_training_data_path):
                _log.warning("âš ï¸ æ²¡æœ‰æå–åˆ°è®°å¿†æ¡ç›®æˆ–ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡è®­ç»ƒ")
                _log.info("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
                _log.info("   - æ¨¡å‹åœ¨æå–è®°å¿†æ¡ç›®æ—¶æ²¡æœ‰è¯†åˆ«åˆ°å€¼å¾—è®°å¿†çš„å†…å®¹")
                _log.info("   - æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼ˆè¯·æŸ¥çœ‹ä¸Šé¢çš„æ—¥å¿—ï¼‰")
                _log.info("   - èŠå¤©è®°å½•ä¸­çš„å†…å®¹å¯èƒ½ä¸é€‚åˆæå–ä¸ºè®°å¿†æ¡ç›®")
                return None

            # åŠ è½½è®­ç»ƒæ•°æ®ä»¥è·å–ç»Ÿè®¡ä¿¡æ¯
            training_data = torch.load(temp_training_data_path, map_location='cpu')
            num_entries = len(training_data.get('texts', []))
            _log.info(f"ğŸ“Š æå–åˆ° {num_entries} ä¸ªè®°å¿†æ¡ç›®ï¼Œå·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶")
            # è®¾ç½®æœ¬è½®SFTæ¯epoché‡‡æ ·å‚è€ƒæ•°ï¼ˆä¸è®°å¿†æ¡ç›®æ•°é‡ç­‰é‡ï¼‰
            try:
                self._memory_entry_count = int(num_entries)
                self._current_epoch_sample_n = int(num_entries)
            except Exception:
                self._memory_entry_count = None
                self._current_epoch_sample_n = None

            # ä¿å­˜ç›‘ç£å‘é‡åˆ°MemoryVectorDBï¼ˆä»è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸­æå–ï¼‰
            self.save_memory_embeddings_from_file(temp_training_data_path)

            # åŒæ—¶æå–ç­‰é‡çš„SFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒï¼Œé˜²æ­¢<recall>tokenè¿‡æ‹Ÿåˆ
            sft_vectors_path = self._extract_sft_vectors_for_recall_training(
                num_entries, base_model, base_processor
            )

        # åŸºç¡€æ¨¡å‹ä¸Šä¸‹æ–‡è‡ªåŠ¨æ¸…ç†

        # 3. ä½¿ç”¨æœ€æ–°çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œè®­ç»ƒ
        _log.info("=" * 60)
        _log.info("æ­¥éª¤3: ä½¿ç”¨æœ€æ–°çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œè®­ç»ƒ")
        _log.info("=" * 60)
        if getattr(self, "_memory_entry_count", None):
            self._current_epoch_sample_n = self._memory_entry_count
        
        # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹
        training_model_path = self._get_latest_trained_model_path()
        _log.info(f"è®­ç»ƒæ¨¡å‹è·¯å¾„: {training_model_path}")
        
        with TrainingModelContext(training_model_path, self.device, multi_gpu_config) as (training_model, training_processor):
            # 3.5. æ¸…ç†æ˜¾å­˜ï¼Œç¡®ä¿æ¨¡å‹å¤„äºå¹²å‡€çŠ¶æ€
            _log.info("=" * 60)
            _log.info("æ­¥éª¤3.5: æ¸…ç†æ˜¾å­˜ï¼Œå‡†å¤‡è®­ç»ƒ")
            _log.info("=" * 60)
            
            # ç¡®ä¿æ¨¡å‹å¤„äºevalæ¨¡å¼ï¼Œæ¸…é™¤æ¢¯åº¦
            training_model.eval()
            with torch.no_grad():
                # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼ˆå¤šæ¬¡æ¸…ç†ç¡®ä¿å½»åº•ï¼‰
                import gc
                for _ in range(5):
                    gc.collect()
                
                if torch.cuda.is_available():
                    # æ¸…ç†æ‰€æœ‰GPUçš„æ˜¾å­˜
                    gpu_count = torch.cuda.device_count()
                    _log.info(f"æ¸…ç† {gpu_count} å¼ GPUçš„æ˜¾å­˜...")
                    
                    # åŒæ­¥å¹¶æ¸…ç†æ‰€æœ‰GPU
                    for i in range(gpu_count):
                        with torch.cuda.device(i):
                            torch.cuda.synchronize()
                            torch.cuda.empty_cache()
                            torch.cuda.reset_peak_memory_stats()
                    
                    # å†æ¬¡æ¸…ç†æ‰€æœ‰GPU
                    for i in range(gpu_count):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                    
                    _log.info(f"âœ… å·²æ¸…ç†æ‰€æœ‰ {gpu_count} å¼ GPUçš„æ˜¾å­˜")
            
            _log.info("âœ… æ˜¾å­˜æ¸…ç†å®Œæˆï¼Œæ¨¡å‹å·²å‡†å¤‡å°±ç»ª")

            # 4. ç¬¬ä¸€æ­¥è®­ç»ƒï¼š<recall> tokenè®­ç»ƒï¼ˆä½¿ç”¨æœ€æ–°çš„è®­ç»ƒæ¨¡å‹ï¼‰
            step1_model_path = self.train_recall_token(temp_training_data_path, training_model, training_processor, sft_vectors_path)

            # 5. ç¬¬äºŒæ­¥è®­ç»ƒï¼šè®°å¿†è§£ç è®­ç»ƒï¼ˆé‡æ–°åŠ è½½ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
            final_model_path = self.train_memory_decoding(temp_training_data_path, step1_model_path)

            # 6. æŒ‰æ—¶é—´æˆ³ä¿å­˜æœ€ç»ˆæ¨¡å‹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            final_save_path = os.path.join(self.trained_model_dir, f"model_{timestamp}")

            import shutil
            if os.path.exists(final_save_path):
                shutil.rmtree(final_save_path)
            shutil.copytree(final_model_path, final_save_path)

            # ä¿å­˜Processoré…ç½®åˆ°æœ€ç»ˆæ¨¡å‹ç›®å½•
            self._save_processor_to_path(final_save_path)
            if self.export_save_full_vl_assets:
                self._ensure_full_vl_assets(final_save_path)

            _log.info(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {final_save_path}")

            # 7. æ¸…ç†è®­ç»ƒæ•°æ®å’Œç¼“å­˜ï¼ˆè®­ç»ƒæ¨¡å‹ç”±ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨æ¸…ç†ï¼‰
            self.cleanup_after_training()

            _log.info("=" * 60)
            _log.info("è®°å¿†è®­ç»ƒæµç¨‹å®Œæˆ")
            _log.info("=" * 60)

            # è®­ç»ƒå®Œæˆåæ¸…ç†ä¸Šä¼ ç¼“å­˜ï¼ˆå›¾ç‰‡/è§†é¢‘ï¼‰ï¼Œé¿å…é•¿æœŸå ç”¨ç£ç›˜
            try:
                script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # server/
                uploaded_images = os.path.join(script_dir, "uploaded_images")
                uploaded_videos = os.path.join(script_dir, "uploaded_videos")
                cleaned = 0
                for d in [uploaded_images, uploaded_videos]:
                    if os.path.isdir(d):
                        for fname in os.listdir(d):
                            fpath = os.path.join(d, fname)
                            try:
                                os.remove(fpath)
                                cleaned += 1
                            except Exception:
                                pass
                if cleaned:
                    _log.info(f"âœ… è®­ç»ƒå®Œæˆåå·²æ¸…ç©ºç¼“å­˜æ–‡ä»¶ {cleaned} ä¸ªï¼ˆimages/videosï¼‰")
            except Exception as ce:
                _log.warning(f"âš ï¸ æ¸…ç†ä¸Šä¼ ç¼“å­˜å¤±è´¥: {ce}")

            return final_save_path

    def _save_processor_to_path(self, target_path: str):
        """
        ä¿å­˜å®Œæ•´çš„Processoré…ç½®åˆ°ç›®æ ‡è·¯å¾„
        ç¡®ä¿ä½¿ç”¨è®­ç»ƒåçš„tokenizerï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰ï¼ŒåŒæ—¶ä¿ç•™processorçš„å…¶ä»–é…ç½®
        """
        try:
            base_path = self.base_model_path
            if not os.path.isabs(base_path):
                script_dir = os.path.dirname(os.path.abspath(__file__))
                project_root = os.path.dirname(script_dir)
                base_path = os.path.abspath(os.path.join(project_root, base_path))
            
            # 1. ä»åŸºç¡€æ¨¡å‹åŠ è½½processorï¼ˆåŒ…å«image_processorã€video_processorç­‰é…ç½®ï¼‰
            base_processor = AutoProcessor.from_pretrained(
                base_path,
                trust_remote_code=True,
                local_files_only=True
            )
            
            # 2. ä»è®­ç»ƒåçš„æ¨¡å‹åŠ è½½tokenizerï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰
            trained_tokenizer = None
            if os.path.exists(target_path):
                try:
                    # å°è¯•åŠ è½½è®­ç»ƒåçš„tokenizer
                    from transformers import AutoTokenizer
                    trained_tokenizer = AutoTokenizer.from_pretrained(
                        target_path,
                        trust_remote_code=True,
                        local_files_only=True
                    )
                    _log.info("âœ… å·²åŠ è½½è®­ç»ƒåçš„tokenizerï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰")
                except Exception as e:
                    _log.warning(f"âš ï¸ åŠ è½½è®­ç»ƒåçš„tokenizerå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„tokenizer")
            
            # 3. å¦‚æœè®­ç»ƒåçš„tokenizerå­˜åœ¨ï¼Œæ›´æ–°processorçš„tokenizer
            if trained_tokenizer is not None:
                base_processor.tokenizer = trained_tokenizer

            # 4. ä¿å­˜å®Œæ•´çš„processoré…ç½®ï¼ˆåŒ…å«è®­ç»ƒåçš„tokenizerå’Œå…¶ä»–processorç»„ä»¶ï¼‰
            base_processor.save_pretrained(target_path)

            # 5. ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶éƒ½è¢«æ­£ç¡®ä¿å­˜ï¼ˆåœ¨save_pretrainedä¹‹åï¼Œç¡®ä¿ä¸è¢«è¦†ç›–ï¼‰
            # è¿™äº›æ–‡ä»¶å¯¹äºQwen3VLProcessorçš„æ­£ç¡®å·¥ä½œè‡³å…³é‡è¦
            import shutil
            essential_files = [
                "chat_template.json",
                "preprocessor_config.json",
                "video_preprocessor_config.json"
            ]
            for file_name in essential_files:
                source_file = os.path.join(base_path, file_name)
                target_file = os.path.join(target_path, file_name)
                if os.path.exists(source_file):
                    try:
                        shutil.copy2(source_file, target_file)
                        _log.info(f"âœ… å·²å¤åˆ¶{file_name}åˆ°: {target_path}")
                    except Exception as e:
                        _log.warning(f"âš ï¸ å¤åˆ¶{file_name}å¤±è´¥: {e}")
                else:
                    _log.warning(f"âš ï¸ åŸºç¡€æ¨¡å‹ä¸­ä¸å­˜åœ¨{file_name}ï¼Œè·³è¿‡å¤åˆ¶")
            _log.info(f"âœ… å·²ä¿å­˜Processoré…ç½®åˆ°: {target_path}ï¼ˆåŒ…å«è®­ç»ƒåçš„tokenizerï¼‰")
            
        except Exception as e:
            _log.warning(f"âš ï¸ ä¿å­˜Processoré…ç½®å¤±è´¥: {e}")

    def _ensure_full_vl_assets(self, output_dir: str):
        if not self.export_save_full_vl_assets:
            return
        try:
            base_path = self.base_model_path
            if not os.path.isabs(base_path):
                script_dir = os.path.dirname(os.path.abspath(__file__))
                project_root = os.path.dirname(script_dir)
                base_path = os.path.abspath(os.path.join(project_root, base_path))
            required_files = [
                "config.json",
                "generation_config.json",
                "tokenizer.json",
                "tokenizer.model",
                "tokenizer_config.json",
                "special_tokens_map.json",
                "preprocessor_config.json",
                "video_preprocessor_config.json",
                "processor_config.json",
            ]
            required_dirs = [
                "image_processor",
                "processor",
            ]
            os.makedirs(output_dir, exist_ok=True)
            for fname in required_files:
                src = os.path.join(base_path, fname)
                if os.path.exists(src):
                    dst = os.path.join(output_dir, fname)
                    if not os.path.exists(dst):
                        try:
                            shutil.copy2(src, dst)
                            _log.info(f"âœ… å¤åˆ¶ç¼ºå¤±æ–‡ä»¶: {fname}")
                        except Exception as ce:
                            _log.warning(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥ {fname}: {ce}")
            for dname in required_dirs:
                srcd = os.path.join(base_path, dname)
                dstd = os.path.join(output_dir, dname)
                if os.path.isdir(srcd) and not os.path.exists(dstd):
                    try:
                        shutil.copytree(srcd, dstd)
                        _log.info(f"âœ… å¤åˆ¶ç›®å½•: {dname}")
                    except Exception as ce:
                        _log.warning(f"å¤åˆ¶ç›®å½•å¤±è´¥ {dname}: {ce}")
        except Exception as e:
            _log.warning(f"âš ï¸ ç¡®ä¿VLèµ„äº§æ—¶å‡ºé”™: {e}")

    def _resolve_dataset_path(self, path_str: str) -> str:
        if not path_str:
            return None
        if os.path.isabs(path_str):
            return path_str
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        abs_path = os.path.abspath(os.path.join(project_root, path_str))
        return abs_path

    def _load_sft_dataset(self) -> List[Dict[str, Any]]:
        """ä»jsonlåŠ è½½é€šç”¨SFTæ ·æœ¬ï¼Œå…¼å®¹å¸¸è§schema"""
        dataset_path = self._resolve_dataset_path(self.sft_path)
        if not dataset_path or not os.path.exists(dataset_path):
            _log.warning(f"âš ï¸ SFTæ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
            return []
        samples = []
        try:
            with open(dataset_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        samples.append(obj)
                    except Exception:
                        continue
        except Exception as e:
            _log.warning(f"åŠ è½½SFTæ•°æ®é›†å¤±è´¥: {e}")
            return []
        _log.info(f"âœ… åŠ è½½SFTæ ·æœ¬: {len(samples)}")
        return samples
    
    def _standardize_sft_messages(self, sample: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ ‡å‡†åŒ–ä¸ºmessagesåˆ—è¡¨ï¼š[{'role': 'user'|'assistant'|'system', 'content': [{'type':'text','text':...}]}]
        ä»…æ–‡æœ¬ï¼Œä¸å¤„ç†å›¾ç‰‡ã€‚
        """
        # 1) å¦‚æœå·²æœ‰messages
        msgs = sample.get("messages")
        if isinstance(msgs, list) and msgs:
            std = []
            for m in msgs:
                role = m.get("role", "user")
                content = m.get("content") or m.get("text") or ""
                if isinstance(content, str):
                    std.append({"role": role, "content": [{"type": "text", "text": content}]})
                elif isinstance(content, list):
                    # å‡è®¾listæ–‡æœ¬
                    text_join = ""
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            text_join += item.get("text", "")
                        elif isinstance(item, str):
                            text_join += item
                    std.append({"role": role, "content": [{"type": "text", "text": text_join}]})
            if std:
                return std
        # 2) instruction + output
        inst = sample.get("instruction") or sample.get("input")
        out = sample.get("output") or sample.get("answer")
        if isinstance(inst, str) and isinstance(out, str):
            return [
                {"role": "user", "content": [{"type": "text", "text": inst}]},
                {"role": "assistant", "content": [{"type": "text", "text": out}]},
            ]
        # 3) å•è½®é—®ç­”é£æ ¼
        q = sample.get("query") or sample.get("question")
        a = sample.get("response") or sample.get("answer")
        if isinstance(q, str) and isinstance(a, str):
            return [
                {"role": "user", "content": [{"type": "text", "text": q}]},
                {"role": "assistant", "content": [{"type": "text", "text": a}]},
            ]
        return []
    
    def _build_simple_sft_batch(self, processor, messages: List[List[Dict[str, Any]]]):
        """
        ç®€å•SFTæ‰¹å¤„ç†ï¼šå°†messagesè½¬æˆinput_idså¹¶ç›´æ¥ç”¨è‡ªå›å½’æ ‡ç­¾ï¼ˆä¸åŒºåˆ†maskï¼‰ã€‚
        å¦‚æœmessagesä¸­åŒ…å«å­—ç¬¦ä¸²æ ¼å¼çš„æ–‡æœ¬ï¼ˆç”¨äº<recall> tokenè®­ç»ƒï¼‰ï¼Œåˆ™åªè®©<recall> tokenå‚ä¸è®­ç»ƒã€‚
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å­—ç¬¦ä¸²æ ¼å¼çš„æ–‡æœ¬ï¼ˆç”¨äº<recall> tokenè®­ç»ƒï¼‰
        recall_token_id = None
        try:
            recall_token_id = processor.tokenizer.convert_tokens_to_ids("<recall>")
        except:
            pass
        
        batch_input_ids = []
        batch_attention_mask = []
        batch_labels = []
        
        for msg in messages:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆç”¨äº<recall> tokenè®­ç»ƒï¼‰
            if isinstance(msg, str):
                # ç›´æ¥tokenizeæ–‡æœ¬
                encoded = processor.tokenizer(
                    msg,
                    return_tensors="pt",
                    add_special_tokens=True,
                    padding=False,
                    truncation=False
                )
                input_ids = encoded["input_ids"][0]  # [seq_len]
                attention_mask = torch.ones_like(input_ids)
                
                # åˆ›å»ºlabelsï¼Œé»˜è®¤å…¨éƒ¨maskï¼ˆ-100ï¼‰
                labels = torch.full_like(input_ids, -100)
                
                # æ‰¾åˆ°<recall> tokençš„ä½ç½®ï¼Œåªè®©è¿™ä¸ªtokenå‚ä¸è®­ç»ƒ
                if recall_token_id is not None:
                    recall_positions = (input_ids == recall_token_id).nonzero(as_tuple=True)[0]
                    if len(recall_positions) > 0:
                        # åªè®©æœ€åä¸€ä¸ª<recall> tokenå‚ä¸è®­ç»ƒ
                        last_recall_pos = recall_positions[-1].item()
                        labels[last_recall_pos] = input_ids[last_recall_pos]
                        _log.debug(f"æ‰¾åˆ°<recall> tokenä½ç½®: {last_recall_pos}, å·²è®¾ç½®ä¸ºå‚ä¸è®­ç»ƒ")
                    else:
                        _log.warning(f"âš ï¸ æ–‡æœ¬ä¸­æœªæ‰¾åˆ°<recall> token: {msg}")
                
                batch_input_ids.append(input_ids)
                batch_attention_mask.append(attention_mask)
                batch_labels.append(labels)
            else:
                # åŸæœ‰çš„messagesæ ¼å¼å¤„ç†
                batch_inputs = processor.apply_chat_template(
                    [msg], tokenize=True, add_generation_prompt=False,
                    return_dict=True, return_tensors="pt"
                )
                input_ids = batch_inputs["input_ids"][0]  # [seq_len]
                attention_mask = batch_inputs.get("attention_mask", (input_ids != 0).long())[0]
                labels = input_ids.clone()
                
                batch_input_ids.append(input_ids)
                batch_attention_mask.append(attention_mask)
                batch_labels.append(labels)
        
        # å¯¹batchè¿›è¡Œpadding
        max_len = max(len(ids) for ids in batch_input_ids)
        padded_input_ids = []
        padded_attention_mask = []
        padded_labels = []
        
        for i in range(len(batch_input_ids)):
            pad_len = max_len - len(batch_input_ids[i])
            padded_input_ids.append(torch.cat([batch_input_ids[i], torch.zeros(pad_len, dtype=batch_input_ids[i].dtype)]))
            padded_attention_mask.append(torch.cat([batch_attention_mask[i], torch.zeros(pad_len, dtype=batch_attention_mask[i].dtype)]))
            padded_labels.append(torch.cat([batch_labels[i], torch.full((pad_len,), -100, dtype=batch_labels[i].dtype)]))
        
        input_ids = torch.stack(padded_input_ids)
        attention_mask = torch.stack(padded_attention_mask)
        labels = torch.stack(padded_labels)
        
        return input_ids, attention_mask, labels
    
    def _run_sft_one_epoch(self, trainer_obj, epoch: int, epoch_sample_n: int):
        """ä½¿ç”¨è®­ç»ƒé˜¶æ®µçš„LoRAæ¨¡å‹ï¼Œè·‘1ä¸ªepochçš„é€šç”¨SFTï¼ˆé‡å»ºä¼˜åŒ–å™¨ï¼Œæƒé‡è¿ç»­ç´¯ç§¯ï¼‰
        
        Args:
            trainer_obj: è®­ç»ƒå™¨å¯¹è±¡
            epoch: å½“å‰epochç¼–å·ï¼ˆç”¨äºæ”¹å˜éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡é‡‡æ ·ä¸åŒï¼‰
            epoch_sample_n: é‡‡æ ·æ•°é‡ï¼ˆä¸è®°å¿†æ¡ç›®æ•°é‡ç›¸åŒï¼‰
        """
        if not self.sft_enabled or not self.sft_per_epoch:
            return
        try:
            # åœ¨SFTè®­ç»ƒå‰ï¼Œæ¸…ç†è®°å¿†è®­ç»ƒå¯èƒ½æ®‹ç•™çš„æ˜¾å­˜
            import torch
            import gc
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            _log.debug("ğŸ§¹ SFTè®­ç»ƒå‰å·²æ¸…ç†æ˜¾å­˜ç¼“å­˜")
            
            # å–å‡ºå¥æŸ„
            handles = getattr(trainer_obj, "expose_training_handles", None)
            if not callable(handles):
                _log.warning("âš ï¸ è®­ç»ƒå™¨æœªæš´éœ²expose_training_handlesï¼Œè·³è¿‡æœ¬æ¬¡SFT")
                return
            handle = trainer_obj.expose_training_handles()
            model = handle.get("model") or handle.get("base_model")
            tokenizer = handle.get("tokenizer")
            accelerator = handle.get("accelerator", None)
            grad_acc_steps = getattr(trainer_obj, "gradient_accumulation_steps", 1)
            if model is None or tokenizer is None:
                _log.warning("âš ï¸ SFTå¥æŸ„ç¼ºå¤±ï¼Œè·³è¿‡")
                return
            # åŠ è½½æ ·æœ¬ä¸é‡‡æ ·
            all_samples = self._load_sft_dataset()
            if not all_samples:
                _log.warning("âš ï¸ æ— SFTæ•°æ®ï¼Œè·³è¿‡")
                return
            # ä½¿ç”¨epochç¼–å·æ¥æ”¹å˜éšæœºç§å­ï¼Œç¡®ä¿æ¯ä¸ªepoché‡‡æ ·ä¸åŒçš„æ ·æœ¬
            random.seed(self.sft_seed + epoch)
            # é‡‡æ ·æ•°é‡ä¸è®°å¿†æ¡ç›®æ•°é‡ç›¸åŒï¼ˆepoch_sample_nå·²ä¼ å…¥ï¼‰
            sample_n = min(epoch_sample_n, len(all_samples)) if epoch_sample_n else len(all_samples)
            if self.sft_max_per_epoch is not None:
                sample_n = min(sample_n, int(self.sft_max_per_epoch))
            picked = random.sample(all_samples, sample_n)
            std_msgs = []
            for s in picked:
                m = self._standardize_sft_messages(s)
                if m:
                    std_msgs.append(m)
            
            if not std_msgs:
                _log.warning("âš ï¸ æœ¬è½®SFTæ— æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡")
                return
            # è·å–SFTè®­ç»ƒçš„batch_sizeï¼ˆé»˜è®¤ä¸º1ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            sft_batch_size = self.training_config.get("sft_batch_size", 1)
            _log.info(f"ğŸ§ª æœ¬epochæ’å…¥SFT: {len(std_msgs)} æ¡ (batch_size={sft_batch_size})")

            # æ„å»ºå°æ‰¹æ•°æ®
            model.train()
            optim = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=float(self.training_config.get("learning_rate", 1e-4)))
            accumulation = 0
            total_loss = 0.0
            steps = 0

            # ä½¿ç”¨tqdmè¿›åº¦æ¡
            try:
                from tqdm import tqdm
                use_tqdm = True
            except ImportError:
                use_tqdm = False
                progress_interval = max(1, len(std_msgs) // 10)  # æ¯10%æ‰“å°ä¸€æ¬¡è¿›åº¦

            if use_tqdm:
                pbar = tqdm(total=len(std_msgs), desc="ğŸ§ª SFTè®­ç»ƒ", unit="æ ·æœ¬")

            # æŒ‰batch_sizeåˆ†ç»„å¤„ç†
            for i in range(0, len(std_msgs), sft_batch_size):
                batch_end = min(i + sft_batch_size, len(std_msgs))
                batch_msgs = std_msgs[i:batch_end]
                actual_batch_size = len(batch_msgs)

                input_ids, attention_mask, labels = self._build_simple_sft_batch(tokenizer, batch_msgs)
                device = next(model.parameters()).device
                input_ids = input_ids.to(device)
                attention_mask = attention_mask.to(device)
                labels = labels.to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss / grad_acc_steps
                if accelerator is not None:
                    accelerator.backward(loss)
                else:
                    loss.backward()
                accumulation += 1
                if accumulation % grad_acc_steps == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optim.step()
                    optim.zero_grad()
                    steps += 1
                    total_loss += loss.item() * grad_acc_steps

                    # æ›´æ–°è¿›åº¦æ¡æˆ–æ‰“å°è¿›åº¦
                    if use_tqdm:
                        pbar.update(actual_batch_size)
                        pbar.set_postfix({'loss': f'{total_loss/steps:.4f}'})
                    else:
                        # æ‰“å°è¿›åº¦ï¼ˆå…¼å®¹æ²¡æœ‰tqdmçš„æƒ…å†µï¼‰
                        if steps % progress_interval == 0 or steps == (len(std_msgs) // grad_acc_steps + 1):
                            progress = (steps * grad_acc_steps) / len(std_msgs) * 100
                            avg_loss_so_far = total_loss / steps
                            _log.info(f"ğŸ§ª SFTè¿›åº¦: {progress:.1f}% ({steps * grad_acc_steps}/{len(std_msgs)}), å½“å‰loss={avg_loss_so_far:.6f}")

            if accumulation % grad_acc_steps != 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optim.step()
                optim.zero_grad()
                steps += 1
                total_loss += loss.item() * grad_acc_steps

            if use_tqdm:
                pbar.close()

            avg_loss = total_loss / max(steps, 1)
            _log.info(f"âœ… æœ¬epoch SFTå®Œæˆï¼Œavg_loss={avg_loss:.6f}, steps={steps}")
            
            # SFTè®­ç»ƒç»“æŸåï¼Œæ¸…ç†SFTæ•°æ®ä»¥é‡Šæ”¾æ˜¾å­˜ï¼Œä¸ºä¸‹ä¸€ä¸ªepochçš„è®°å¿†è®­ç»ƒåšå‡†å¤‡
            import torch
            import gc
            # æ¸…ç†SFTè®­ç»ƒä¸­åˆ›å»ºçš„tensor
            del input_ids, attention_mask, labels, outputs, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            _log.debug("ğŸ§¹ SFTè®­ç»ƒåå·²æ¸…ç†æ˜¾å­˜ç¼“å­˜")
        except Exception as e:
            _log.warning(f"âš ï¸ SFTæ‰§è¡Œå¤±è´¥ï¼Œå·²è·³è¿‡: {e}", exc_info=True)
    
    def save_memory_chat_histories_to_storage(self):
        """
        å°†å†…å­˜ä¸­çš„èŠå¤©è®°å½•ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆè®­ç»ƒå‰è°ƒç”¨ï¼‰
        è¿™æ ·å¯ä»¥ç¡®ä¿è®­ç»ƒæ—¶ä½¿ç”¨æœ€æ–°çš„èŠå¤©è®°å½•ï¼ŒåŒæ—¶æ¸…ç†å†…å­˜
        
        æ³¨æ„ï¼šæ­¤å‡½æ•°é€šè¿‡æ¨¡å—å¯¼å…¥è·å–å…¨å±€å˜é‡ï¼Œå¯èƒ½è·å–ä¸åˆ°è¿è¡Œæ—¶çš„å¯¹è±¡ã€‚
        å¦‚æœè·å–ä¸åˆ°æ•°æ®ï¼Œè¯·ä½¿ç”¨api_serverçš„å…¨å±€å‡½æ•°ç›´æ¥ä¿å­˜ã€‚
        """
        _log.info("å¼€å§‹ä¿å­˜å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°å­˜å‚¨...")
        
        # ä½¿ç”¨å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
        import importlib
        import sys
        
        # å°è¯•å¤šç§æ–¹å¼å¯¼å…¥api_server_qwen3vlæ¨¡å—
        api_server = None
        try:
            # æ–¹å¼1ï¼šç›´æ¥å¯¼å…¥ï¼ˆå¦‚æœå·²ç»åœ¨sys.modulesä¸­ï¼‰- è¿™æ˜¯æœ€å¯é çš„æ–¹å¼
            if 'api_server_qwen3vl' in sys.modules:
                api_server = sys.modules['api_server_qwen3vl']
                _log.info("ä»sys.modulesè·å–api_server_qwen3vlæ¨¡å—")
            else:
                # æ–¹å¼2ï¼šä½¿ç”¨importlibå¯¼å…¥
                api_server = importlib.import_module('api_server_qwen3vl')
                _log.info("ä½¿ç”¨importlibå¯¼å…¥api_server_qwen3vlæ¨¡å—")
        except Exception as e:
            _log.error(f"å¯¼å…¥api_server_qwen3vlæ¨¡å—å¤±è´¥: {e}")
            # æ–¹å¼3ï¼šå°è¯•ä»serverç›®å½•å¯¼å…¥
            try:
                script_dir = os.path.dirname(os.path.abspath(__file__))
                if script_dir not in sys.path:
                    sys.path.insert(0, script_dir)
                api_server = importlib.import_module('api_server_qwen3vl')
                _log.info("ä»serverç›®å½•å¯¼å…¥api_server_qwen3vlæ¨¡å—")
            except Exception as e2:
                _log.error(f"ä»serverç›®å½•å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}")
                _log.warning("âš ï¸ æ— æ³•å¯¼å…¥æ¨¡å—ï¼Œæ— æ³•ä¿å­˜å†…å­˜ä¸­çš„èŠå¤©è®°å½•")
                return
        
        if api_server is None:
            _log.error("æ— æ³•å¯¼å…¥api_server_qwen3vlæ¨¡å—ï¼Œè·³è¿‡ä¿å­˜")
            return
        
        # è·å–å…¨å±€å˜é‡ï¼ˆæ³¨æ„ï¼šå¦‚æœæ¨¡å—é‡æ–°å¯¼å…¥ï¼Œå¯èƒ½è·å–ä¸åˆ°è¿è¡Œæ—¶çš„å¯¹è±¡ï¼‰
        group_chat_histories = getattr(api_server, 'group_chat_histories', {})
        private_chat_histories = getattr(api_server, 'private_chat_histories', {})
        save_chat_history_to_storage = getattr(api_server, 'save_chat_history_to_storage', None)
        
        # æ£€æŸ¥æ˜¯å¦è·å–åˆ°äº†è¿è¡Œæ—¶çš„å¯¹è±¡ï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹ï¼‰
        # å¦‚æœè·å–åˆ°çš„å¯¹è±¡æ˜¯ç©ºå­—å…¸ï¼Œå¯èƒ½æ˜¯æ¨¡å—å¯¼å…¥é—®é¢˜ï¼Œè®°å½•è­¦å‘Š
        if isinstance(group_chat_histories, dict) and len(group_chat_histories) == 0 and isinstance(private_chat_histories, dict) and len(private_chat_histories) == 0:
            _log.warning("âš ï¸ è·å–åˆ°çš„èŠå¤©è®°å½•ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ¨¡å—å¯¼å…¥é—®é¢˜")
            _log.warning("ğŸ’¡ æç¤ºï¼šå¦‚æœç¡®å®æœ‰èŠå¤©è®°å½•ï¼Œè¯·æ£€æŸ¥æ¨¡å—å¯¼å…¥æ˜¯å¦æ­£ç¡®")
        
        _log.info(f"ğŸ“Š å†…å­˜ä¸­çš„èŠå¤©è®°å½•ç»Ÿè®¡:")
        _log.info(f"   ç¾¤èŠæ•°é‡: {len(group_chat_histories)}")
        _log.info(f"   ç§èŠæ•°é‡: {len(private_chat_histories)}")
        
        # è¯¦ç»†ç»Ÿè®¡æ¯ä¸ªèŠå¤©çš„æ¶ˆæ¯æ•°
        for chat_id, history in group_chat_histories.items():
            _log.info(f"   ç¾¤èŠ {chat_id}: {len(history)} æ¡æ¶ˆæ¯")
        for chat_id, history in private_chat_histories.items():
            _log.info(f"   ç§èŠ {chat_id}: {len(history)} æ¡æ¶ˆæ¯")
        
        # å¦‚æœè·å–åˆ°çš„è®°å½•ä¸ºç©ºï¼Œç›´æ¥è¿”å›ï¼ˆä¸ä¿å­˜ï¼‰
        if len(group_chat_histories) == 0 and len(private_chat_histories) == 0:
            _log.warning("âš ï¸ è·å–åˆ°çš„èŠå¤©è®°å½•ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            _log.warning("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸ºæ¨¡å—å¯¼å…¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æˆ–ä½¿ç”¨æ‰‹åŠ¨ä¿å­˜API")
            return
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        # ä¼˜å…ˆä½¿ç”¨api_serverçš„CHAT_HISTORY_STORAGE_DIRï¼ˆè¿™æ˜¯å®é™…ä¿å­˜æ–‡ä»¶çš„ç›®å½•ï¼‰
        # å¦‚æœè·å–ä¸åˆ°ï¼Œä½¿ç”¨training_serviceé…ç½®çš„ç›®å½•
        chat_history_storage_dir = getattr(api_server, 'CHAT_HISTORY_STORAGE_DIR', None)
        if not chat_history_storage_dir:
            chat_history_storage_dir = self.chat_history_storage_dir
            _log.warning(f"âš ï¸ æ— æ³•è·å–api_serverçš„CHAT_HISTORY_STORAGE_DIRï¼Œä½¿ç”¨é…ç½®ç›®å½•: {chat_history_storage_dir}")
        else:
            _log.info(f"âœ… ä½¿ç”¨api_serverçš„CHAT_HISTORY_STORAGE_DIR: {chat_history_storage_dir}")
        
        if chat_history_storage_dir:
            os.makedirs(chat_history_storage_dir, exist_ok=True)
            _log.info(f"âœ… ç¡®ä¿èŠå¤©è®°å½•å­˜å‚¨ç›®å½•å­˜åœ¨: {chat_history_storage_dir}")
        
        saved_count = 0

        def _get_pending_messages(chat_type_key: str, chat_id: str, history: List[Dict[str, Any]]):
            last_saved = self._saved_history_counts[chat_type_key].get(chat_id, 0)
            if last_saved < 0 or last_saved > len(history):
                last_saved = 0
            if last_saved == len(history):
                return [], len(history)
            return history[last_saved:], len(history)
        
        # ä¼˜å…ˆä½¿ç”¨api_serverçš„save_chat_history_to_storageå‡½æ•°ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç›®å½•ï¼‰
        if save_chat_history_to_storage:
            # ä¿å­˜ç¾¤èŠè®°å½•
            for chat_id, history in group_chat_histories.items():
                if not history:  # åªä¿å­˜éç©ºè®°å½•
                    continue
                pending_messages, final_len = _get_pending_messages("group", chat_id, history)
                if not pending_messages:
                    continue
                try:
                    save_chat_history_to_storage("group", chat_id, pending_messages)
                    saved_count += len(pending_messages)
                    self._saved_history_counts["group"][chat_id] = final_len
                    _log.info(f"âœ… ä¿å­˜ç¾¤èŠ {chat_id} çš„ {len(pending_messages)} æ¡æ–°æ¶ˆæ¯åˆ° {chat_history_storage_dir}")
                except Exception as e:
                    _log.warning(f"ä¿å­˜ç¾¤èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
                    # å¦‚æœä¿å­˜å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¿å­˜
                    try:
                        self._save_chat_history_directly("group", chat_id, pending_messages)
                        saved_count += len(pending_messages)
                        self._saved_history_counts["group"][chat_id] = final_len
                        _log.info(f"âœ… ä½¿ç”¨ç›´æ¥ä¿å­˜æ–¹å¼æˆåŠŸä¿å­˜ç¾¤èŠ {chat_id} çš„ {len(pending_messages)} æ¡æ–°æ¶ˆæ¯")
                    except Exception as e2:
                        _log.error(f"ç›´æ¥ä¿å­˜ä¹Ÿå¤±è´¥: {e2}", exc_info=True)
            
            # ä¿å­˜ç§èŠè®°å½•
            for chat_id, history in private_chat_histories.items():
                if not history:  # åªä¿å­˜éç©ºè®°å½•
                    continue
                pending_messages, final_len = _get_pending_messages("private", chat_id, history)
                if not pending_messages:
                    continue
                try:
                    save_chat_history_to_storage("private", chat_id, pending_messages)
                    saved_count += len(pending_messages)
                    self._saved_history_counts["private"][chat_id] = final_len
                    _log.info(f"âœ… ä¿å­˜ç§èŠ {chat_id} çš„ {len(pending_messages)} æ¡æ–°æ¶ˆæ¯åˆ° {chat_history_storage_dir}")
                except Exception as e:
                    _log.warning(f"ä¿å­˜ç§èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
                    # å¦‚æœä¿å­˜å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¿å­˜
                    try:
                        self._save_chat_history_directly("private", chat_id, pending_messages)
                        saved_count += len(pending_messages)
                        self._saved_history_counts["private"][chat_id] = final_len
                        _log.info(f"âœ… ä½¿ç”¨ç›´æ¥ä¿å­˜æ–¹å¼æˆåŠŸä¿å­˜ç§èŠ {chat_id} çš„ {len(pending_messages)} æ¡æ–°æ¶ˆæ¯")
                    except Exception as e2:
                        _log.error(f"ç›´æ¥ä¿å­˜ä¹Ÿå¤±è´¥: {e2}", exc_info=True)
        else:
            # å¦‚æœæ— æ³•ä½¿ç”¨api_serverçš„å‡½æ•°ï¼Œç›´æ¥ä¿å­˜
            _log.warning("æ— æ³•æ‰¾åˆ°save_chat_history_to_storageå‡½æ•°ï¼Œä½¿ç”¨ç›´æ¥ä¿å­˜æ–¹å¼...")
            for chat_id, history in group_chat_histories.items():
                if history:
                    pending_messages, final_len = _get_pending_messages("group", chat_id, history)
                    if not pending_messages:
                        continue
                    try:
                        self._save_chat_history_directly("group", chat_id, pending_messages)
                        saved_count += len(pending_messages)
                        self._saved_history_counts["group"][chat_id] = final_len
                        _log.info(f"âœ… ç›´æ¥ä¿å­˜ç¾¤èŠ {chat_id} çš„ {len(pending_messages)} æ¡æ–°æ¶ˆæ¯")
                    except Exception as e:
                        _log.warning(f"ç›´æ¥ä¿å­˜ç¾¤èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
            for chat_id, history in private_chat_histories.items():
                if history:
                    pending_messages, final_len = _get_pending_messages("private", chat_id, history)
                    if not pending_messages:
                        continue
                    try:
                        self._save_chat_history_directly("private", chat_id, pending_messages)
                        saved_count += len(pending_messages)
                        self._saved_history_counts["private"][chat_id] = final_len
                        _log.info(f"âœ… ç›´æ¥ä¿å­˜ç§èŠ {chat_id} çš„ {len(pending_messages)} æ¡æ–°æ¶ˆæ¯")
                    except Exception as e:
                        _log.warning(f"ç›´æ¥ä¿å­˜ç§èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
        
        _log.info(f"âœ… å…±ä¿å­˜ {saved_count} æ¡å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°å­˜å‚¨")
    
    def load_chat_histories_from_json_only(self) -> List[Dict[str, Any]]:
        """
        åªä»JSONæ–‡ä»¶åŠ è½½èŠå¤©è®°å½•ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼Œä¸ä»å†…å­˜åŠ è½½ï¼‰
        
        Returns:
            æ‰€æœ‰èŠå¤©è®°å½•çš„åˆ—è¡¨
        """
        all_messages = []
        json_count = 0
        
        # ä½¿ç”¨api_serverçš„CHAT_HISTORY_STORAGE_DIRï¼ˆå¦‚æœå¯ç”¨ï¼‰
        import importlib
        import sys
        
        chat_history_storage_dir = self.chat_history_storage_dir
        api_server = None
        if 'api_server_qwen3vl' in sys.modules:
            api_server = sys.modules['api_server_qwen3vl']
        else:
            try:
                api_server = importlib.import_module('api_server_qwen3vl')
            except:
                pass
        
        if api_server:
            api_storage_dir = getattr(api_server, 'CHAT_HISTORY_STORAGE_DIR', None)
            if api_storage_dir and os.path.exists(api_storage_dir):
                chat_history_storage_dir = api_storage_dir
                _log.info(f"âœ… ä½¿ç”¨api_serverçš„CHAT_HISTORY_STORAGE_DIR: {chat_history_storage_dir}")
        
        # åŠ è½½JSONæ–‡ä»¶
        _log.info(f"æ£€æŸ¥èŠå¤©è®°å½•å­˜å‚¨ç›®å½•: {chat_history_storage_dir}")
        if os.path.exists(chat_history_storage_dir):
            json_files = list(Path(chat_history_storage_dir).glob("*.json"))
            _log.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            # å¦‚æœdataæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                            json_count += len(data)
                            all_messages.extend(data)
                            _log.info(f"ä» {json_file.name} åŠ è½½ {len(data)} æ¡æ¶ˆæ¯ï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰")
                        elif isinstance(data, dict) and "messages" in data:
                            # å¦‚æœdataæ˜¯å­—å…¸ä¸”åŒ…å«messageså­—æ®µ
                            messages = data.get("messages", [])
                            json_count += len(messages)
                            # éœ€è¦å°†æ¶ˆæ¯è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                            chat_type = data.get("chat_type", "unknown")
                            chat_id = data.get("chat_id", "unknown")
                            for msg in messages:
                                all_messages.append({
                                    "chat_type": chat_type,
                                    "chat_id": chat_id,
                                    "message": msg
                                })
                            _log.info(f"ä» {json_file.name} åŠ è½½ {len(messages)} æ¡æ¶ˆæ¯ï¼ˆå­—å…¸æ ¼å¼ï¼Œchat_type={chat_type}, chat_id={chat_id}ï¼‰")
                        else:
                            _log.warning(f"JSONæ–‡ä»¶ {json_file.name} æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡")
                except Exception as e:
                    _log.warning(f"åŠ è½½ {json_file} å¤±è´¥: {e}", exc_info=True)
        else:
            _log.warning(f"èŠå¤©è®°å½•å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {chat_history_storage_dir}")
        
        _log.info(f"æ€»å…±ä»JSONæ–‡ä»¶åŠ è½½ {len(all_messages)} æ¡æ¶ˆæ¯")
        return all_messages
    
    def load_chat_histories(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ‰€æœ‰èŠå¤©è®°å½•ï¼ˆåŒ…æ‹¬å†…å­˜ä¸­çš„å’Œå†å²JSONæ–‡ä»¶ï¼‰
        æ³¨æ„ï¼šè°ƒç”¨æ­¤å‡½æ•°å‰åº”è¯¥å…ˆè°ƒç”¨save_memory_chat_histories_to_storage()ä¿å­˜å†…å­˜ä¸­çš„è®°å½•
        
        Returns:
            æ‰€æœ‰èŠå¤©è®°å½•çš„åˆ—è¡¨
        """
        all_messages = []
        
        # 1. åŠ è½½å†…å­˜ä¸­çš„èŠå¤©è®°å½•ï¼ˆæœ€æ–°çš„30æ¡ï¼‰
        # æ³¨æ„ï¼šè¿™äº›è®°å½•åº”è¯¥åœ¨è®­ç»ƒå‰å·²ç»ä¿å­˜åˆ°JSONæ–‡ä»¶äº†
        # ä½¿ç”¨å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
        import importlib
        import sys
        
        memory_count = 0
        
        # å°è¯•å¤šç§æ–¹å¼å¯¼å…¥api_server_qwen3vlæ¨¡å—
        api_server = None
        try:
            # æ–¹å¼1ï¼šç›´æ¥å¯¼å…¥ï¼ˆå¦‚æœå·²ç»åœ¨sys.modulesä¸­ï¼‰
            if 'api_server_qwen3vl' in sys.modules:
                api_server = sys.modules['api_server_qwen3vl']
                _log.info("ä»sys.modulesè·å–api_server_qwen3vlæ¨¡å—ï¼ˆç”¨äºåŠ è½½å†…å­˜è®°å½•ï¼‰")
            else:
                # æ–¹å¼2ï¼šä½¿ç”¨importlibå¯¼å…¥
                api_server = importlib.import_module('api_server_qwen3vl')
                _log.info("ä½¿ç”¨importlibå¯¼å…¥api_server_qwen3vlæ¨¡å—ï¼ˆç”¨äºåŠ è½½å†…å­˜è®°å½•ï¼‰")
        except Exception as e:
            _log.warning(f"å¯¼å…¥api_server_qwen3vlæ¨¡å—å¤±è´¥: {e}ï¼Œå°†è·³è¿‡å†…å­˜è®°å½•åŠ è½½")
            # æ–¹å¼3ï¼šå°è¯•ä»serverç›®å½•å¯¼å…¥
            try:
                script_dir = os.path.dirname(os.path.abspath(__file__))
                if script_dir not in sys.path:
                    sys.path.insert(0, script_dir)
                api_server = importlib.import_module('api_server_qwen3vl')
                _log.info("ä»serverç›®å½•å¯¼å…¥api_server_qwen3vlæ¨¡å—ï¼ˆç”¨äºåŠ è½½å†…å­˜è®°å½•ï¼‰")
            except Exception as e2:
                _log.warning(f"ä»serverç›®å½•å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}ï¼Œå°†è·³è¿‡å†…å­˜è®°å½•åŠ è½½")
                api_server = None
        
        if api_server is not None:
            group_chat_histories = getattr(api_server, 'group_chat_histories', {})
            private_chat_histories = getattr(api_server, 'private_chat_histories', {})
            
            _log.info(f"ğŸ“Š å†…å­˜ä¸­çš„èŠå¤©è®°å½•ç»Ÿè®¡ï¼ˆåŠ è½½æ—¶ï¼‰:")
            _log.info(f"   ç¾¤èŠæ•°é‡: {len(group_chat_histories)}")
            _log.info(f"   ç§èŠæ•°é‡: {len(private_chat_histories)}")
            
            for chat_id, history in group_chat_histories.items():
                history_len = len(history)
                memory_count += history_len
                _log.info(f"   ç¾¤èŠ {chat_id}: {history_len} æ¡æ¶ˆæ¯")
                all_messages.extend([
                    {
                        "chat_type": "group",
                        "chat_id": chat_id,
                        "message": msg
                    }
                    for msg in history
                ])
            
            for chat_id, history in private_chat_histories.items():
                history_len = len(history)
                memory_count += history_len
                _log.info(f"   ç§èŠ {chat_id}: {history_len} æ¡æ¶ˆæ¯")
                all_messages.extend([
                    {
                        "chat_type": "private",
                        "chat_id": chat_id,
                        "message": msg
                    }
                    for msg in history
                ])
        else:
            _log.warning("æ— æ³•è®¿é—®api_server_qwen3vlæ¨¡å—ï¼Œè·³è¿‡å†…å­˜è®°å½•åŠ è½½")
        
        _log.info(f"ä»å†…å­˜åŠ è½½ {memory_count} æ¡æ¶ˆæ¯")
        
        # 2. åŠ è½½å†å²JSONæ–‡ä»¶
        json_count = 0
        _log.info(f"æ£€æŸ¥èŠå¤©è®°å½•å­˜å‚¨ç›®å½•: {self.chat_history_storage_dir}")
        if os.path.exists(self.chat_history_storage_dir):
            json_files = list(Path(self.chat_history_storage_dir).glob("*.json"))
            _log.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            # å¦‚æœdataæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                            json_count += len(data)
                            all_messages.extend(data)
                            _log.info(f"ä» {json_file.name} åŠ è½½ {len(data)} æ¡æ¶ˆæ¯ï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰")
                        elif isinstance(data, dict) and "messages" in data:
                            # å¦‚æœdataæ˜¯å­—å…¸ä¸”åŒ…å«messageså­—æ®µ
                            messages = data.get("messages", [])
                            json_count += len(messages)
                            # éœ€è¦å°†æ¶ˆæ¯è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                            chat_type = data.get("chat_type", "unknown")
                            chat_id = data.get("chat_id", "unknown")
                            for msg in messages:
                                all_messages.append({
                                    "chat_type": chat_type,
                                    "chat_id": chat_id,
                                    "message": msg
                                })
                            _log.info(f"ä» {json_file.name} åŠ è½½ {len(messages)} æ¡æ¶ˆæ¯ï¼ˆå­—å…¸æ ¼å¼ï¼Œchat_type={chat_type}, chat_id={chat_id}ï¼‰")
                        else:
                            _log.warning(f"JSONæ–‡ä»¶ {json_file.name} æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡")
                except Exception as e:
                    _log.warning(f"åŠ è½½ {json_file} å¤±è´¥: {e}", exc_info=True)
        else:
            _log.warning(f"èŠå¤©è®°å½•å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {self.chat_history_storage_dir}")
        
        _log.info(f"æ€»å…±åŠ è½½ {len(all_messages)} æ¡æ¶ˆæ¯ï¼ˆå†…å­˜: {memory_count}, JSON: {json_count}ï¼‰")
        return all_messages
    
    def _save_chat_history_directly(self, chat_type: str, chat_id: str, messages: List[Dict[str, Any]]):
        """
        ç›´æ¥ä¿å­˜èŠå¤©è®°å½•åˆ°JSONæ–‡ä»¶ï¼ˆå½“æ— æ³•ä½¿ç”¨api_serverçš„å‡½æ•°æ—¶ï¼‰
        
        Args:
            chat_type: "group" æˆ– "private"
            chat_id: ç¾¤IDæˆ–ç”¨æˆ·ID
            messages: è¦ä¿å­˜çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not self.chat_history_storage_dir:
            raise ValueError("èŠå¤©è®°å½•å­˜å‚¨ç›®å½•æœªé…ç½®")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.chat_history_storage_dir, exist_ok=True)
        
        # åˆ›å»ºå­˜å‚¨æ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{chat_type}_{chat_id}_{timestamp}.json"
        filepath = os.path.join(self.chat_history_storage_dir, filename)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                "chat_type": chat_type,
                "chat_id": chat_id,
                "timestamp": timestamp,
                "messages": messages
            }, f, ensure_ascii=False, indent=2)
        
        _log.info(f"âœ… ç›´æ¥ä¿å­˜ {len(messages)} æ¡èŠå¤©è®°å½•åˆ°: {filename}")
    
    def extract_memory_entries(self, chat_messages: List[Dict[str, Any]], model=None, processor=None) -> str:
        """
        æå–è®°å¿†æ¡ç›®å¹¶ç”Ÿæˆç›‘ç£å‘é‡ï¼Œç›´æ¥ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

        Args:
            chat_messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨

        Returns:
            ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        _log.info("å¼€å§‹æå–è®°å¿†æ¡ç›®...")

        # æŒ‰èŠå¤©åˆ†ç»„
        chat_groups = {}
        for msg_data in chat_messages:
            chat_type = msg_data.get("chat_type", "unknown")
            chat_id = msg_data.get("chat_id", "unknown")
            message = msg_data.get("message", {})

            key = f"{chat_type}_{chat_id}"
            if key not in chat_groups:
                chat_groups[key] = []
            chat_groups[key].append(message)

        _log.info(f"å…± {len(chat_groups)} ä¸ªèŠå¤©ç»„")

        # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ¨¡å‹å’Œprocessor
        if model is None or processor is None:
            _log.error("âŒ extract_memory_entrieséœ€è¦æä¾›modelå’Œprocessorå‚æ•°")
            return None

        _log.info("ä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œè®°å¿†æå–")

        # ä»é…ç½®ä¸­è·å–æœ€å¤§tokené™åˆ¶ï¼ˆç”¨äºæ‰¹é‡æå–å‘é‡æ—¶çš„æˆªæ–­ï¼‰
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å€¼ 35000
        max_tokens = self.training_config.get("max_tokens_for_embedding", 35000)
        _log.debug(f"ä½¿ç”¨æœ€å¤§tokené™åˆ¶: {max_tokens}ï¼ˆç”¨äºæ‰¹é‡æå–å‘é‡æ—¶çš„æˆªæ–­ï¼‰")

        # è§’è‰²è®¾å®šï¼ˆç”¨äºè®°å¿†æå–æ—¶æé†’æ¨¡å‹è‡ªå·±çš„èº«ä»½ï¼‰
        role_playing_prompt = ""
        try:
            role_playing_prompt = self.config.get("prompt", {}).get("role_playing", "")
            if role_playing_prompt:
                role_playing_prompt = role_playing_prompt.strip()
        except Exception:
            role_playing_prompt = ""

        # ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼ˆåªåŒ…å«è®°å¿†æ¡ç›®æ–‡æœ¬ï¼‰
        temp_texts_path = os.path.join(self.memory_db_dir, "temp_memory_texts.pt")
        # æ³¨æ„ï¼šä¿ç•™å·²æœ‰çš„ä¸´æ—¶æ–‡ä»¶ï¼Œæ–°çš„è®°å¿†æ¡ç›®å°†è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
        # è¿™å…è®¸åˆ†æ‰¹å¤„ç†èŠå¤©è®°å½•è€Œä¸ä¸¢å¤±ä¹‹å‰çš„ç»“æœ
        _log.debug(f"è®°å¿†æ¡ç›®å°†ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_texts_path}")

        try:
            
            # å¯¹æ¯ä¸ªèŠå¤©ç»„è¿›è¡Œæ€»ç»“ï¼ˆé€’å½’å¤„ç†ï¼Œæ”¯æŒå¯¹åŠåˆ†ï¼‰
            def process_chat_group(messages: List[Dict[str, Any]], chat_key: str, depth: int = 0):
                """
                å¤„ç†å•ä¸ªèŠå¤©ç»„ï¼ˆé€’å½’å‡½æ•°ï¼Œæ”¯æŒå¯¹åŠåˆ†ï¼‰
                
                Args:
                    messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
                    chat_key: èŠå¤©ç»„æ ‡è¯†
                    depth: é€’å½’æ·±åº¦ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                """
                if not messages:
                    return
                
                # æ„å»ºæ ‡å‡†æ ¼å¼çš„èŠå¤©å†å²ï¼ˆä¿ç•™å¤šæ¨¡æ€ä¿¡æ¯ï¼‰
                # ä½¿ç”¨ä¸generate_replyç›¸åŒçš„æ ¼å¼ï¼Œè¿™æ ·æ¨¡å‹å¯ä»¥è¯»å–å›¾ç‰‡ä¿¡æ¯
                chat_messages_for_extraction = []
                for msg in messages:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")  # é»˜è®¤å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä¸æ—§ç‰ˆæœ¬ä¿æŒä¸€è‡´
                    
                    # ä¿æŒåŸå§‹contentæ ¼å¼ï¼ˆå¯èƒ½æ˜¯listï¼ŒåŒ…å«å›¾ç‰‡ä¿¡æ¯ï¼‰
                    if isinstance(content, list):
                        _log.debug(f"ğŸ” èŠå¤©ç»„ {chat_key} æ¶ˆæ¯ {role} çš„contentæ˜¯åˆ—è¡¨ï¼ŒåŒ…å« {len(content)} é¡¹")
                        # å¤šæ¨¡æ€å†…å®¹ï¼Œéœ€è¦éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ
                        # å¦‚æœå›¾ç‰‡URLå¤±æ•ˆï¼Œåªä¿ç•™æ–‡æœ¬éƒ¨åˆ†
                        filtered_content = []
                        image_count = 0
                        valid_image_count = 0
                        for item in content:
                            if item.get("type") == "text":
                                # æ–‡æœ¬å†…å®¹ç›´æ¥ä¿ç•™
                                filtered_content.append(item)
                            elif item.get("type") == "image":
                                # å›¾ç‰‡å†…å®¹ï¼Œç®€åŒ–éªŒè¯ï¼ˆå› ä¸ºå›¾ç‰‡å·²ç»åœ¨èŠå¤©æ—¶è¢«éªŒè¯è¿‡äº†ï¼‰
                                image_url = item.get("image", "")
                                image_count += 1
                                if image_url:
                                    # è®­ç»ƒæ—¶ç®€åŒ–éªŒè¯ï¼šåªæ£€æŸ¥URLæ ¼å¼ï¼Œä¸è¿›è¡Œå®é™…ç½‘ç»œè®¿é—®
                                    # å› ä¸ºQQå›¾ç‰‡URLæœ‰æ—¶æ•ˆæ€§ï¼Œåœ¨èŠå¤©æ—¶æœ‰æ•ˆä½†è®­ç»ƒæ—¶å¯èƒ½å·²è¿‡æœŸ
                                    if image_url.startswith('http://') or image_url.startswith('https://'):
                                        # URLæ ¼å¼æ­£ç¡®ï¼Œä¿ç•™å›¾ç‰‡ï¼ˆä¿¡ä»»èŠå¤©æ—¶çš„éªŒè¯ç»“æœï¼‰
                                        filtered_content.append(item)
                                        valid_image_count += 1
                                    else:
                                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å›¾ç‰‡URLæ ¼å¼æ— æ•ˆï¼Œè·³è¿‡")
                                else:
                                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å‘ç°æ— æ•ˆçš„å›¾ç‰‡é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                            elif item.get("type") == "video":
                                video_url = item.get("video") or item.get("url")
                                # è§†é¢‘å†…å®¹ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆçš„URLï¼ˆè§†é¢‘ä¸åƒå›¾ç‰‡é‚£æ ·æœ‰URLè¿‡æœŸé—®é¢˜ï¼‰
                                if not video_url:
                                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å‘ç°æ— æ•ˆçš„è§†é¢‘é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                                    continue

                                # è®­ç»ƒæ—¶ç®€åŒ–éªŒè¯ï¼šä¿ç•™æœ¬åœ°æœåŠ¡å™¨URLå’Œæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆä¿¡ä»»èŠå¤©æ—¶çš„éªŒè¯ç»“æœï¼‰
                                from server.api_server_qwen3vl import server_base_url
                                is_local_server_url = (video_url.startswith('http://127.0.0.1:9999/static/videos/') or \
                                                       video_url.startswith('http://localhost:9999/static/videos/') or \
                                                       (server_base_url and video_url.startswith(f"{server_base_url.rstrip('/')}/static/videos/")))
                                is_local_file = os.path.exists(video_url) and os.path.isfile(video_url)
                                is_file_url = video_url.startswith('file://') and os.path.exists(video_url[7:])

                                _log.debug(f"ğŸ” è§†é¢‘URLæ£€æŸ¥: {video_url}")
                                _log.debug(f"  is_local_server_url: {is_local_server_url}")
                                _log.debug(f"  is_local_file: {is_local_file} (æ–‡ä»¶å­˜åœ¨: {os.path.exists(video_url) if video_url else False})")
                                _log.debug(f"  is_file_url: {is_file_url}")
                                _log.debug(f"  is_http: {video_url.startswith('http://') or video_url.startswith('https://') if video_url else False}")

                                if is_local_server_url or is_local_file or is_file_url or video_url.startswith('http://') or video_url.startswith('https://'):
                                    # ä¿ç•™è§†é¢‘ï¼ˆä¿¡ä»»èŠå¤©æ—¶çš„éªŒè¯ç»“æœï¼‰
                                    filtered_content.append({
                                        "type": "video",
                                        "video": video_url
                                    })
                                    _log.info(f"âœ… ä¿ç•™è§†é¢‘: {video_url}")
                                else:
                                    # æ— æ•ˆURLæ ¼å¼ï¼Œç§»é™¤
                                    _log.warning(f"âš ï¸ ç§»é™¤æ— æ•ˆè§†é¢‘URL: {video_url}")

                        # å¦‚æœè¿‡æ»¤åè¿˜æœ‰å†…å®¹ï¼Œæ·»åŠ æ¶ˆæ¯
                        if filtered_content:
                            # ç»Ÿè®¡å›¾ç‰‡å’Œè§†é¢‘æ•°é‡
                            img_count = sum(1 for item in filtered_content if item.get("type") == "image")
                            vid_count = sum(1 for item in filtered_content if item.get("type") == "video")
                            if img_count > 0 or vid_count > 0:
                                _log.info(f"ğŸ“Š èŠå¤©ç»„ {chat_key} æ¶ˆæ¯åŒ…å« {img_count} å¼ å›¾ç‰‡å’Œ {vid_count} ä¸ªè§†é¢‘")
                                for item in filtered_content:
                                    if item.get("type") == "image":
                                        _log.info(f"   ğŸ“· å›¾ç‰‡URL: {item.get('image', '')}")
                                    elif item.get("type") == "video":
                                        _log.info(f"   ğŸ¥ è§†é¢‘URL: {item.get('video', '')}")
                            chat_messages_for_extraction.append({
                                "role": role,
                                "content": filtered_content
                            })
                        else:
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯è¿‡æ»¤åæ— å†…å®¹ï¼Œè·³è¿‡è¯¥æ¶ˆæ¯")
                    elif isinstance(content, str):
                        # çº¯æ–‡æœ¬å†…å®¹ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                        chat_messages_for_extraction.append({
                            "role": role,
                            "content": [{"type": "text", "text": content}]
                        })
                    else:
                        # æœªçŸ¥æ ¼å¼ï¼Œè·³è¿‡
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯contentæ ¼å¼æœªçŸ¥: {type(content)}ï¼Œè·³è¿‡")
                
                # æ·»åŠ ç³»ç»Ÿæç¤ºï¼Œè¦æ±‚æ¨¡å‹æå–è®°å¿†æ¡ç›®
                extraction_system_prompt = """è¯·åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–å‡ºå€¼å¾—è®°å¿†çš„ç‹¬ç«‹ä¿¡æ¯æ¡ç›®ã€‚

æ³¨æ„ï¼šå¯¹è¯ä¸­çš„"åŠ©æ‰‹"å°±æ˜¯ä½ è‡ªå·±ï¼ˆAIåŠ©æ‰‹ï¼‰ï¼Œåœ¨æ€»ç»“è®°å¿†æ¡ç›®æ—¶ï¼Œå¦‚æœæ¶‰åŠåŠ©æ‰‹çš„è¡Œä¸ºã€å›å¤æˆ–ä¿¡æ¯ï¼Œè¯·ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆæˆ‘ã€æˆ‘çš„ï¼‰æ¥æè¿°ã€‚

å€¼å¾—è®°å¿†çš„ä¿¡æ¯ç±»å‹åŒ…æ‹¬ä½†ä¸é™äºï¼š
1. å…³äºäººç‰©çš„äº‹å®æ€§çŸ¥è¯†ï¼šå§“åã€èº«ä»½ã€èŒä¸šã€å…³ç³»ã€æ€§æ ¼ç‰¹ç‚¹ã€å…´è¶£çˆ±å¥½ã€ä¹ æƒ¯ç­‰
2. å…³äºä¸–ç•Œçš„äº‹å®æ€§çŸ¥è¯†ï¼šåœ°ç‚¹ã€äº‹ä»¶ã€å†å²ã€æ–‡åŒ–èƒŒæ™¯ç­‰
3. æ—¶äº‹æ–°é—»ï¼šå½“å‰å‘ç”Ÿçš„é‡è¦äº‹ä»¶ã€ç¤¾ä¼šåŠ¨æ€ç­‰
4. ç§‘å­¦çŸ¥è¯†ï¼šç§‘å­¦åŸç†ã€æŠ€æœ¯ä¿¡æ¯ã€ä¸“ä¸šçŸ¥è¯†ç­‰
5. ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯ï¼šå–œæ¬¢ä»€ä¹ˆã€ä¸å–œæ¬¢ä»€ä¹ˆã€å¸¸ç”¨è¡¨è¾¾æ–¹å¼ç­‰
6. é‡è¦çš„çº¦å®šå’Œæ‰¿è¯ºï¼šç”¨æˆ·æåˆ°çš„é‡è¦äº‹é¡¹ã€çº¦å®šç­‰
7. å…³äºæˆ‘ï¼ˆåŠ©æ‰‹ï¼‰çš„ä¿¡æ¯ï¼šç”¨æˆ·å¯¹æˆ‘çš„ç§°å‘¼ã€æˆ‘ä¸ç”¨æˆ·çš„å…³ç³»ã€æˆ‘å‘Šè¯‰ç”¨æˆ·çš„ä¿¡æ¯ç­‰

æ¯ä¸ªè®°å¿†æ¡ç›®åº”è¯¥æ˜¯ä¸€ä¸ªå…·ä½“ã€ç‹¬ç«‹çš„äº‹å®æˆ–åå¥½ï¼Œæ ¼å¼å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ï¼š
æ¡ç›®1: [å…·ä½“è®°å¿†å†…å®¹]
æ¡ç›®2: [å…·ä½“è®°å¿†å†…å®¹]
...

é‡è¦è¦æ±‚ï¼š
1. æ¯æ¡è®°å¿†æ¡ç›®åº”è¯¥å°½å¯èƒ½åŒ…å«å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬äººç‰©ã€æ—¶é—´ã€åœ°ç‚¹ã€äº‹ä»¶ç­‰æ‰€æœ‰ç›¸å…³ç»†èŠ‚ï¼Œä¸è¦å°†è¿™äº›ä¿¡æ¯åˆ†æ•£åœ¨å¤šä¸ªæ¡ç›®ä¸­
2. æ¡ç›®ä¸æ¡ç›®ä¹‹é—´åº”è¯¥æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œä¸å­˜åœ¨å…³è”æˆ–ä¾èµ–å…³ç³»
3. å¦‚æœä¸€æ¡è®°å¿†æ¶‰åŠå¤šä¸ªè¦ç´ ï¼ˆå¦‚äººç‰©ã€æ—¶é—´ã€åœ°ç‚¹ã€äº‹ä»¶ï¼‰ï¼Œåº”è¯¥å°†å®ƒä»¬æ•´åˆåœ¨ä¸€æ¡è®°å¿†æ¡ç›®ä¸­
4. å¦‚æœè®°å¿†æ¶‰åŠåŠ©æ‰‹ï¼ˆæˆ‘ï¼‰çš„ä¿¡æ¯ï¼Œè¯·ä½¿ç”¨ç¬¬ä¸€äººç§°æè¿°ï¼Œä¾‹å¦‚"æˆ‘å‘Šè¯‰ç”¨æˆ·..."ã€"ç”¨æˆ·ç§°å‘¼æˆ‘ä¸º..."ç­‰
5. è®°å¿†æ¡ç›®ä¸ä¸€å®šè¦éå¸¸ç®€ç•¥ï¼Œå¿…è¦çš„ä¿¡æ¯è¦å……åˆ†è®°å½•ã€‚å¦‚æœæŸä¸ªç»†èŠ‚å¯¹äºç†è§£æˆ–å›å¿†è¿™æ¡è®°å¿†å¾ˆé‡è¦ï¼Œåº”è¯¥åŒ…å«åœ¨å†…

å¦‚æœå¯¹è¯ä¸­æ²¡æœ‰å€¼å¾—è®°å¿†çš„ä¿¡æ¯ï¼Œè¯·è¾“å‡º"æ— è®°å¿†æ¡ç›®"ã€‚

è¯·å…ˆè¿›è¡Œæ€è€ƒï¼ˆä½¿ç”¨<think>æ ‡ç­¾ï¼‰ï¼Œç„¶ååœ¨</think>æ ‡ç­¾åçš„æ­£å¼å›ç­”ä¸­ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºè®°å¿†æ¡ç›®ã€‚"""

                if role_playing_prompt:
                    extraction_system_prompt += (
                        "\n\n======\nè§’è‰²è®¾å®šæç¤ºï¼ˆå¸®åŠ©ä½ ç†è§£å¯¹è¯ä¸­è‡ªå·±çš„èº«ä»½ï¼‰ï¼š\n"
                        f"{role_playing_prompt}\n"
                        "------\n"
                        "æ³¨æ„ï¼šä»¥ä¸Šå†…å®¹ä»…ç”¨äºæé†’ä½ åœ¨åŸå§‹èŠå¤©ä¸­çš„èº«ä»½å’Œå…³ç³»ã€‚"
                        " åœ¨æå–è®°å¿†æ¡ç›®æ—¶ï¼Œè¯·ç”¨å®¢è§‚ã€ç¬¬ä¸€äººç§°çš„æ–¹å¼æè¿°äº‹å®ï¼Œä¸éœ€è¦æ¨¡ä»¿å£ç™–æˆ–èŠå¤©è¯­æ°”ã€‚"
                        "\n======"
                    )
                
                # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆç³»ç»Ÿæç¤º + èŠå¤©å†å² + ç”¨æˆ·æç¤ºï¼‰
                full_messages = [
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": extraction_system_prompt}]
                    }
                ]
                full_messages.extend(chat_messages_for_extraction)
                full_messages.append({
                    "role": "user",
                    "content": [{"type": "text", "text": "è¯·å¼€å§‹æå–è®°å¿†æ¡ç›®ã€‚"}]
                })
                
                # ç»Ÿè®¡å›¾ç‰‡å’Œè§†é¢‘æ•°é‡ï¼ˆä¸v1.0ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼Œä¸åšURLè½¬æ¢ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹URLï¼‰
                # v1.0ç‰ˆæœ¬ç›´æ¥ä½¿ç”¨HTTP URLï¼Œprocessorèƒ½å¤Ÿè®¿é—®ï¼ˆå› ä¸ºæœåŠ¡å™¨æ­£åœ¨è¿è¡Œï¼‰
                total_images = 0
                total_videos = 0
                for msg in full_messages:
                    msg_content = msg.get("content", [])
                    if isinstance(msg_content, list):
                        for item in msg_content:
                            if item.get("type") == "image":
                                total_images += 1
                                _log.debug(f"ğŸ” å›¾ç‰‡URL: {item.get('image', '')}")
                            elif item.get("type") == "video":
                                total_videos += 1
                                _log.debug(f"ğŸ” è§†é¢‘URL: {item.get('video', '')}")
                _log.info(f"ğŸ“Š å‡†å¤‡å¤„ç†çš„æ¶ˆæ¯åŒ…å« {total_images} å¼ å›¾ç‰‡å’Œ {total_videos} ä¸ªè§†é¢‘")
                
                try:
                    # ä½¿ç”¨processor.apply_chat_templateå¤„ç†æ¶ˆæ¯ï¼ˆä¸generate_replyä¸€è‡´ï¼‰
                    # è¿™æ ·å¯ä»¥ä¿ç•™å›¾ç‰‡ä¿¡æ¯
                    # å¦‚æœå›¾ç‰‡URLå¤±æ•ˆï¼Œä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œéœ€è¦æ•è·å¹¶å¤„ç†
                    try:
                        inputs = processor.apply_chat_template(
                            full_messages,
                            tokenize=True,
                            add_generation_prompt=True,
                            return_dict=True,
                            return_tensors="pt"
                        )

                        # æ‰“å°å®Œæ•´çš„è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼Œä¸api_server_qwen3vl.pyä¿æŒä¸€è‡´
                        input_ids_text = processor.batch_decode(
                            inputs['input_ids'],
                            skip_special_tokens=False,
                            clean_up_tokenization_spaces=False
                        )
                        _log.info("=" * 80)
                        _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
                        _log.info(input_ids_text[0])
                        _log.info("=" * 80)
                    except (PIL.UnidentifiedImageError, OSError, requests.RequestException, Exception) as img_error:
                        # å›¾ç‰‡/è§†é¢‘åŠ è½½å¤±è´¥ï¼Œå°è¯•ç§»é™¤æ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘ï¼Œåªä¿ç•™æ–‡æœ¬
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å›¾ç‰‡/è§†é¢‘å¤„ç†å¤±è´¥: {str(img_error)}")
                        _log.warning(f"   é”™è¯¯ç±»å‹: {type(img_error).__name__}")
                        _log.warning(f"   é”™è¯¯è¯¦æƒ…: {img_error}", exc_info=True)
                        if isinstance(img_error, requests.RequestException):
                            _log.warning("   ğŸ“· å›¾ç‰‡/è§†é¢‘URLå¤±æ•ˆæˆ–ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå¯èƒ½æ˜¯QQä¸´æ—¶URLå·²è¿‡æœŸ")
                        elif isinstance(img_error, PIL.UnidentifiedImageError):
                            _log.warning("   ğŸ“· å›¾ç‰‡æ ¼å¼æ— æ³•è¯†åˆ«ï¼Œå¯èƒ½æ˜¯å›¾ç‰‡æ–‡ä»¶æŸå")
                        else:
                            _log.warning("   ğŸ“· å›¾ç‰‡/è§†é¢‘åŠ è½½å‡ºé”™ï¼Œå°†ç§»é™¤æ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘ç»§ç»­å¤„ç†")
                        _log.info(f"   ğŸ”„ è‡ªåŠ¨é™çº§ï¼šç§»é™¤æ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘ï¼Œåªä½¿ç”¨æ–‡æœ¬å†…å®¹è¿›è¡Œè®°å¿†æå–...")
                        
                        # é‡æ–°æ„å»ºæ¶ˆæ¯ï¼Œç§»é™¤æ‰€æœ‰å›¾ç‰‡
                        text_only_messages = []
                        for msg in full_messages:
                            msg_content = msg.get("content", [])
                            if isinstance(msg_content, list):
                                # åªä¿ç•™æ–‡æœ¬é¡¹
                                text_items = [item for item in msg_content if item.get("type") == "text"]
                                if text_items:
                                    text_only_messages.append({
                                        "role": msg.get("role"),
                                        "content": text_items
                                    })
                            else:
                                # å·²ç»æ˜¯æ–‡æœ¬ï¼Œç›´æ¥ä¿ç•™
                                text_only_messages.append(msg)
                        
                        if not text_only_messages:
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} ç§»é™¤å›¾ç‰‡åæ— æœ‰æ•ˆæ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                            return
                        
                        # ä½¿ç”¨çº¯æ–‡æœ¬æ¶ˆæ¯é‡æ–°å¤„ç†
                        inputs = processor.apply_chat_template(
                            text_only_messages,
                            tokenize=True,
                            add_generation_prompt=True,
                            return_dict=True,
                            return_tensors="pt"
                        )
                    
                    # æ£€æŸ¥è¾“å…¥tokené•¿åº¦ï¼ˆä½¿ç”¨å‡½æ•°çº§åˆ«çš„max_tokenså˜é‡ï¼‰
                    input_length = inputs["input_ids"].shape[-1]
                    _log.info(f"ğŸ“Š èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) è¾“å…¥tokené•¿åº¦: {input_length}, æœ€å¤§é™åˆ¶: {max_tokens}")
                    
                    # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œå°†èŠå¤©è®°å½•å¯¹åŠåˆ†
                    if input_length > max_tokens:
                        if len(messages) <= 1:
                            # å³ä½¿åªæœ‰1æ¡æ¶ˆæ¯ä¹Ÿè¶…è¿‡é™åˆ¶ï¼Œåªèƒ½è·³è¿‡
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å³ä½¿åªæœ‰1æ¡æ¶ˆæ¯ä¹Ÿè¶…è¿‡é™åˆ¶ ({input_length} > {max_tokens})ï¼Œè·³è¿‡å¤„ç†")
                            return
                        else:
                            # å¯¹åŠåˆ†
                            _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) è¾“å…¥tokené•¿åº¦ ({input_length}) è¶…è¿‡é™åˆ¶ ({max_tokens})ï¼Œå¯¹åŠåˆ†å¤„ç†")
                            half_point = len(messages) // 2
                            first_half = messages[:half_point]
                            second_half = messages[half_point:]
                            
                            # é€’å½’å¤„ç†ä¸¤åŠ
                            process_chat_group(first_half, chat_key, depth + 1)
                            process_chat_group(second_half, chat_key, depth + 1)
                            return
                    
                    # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
                    # åœ¨å¤šGPUæ¨¡å¼ä¸‹ï¼Œéœ€è¦å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨çš„è®¾å¤‡ï¼‰
                    if isinstance(self.device, list):
                        # å¤šGPUæ¨¡å¼ï¼šè·å–æ¨¡å‹å®é™…æ‰€åœ¨çš„è®¾å¤‡ï¼ˆç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨çš„è®¾å¤‡ï¼‰
                        model_device = next(model.parameters()).device
                        inputs = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v
                                 for k, v in inputs.items()}
                    else:
                        # å•GPUæ¨¡å¼ï¼šæ£€æŸ¥CUDA_VISIBLE_DEVICESè®¾ç½®çŠ¶æ€
                        cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
                        if cuda_visible and cuda_visible.strip():
                            # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„è®¾å¤‡
                            device_for_inputs = "cuda:0"
                            _log.debug(f"ğŸ”§ è®°å¿†æå–: CUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {device_for_inputs}ï¼ˆå¯¹åº”ç‰©ç†GPU {self.device}ï¼‰")
                        else:
                            # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨åŸå§‹è®¾å¤‡é…ç½®
                            device_for_inputs = self.device
                            _log.debug(f"ğŸ”§ è®°å¿†æå–: ä½¿ç”¨è®¾å¤‡ {device_for_inputs}")
                        inputs = {k: v.to(device_for_inputs) if isinstance(v, torch.Tensor) else v
                                 for k, v in inputs.items()}
                    
                    # 1. è®©æ¨¡å‹ç”Ÿæˆè®°å¿†æ¡ç›®åˆ—è¡¨
                    # ä½¿ç”¨ä¸æ¨¡å‹è¿è¡Œæ—¶ç›¸åŒçš„ç”Ÿæˆå‚æ•°ï¼ˆä»configä¸­è¯»å–ï¼‰
                    gen_config = self.config.get("generation", {})
                    max_new_tokens = gen_config.get("max_new_tokens", 1000)
                    temperature = gen_config.get("temperature", 1.0)
                    do_sample = gen_config.get("do_sample", True)
                    top_p = gen_config.get("top_p", 0.95)
                    top_k = gen_config.get("top_k", 20)
                    repetition_penalty = gen_config.get("repetition_penalty", 1.0)
                    
                    # ä½¿ç”¨transformerså®˜æ–¹çš„model.generate()æ–¹æ³•
                    with torch.no_grad():
                        generated = model.generate(
                            **inputs,
                            max_new_tokens=max_new_tokens,
                            temperature=temperature if do_sample else None,
                            do_sample=do_sample,
                            top_p=top_p if do_sample else None,
                            top_k=top_k if do_sample else None,
                            repetition_penalty=repetition_penalty if repetition_penalty != 1.0 else None
                        )
                    
                    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                    generated_text = processor.batch_decode(
                        generated[:, inputs['input_ids'].shape[1]:],
                        skip_special_tokens=True
                    )[0].strip()
                    
                    # 2. æå–æ­£å¼å›ç­”ï¼ˆä»æœ€åä¸€ä¸ª</think>æ ‡ç­¾åï¼‰
                    # ä½¿ç”¨ä¸api_serverç›¸åŒçš„æå–é€»è¾‘ï¼ˆextract_final_replyå‡½æ•°ï¼‰
                    import re
                    thinking_patterns = [
                        r'</think>\s*',
                        r'</thinking>\s*'
                    ]
                    
                    last_match = None
                    for pattern in thinking_patterns:
                        matches = list(re.finditer(pattern, generated_text, re.IGNORECASE))
                        if matches:
                            current_match = matches[-1]
                            if last_match is None or current_match.end() > last_match.end():
                                last_match = current_match
                    
                    if last_match:
                        # æå–æœ€åä¸€ä¸ªæ ‡ç­¾åçš„å†…å®¹ï¼ˆè¿™æ˜¯æ­£å¼å›ç­”éƒ¨åˆ†ï¼‰
                        final_reply = generated_text[last_match.end():].strip()
                        _log.info(f"âœ… ä»æ¨¡å‹è¾“å‡ºä¸­æå–åˆ°æ­£å¼å›ç­”ï¼ˆä»æœ€åä¸€ä¸ª{last_match.group(0).strip()}æ ‡ç­¾å¼€å§‹ï¼‰")
                        generated_text = final_reply
                    else:
                        _log.warning("âš ï¸ æœªæ‰¾åˆ°</think>æˆ–</thinking>æ ‡ç­¾ï¼Œä½¿ç”¨å®Œæ•´è¾“å‡º")
                    
                    # è®°å½•ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    _log.info(f"ğŸ“ èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) æ¨¡å‹ç”Ÿæˆçš„åŸå§‹æ–‡æœ¬:")
                    _log.info("=" * 60)
                    _log.info(f"ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦: {len(generated_text)} å­—ç¬¦")
                    if len(generated_text) > 500:
                        _log.info(f"ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰: {generated_text[:500]}")
                    else:
                        _log.info(f"ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬: {generated_text}")
                    
                    # 2. è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¤šä¸ªè®°å¿†æ¡ç›®
                    memory_texts = self._parse_memory_entries(generated_text)
                    
                    _log.info(f"ğŸ“Š è§£æåæå–åˆ° {len(memory_texts)} ä¸ªè®°å¿†æ¡ç›®")
                    if memory_texts:
                        for i, mem_text in enumerate(memory_texts, 1):
                            _log.info(f"   è®°å¿†æ¡ç›® {i}: {mem_text[:100]}...")
                    
                    # 3. ä¸´æ—¶ä¿å­˜è®°å¿†æ¡ç›®æ–‡æœ¬åˆ°æ–‡ä»¶ï¼ˆä¸æå–å‘é‡ï¼Œç­‰å¾…æ‰¹é‡å¤„ç†ï¼‰
                    for memory_text in memory_texts:
                        self._append_memory_text_to_file(memory_text, temp_texts_path)
                        _log.info(f"âœ… æå–è®°å¿†æ¡ç›®æ–‡æœ¬ (æ·±åº¦ {depth}): {memory_text[:80]}...")
                
                except Exception as e:
                    _log.warning(f"å¤„ç†èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) æ—¶å‡ºé”™: {e}", exc_info=True)
                    return
        
            # å¯¹æ¯ä¸ªèŠå¤©ç»„è°ƒç”¨å¤„ç†å‡½æ•°ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šåªæå–æ–‡æœ¬ï¼‰
            for chat_key, messages in chat_groups.items():
                process_chat_group(messages, chat_key)

            # æ£€æŸ¥æ˜¯å¦æå–åˆ°è®°å¿†æ¡ç›®
            if not os.path.exists(temp_texts_path):
                _log.warning("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•è®°å¿†æ¡ç›®")
                return None

            # ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½æ‰€æœ‰è®°å¿†æ¡ç›®æ–‡æœ¬
            all_memory_texts = self._load_memory_texts_from_file(temp_texts_path)
            if not all_memory_texts:
                _log.warning("âš ï¸ ä¸´æ—¶æ–‡ä»¶ä¸­æ²¡æœ‰è®°å¿†æ¡ç›®")
                return None

            _log.info(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼šå…±æå– {len(all_memory_texts)} ä¸ªè®°å¿†æ¡ç›®æ–‡æœ¬")
            _log.info("=" * 60)
            _log.info("å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡æå–è®°å¿†æ¡ç›®å‘é‡")
            _log.info("=" * 60)

            # ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡æå–æ‰€æœ‰è®°å¿†æ¡ç›®çš„å‘é‡
            all_texts, all_embeddings = self._batch_extract_embeddings(
                all_memory_texts, model, processor, max_tokens
            )

            # ä¿å­˜æ‰€æœ‰æ¡ç›®
            if all_texts and all_embeddings:
                self._save_training_data_batch(all_texts, all_embeddings)
                _log.info(f"âœ… æˆåŠŸä¿å­˜ {len(all_texts)} ä¸ªè®°å¿†æ¡ç›®åŠå…¶å‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶")
                
                # åˆ é™¤åªåŒ…å«è®°å¿†æ¡ç›®æ–‡æœ¬çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆå·²ç»è·å¾—å‘é‡ï¼Œä¸å†éœ€è¦ï¼‰
                try:
                    if os.path.exists(temp_texts_path):
                        os.remove(temp_texts_path)
                        _log.info(f"âœ… å·²åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶: temp_memory_texts.pt")
                except Exception as e:
                    _log.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            else:
                _log.warning("âŒ æ²¡æœ‰æˆåŠŸæå–åˆ°å‘é‡")
                return None

            # è·å–æœ€ç»ˆçš„è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
            if os.path.exists(temp_data_path):
                data = torch.load(temp_data_path, map_location='cpu')
                total_entries = len(data.get('texts', []))
                _log.info(f"âœ… æˆåŠŸæå–å¹¶ä¿å­˜ {total_entries} ä¸ªè®°å¿†æ¡ç›®åˆ°ä¸´æ—¶æ–‡ä»¶")
            else:
                _log.warning("âŒ æ²¡æœ‰ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶")
                return None

        finally:
            # æ¸…ç†ä¸´æ—¶å˜é‡
            if 'all_texts' in locals():
                del all_texts
            if 'all_embeddings' in locals():
                del all_embeddings
            
            # å¦‚æœç¨‹åºå¼‚å¸¸é€€å‡ºï¼Œå°è¯•æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶
            temp_texts_path = os.path.join(self.memory_db_dir, "temp_memory_texts.pt")
            if os.path.exists(temp_texts_path):
                try:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®Œæ•´çš„è®­ç»ƒæ•°æ®æ–‡ä»¶
                    temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
                    if os.path.exists(temp_data_path):
                        # å¦‚æœæœ‰å®Œæ•´æ–‡ä»¶ï¼Œåˆ é™¤æ–‡æœ¬ä¸´æ—¶æ–‡ä»¶
                        os.remove(temp_texts_path)
                        _log.debug(f"æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶: temp_memory_texts.pt")
                except Exception as e:
                    _log.debug(f"æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰: {e}")

            # ä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹ï¼Œä¸åœ¨è¿™é‡Œå¸è½½
            # æ¨¡å‹å°†åœ¨è®­ç»ƒæµç¨‹ç»“æŸåç»Ÿä¸€å¸è½½
            _log.info("âœ… è®°å¿†æå–å®Œæˆï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹ï¼‰")

        return temp_data_path

    def _batch_extract_embeddings(
        self, 
        memory_texts: List[str], 
        model, 
        processor, 
        max_tokens: int
    ) -> Tuple[List[str], List[torch.Tensor]]:
        """
        æ‰¹é‡æå–è®°å¿†æ¡ç›®çš„å‘é‡ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰
        
        Args:
            memory_texts: è®°å¿†æ¡ç›®æ–‡æœ¬åˆ—è¡¨
            model: æ¨¡å‹å®ä¾‹
            processor: å¤„ç†å™¨å®ä¾‹
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            (texts, embeddings) å…ƒç»„ï¼Œä¸€ä¸€å¯¹åº”
        """
        all_texts = []
        all_embeddings = []
        
        # ç¡®å®šè®¾å¤‡
        if isinstance(self.device, list):
            model_device = next(model.parameters()).device
        else:
            # å•GPUæ¨¡å¼ï¼šæ£€æŸ¥CUDA_VISIBLE_DEVICESè®¾ç½®çŠ¶æ€
            cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
            if cuda_visible and cuda_visible.strip():
                # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„è®¾å¤‡
                model_device = "cuda:0"
                _log.debug(f"ğŸ”§ æ‰¹é‡å‘é‡æå–: CUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {model_device}ï¼ˆå¯¹åº”ç‰©ç†GPU {self.device}ï¼‰")
            else:
                # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨åŸå§‹è®¾å¤‡é…ç½®
                model_device = self.device
                _log.debug(f"ğŸ”§ æ‰¹é‡å‘é‡æå–: ä½¿ç”¨è®¾å¤‡ {model_device}")
        
        # Batchå¤§å°ï¼ˆå¯ä»¥æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼‰
        batch_size = self.training_config.get("embedding_batch_size", 8)
        _log.info(f"ğŸ“¦ ä½¿ç”¨batch_size={batch_size}è¿›è¡Œæ‰¹é‡å‘é‡æå–")
        
        # æ„å»ºæ‰€æœ‰prompts
        prompts = []
        valid_indices = []  # è®°å½•æœ‰æ•ˆçš„ç´¢å¼•ï¼ˆç”¨äºå¤„ç†æˆªæ–­å¤±è´¥çš„æƒ…å†µï¼‰
        
        for idx, memory_text in enumerate(memory_texts):
            prompt = f"è¯·ç”¨ä¸€ä¸ªTokenæ€»ç»“ä»¥ä¸‹æ–‡æœ¬\"{memory_text}\"ï¼š"
            prompts.append(prompt)
            valid_indices.append(idx)
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (len(prompts) + batch_size - 1) // batch_size
        _log.info(f"ğŸ“Š å…± {len(prompts)} ä¸ªè®°å¿†æ¡ç›®ï¼Œåˆ†ä¸º {total_batches} ä¸ªbatchå¤„ç†")
        
        for batch_idx in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_idx:batch_idx + batch_size]
            batch_texts = [memory_texts[valid_indices[batch_idx + i]] for i in range(len(batch_prompts))]
            
            try:
                # Batch tokenizeï¼ˆè‡ªåŠ¨å¤„ç†paddingï¼‰
                # æ³¨æ„ï¼šå¯¹äºçº¯æ–‡æœ¬ï¼Œåº”è¯¥ä½¿ç”¨processor.tokenizerï¼Œè€Œä¸æ˜¯processor
                # processoræ˜¯å¤šæ¨¡æ€å¤„ç†å™¨ï¼Œä¼šå°†æ–‡æœ¬è¯¯å½“ä½œå›¾ç‰‡å¤„ç†
                # æ˜ç¡®è®¾ç½®å‚æ•°é¡ºåºï¼Œç¡®ä¿truncationåœ¨max_lengthä¹‹å‰
                batch_inputs = processor.tokenizer(
                    batch_prompts,
                    truncation=True,  # æ˜ç¡®è®¾ç½®æˆªæ–­
                    max_length=max_tokens,  # è®¾ç½®æœ€å¤§é•¿åº¦
                    padding=True,  # å¡«å……åˆ°ç›¸åŒé•¿åº¦
                    return_tensors="pt"
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch_inputs = {
                    k: v.to(model_device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch_inputs.items()
                }
                
                # Batchæ¨ç†
                with torch.no_grad():
                    backbone_outputs = forward_backbone(
                        model,
                        input_ids=batch_inputs["input_ids"],
                        attention_mask=batch_inputs["attention_mask"],
                        use_cache=False,
                        output_hidden_states=False,
                        return_dict=True,
                    )
                
                # æå–æ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„hidden state
                last_hidden_states = ensure_last_hidden_state(backbone_outputs)
                attention_mask = batch_inputs["attention_mask"]  # [batch_size, seq_len]
                
                for i in range(len(batch_prompts)):
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„ä½ç½®
                    last_token_idx = attention_mask[i].sum().item() - 1
                    if last_token_idx < 0:
                        _log.warning(f"âš ï¸ Batch {batch_idx//batch_size + 1} æ ·æœ¬ {i} çš„attention_maskæ— æ•ˆï¼Œè·³è¿‡")
                        continue
                    
                    # æå–æœ€åä¸€ä¸ªtokençš„embedding
                    embedding = last_hidden_states[i, last_token_idx, :].detach().cpu()
                    
                    all_texts.append(batch_texts[i])
                    all_embeddings.append(embedding)
                
                # è¿›åº¦æ—¥å¿—
                processed = min(batch_idx + batch_size, len(prompts))
                _log.info(f"âœ… Batch {batch_idx//batch_size + 1}/{total_batches}: å·²å¤„ç† {processed}/{len(prompts)} ä¸ªæ¡ç›®")
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if (batch_idx // batch_size + 1) % 10 == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        _log.debug(f"ğŸ§¹ å·²æ¸…ç†GPUæ˜¾å­˜ï¼ˆå¤„ç†äº† {processed} ä¸ªæ¡ç›®ï¼‰")
                
            except Exception as e:
                _log.error(f"âŒ Batch {batch_idx//batch_size + 1} å¤„ç†å¤±è´¥: {e}", exc_info=True)
                # å¦‚æœbatchå¤±è´¥ï¼Œå°è¯•é€ä¸ªå¤„ç†è¿™ä¸ªbatchä¸­çš„æ¡ç›®
                _log.warning(f"ğŸ”„ å°è¯•é€ä¸ªå¤„ç†è¯¥batchä¸­çš„æ¡ç›®...")
                for i, memory_text in enumerate(batch_texts):
                    try:
                        prompt = batch_prompts[i]
                        # æ³¨æ„ï¼šå¯¹äºçº¯æ–‡æœ¬ï¼Œåº”è¯¥ä½¿ç”¨processor.tokenizerï¼Œè€Œä¸æ˜¯processor
                        inputs = processor.tokenizer(
                            prompt,
                            truncation=True,
                            max_length=max_tokens,
                            return_tensors="pt"
                        )
                        inputs = {
                            k: v.to(model_device) if isinstance(v, torch.Tensor) else v
                            for k, v in inputs.items()
                        }
                        
                        with torch.no_grad():
                            backbone_outputs = forward_backbone(
                                model,
                                input_ids=inputs["input_ids"],
                                attention_mask=inputs["attention_mask"],
                                use_cache=False,
                                output_hidden_states=False,
                                return_dict=True,
                            )
                        
                        last_token_idx = inputs["attention_mask"].sum().item() - 1
                        if last_token_idx >= 0:
                            last_hidden = ensure_last_hidden_state(backbone_outputs)
                            embedding = last_hidden[0, last_token_idx, :].detach().cpu()
                            all_texts.append(memory_text)
                            all_embeddings.append(embedding)
                    except Exception as single_e:
                        _log.warning(f"âš ï¸ å•ä¸ªæ¡ç›®å¤„ç†ä¹Ÿå¤±è´¥: {memory_text[:50]}... é”™è¯¯: {single_e}")
                        continue
        
        _log.info(f"âœ… æ‰¹é‡å‘é‡æå–å®Œæˆï¼šæˆåŠŸæå– {len(all_embeddings)}/{len(memory_texts)} ä¸ªå‘é‡")
        
        return all_texts, all_embeddings

    def _append_memory_text_to_file(self, memory_text: str, file_path: str):
        """
        è¿½åŠ è®°å¿†æ¡ç›®æ–‡æœ¬åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        
        Args:
            memory_text: è®°å¿†æ¡ç›®æ–‡æœ¬
            file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        """
        try:
            if os.path.exists(file_path):
                # åŠ è½½ç°æœ‰æ•°æ®å¹¶è¿½åŠ 
                existing_data = torch.load(file_path, map_location='cpu')
                existing_texts = existing_data.get('texts', [])
                existing_texts.append(memory_text)
            else:
                # åˆ›å»ºæ–°æ–‡ä»¶
                existing_texts = [memory_text]
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            torch.save({"texts": existing_texts}, file_path)
        except Exception as e:
            _log.warning(f"è¿½åŠ è®°å¿†æ¡ç›®æ–‡æœ¬åˆ°æ–‡ä»¶å¤±è´¥: {e}")

    def _extract_sft_vectors_for_recall_training(
        self,
        num_memory_entries: int,
        model,
        processor
    ) -> Optional[str]:
        """
        æå–ç­‰é‡çš„SFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒï¼Œé˜²æ­¢<recall>tokenè¿‡æ‹Ÿåˆ

        Args:
            num_memory_entries: è®°å¿†æ¡ç›®æ•°é‡ï¼Œéœ€è¦æå–ç­‰é‡çš„SFTå‘é‡
            model: åŸºç¡€æ¨¡å‹
            processor: åŸºç¡€å¤„ç†å™¨

        Returns:
            SFTå‘é‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
        """
        try:
            if not self.sft_enabled or not self.sft_path:
                _log.info("â„¹ï¸ SFTæœªå¯ç”¨æˆ–æœªé…ç½®ï¼Œè·³è¿‡SFTå‘é‡æå–")
                return None

            # è®¡ç®—éœ€è¦çš„SFTå‘é‡æ•°é‡ï¼š1.5å€äºè®°å¿†æ¡ç›®æ•°é‡
            required_sft_count = int(num_memory_entries * 1.5)
            _log.info(f"ğŸ§ª å¼€å§‹æå– {required_sft_count} ä¸ªSFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒï¼ˆè®°å¿†æ¡ç›®æ•°: {num_memory_entries}ï¼‰")

            # åŠ è½½SFTæ•°æ®é›†
            sft_samples = self._load_sft_dataset()
            if not sft_samples:
                _log.warning("âš ï¸ æ— æ³•åŠ è½½SFTæ•°æ®é›†ï¼Œè·³è¿‡SFTå‘é‡æå–")
                return None

            # éšæœºæŠ½å–1.5å€æ•°é‡çš„SFTæ ·æœ¬
            if len(sft_samples) >= required_sft_count:
                selected_samples = random.sample(sft_samples, required_sft_count)
            else:
                _log.warning(f"âš ï¸ SFTæ•°æ®é›†æ ·æœ¬æ•° {len(sft_samples)} å°‘äºæ‰€éœ€æ•°é‡ {required_sft_count}ï¼Œä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
                selected_samples = sft_samples

            # æå–æ€è€ƒéƒ¨åˆ†å†…å®¹ï¼ˆä½¿ç”¨ç¬¬äºŒæ­¥éªŒè¯è¿‡çš„æ–¹æ³•ï¼‰
            sft_thinking_texts = []
            for sample in selected_samples:
                # ä½¿ç”¨ç¬¬äºŒæ­¥ç›¸åŒçš„æ ‡å‡†åŒ–æ–¹æ³•å¤„ç†SFTæ•°æ®
                messages = self._standardize_sft_messages(sample)
                if messages:
                    # ä½¿ç”¨processorå°†messagesè½¬æ¢ä¸ºå®Œæ•´æ–‡æœ¬ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ¶ˆæ¯ï¼‰
                    try:
                        # ä½¿ç”¨apply_chat_templateè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                        full_text = processor.apply_chat_template(
                            messages,
                            tokenize=False,
                            add_generation_prompt=False
                        )

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ€è€ƒéƒ¨åˆ†
                        start_tag = "<think>"
                        end_tag = "</think>"
                        start_idx = full_text.find(start_tag)
                        end_idx = full_text.find(end_tag)

                        if start_idx != -1 and end_idx != -1:
                            # æå–æ€è€ƒéƒ¨åˆ†çš„å†…å®¹
                            thinking_content = full_text[start_idx + len(start_tag):end_idx].strip()
                            if thinking_content:
                                sft_thinking_texts.append(thinking_content)
                    except Exception as e:
                        _log.debug(f"å¤„ç†SFTæ ·æœ¬å¤±è´¥: {e}")
                        continue

            if not sft_thinking_texts:
                _log.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„SFTæ€è€ƒå†…å®¹ï¼Œè·³è¿‡SFTå‘é‡æå–")
                return None

            _log.info(f"âœ… æå–åˆ° {len(sft_thinking_texts)} ä¸ªSFTæ€è€ƒå†…å®¹")

            # ä½¿ç”¨_batch_extract_embeddingsæå–SFTå‘é‡
            sft_texts, sft_embeddings = self._batch_extract_embeddings(
                sft_thinking_texts, model, processor, max_tokens=35000
            )

            if not sft_embeddings:
                _log.warning("âš ï¸ SFTå‘é‡æå–å¤±è´¥")
                return None

            # ä¿å­˜SFTå‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶
            sft_vectors_path = os.path.join(self.memory_db_dir, "temp_sft_vectors.pt")
            torch.save({
                "texts": sft_texts,
                "embeddings": torch.stack(sft_embeddings)
            }, sft_vectors_path)

            _log.info(f"âœ… å·²ä¿å­˜ {len(sft_embeddings)} ä¸ªSFTå‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶: {sft_vectors_path}")
            return sft_vectors_path

        except Exception as e:
            _log.error(f"âŒ SFTå‘é‡æå–å¤±è´¥: {e}", exc_info=True)
            return None

    def _load_memory_texts_from_file(self, file_path: str) -> List[str]:
        """
        ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½æ‰€æœ‰è®°å¿†æ¡ç›®æ–‡æœ¬
        
        Args:
            file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            
        Returns:
            è®°å¿†æ¡ç›®æ–‡æœ¬åˆ—è¡¨
        """
        try:
            if not os.path.exists(file_path):
                return []
            
            data = torch.load(file_path, map_location='cpu')
            texts = data.get('texts', [])
            return texts
        except Exception as e:
            _log.error(f"ä»æ–‡ä»¶åŠ è½½è®°å¿†æ¡ç›®æ–‡æœ¬å¤±è´¥: {e}")
            return []

    def _save_training_data_batch(self, texts: List[str], embeddings: List[torch.Tensor]):
        """
        åˆ†æ‰¹ä¿å­˜è®­ç»ƒæ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰

        Args:
            texts: è®°å¿†æ–‡æœ¬åˆ—è¡¨
            embeddings: å¯¹åº”çš„å‘é‡åˆ—è¡¨
        """
        temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")

        if not texts:
            return

        try:
            # åˆå¹¶embeddingsä¸ºå¼ é‡
            embeddings_tensor = torch.stack(embeddings)

            if os.path.exists(temp_data_path):
                # åŠ è½½ç°æœ‰æ•°æ®å¹¶è¿½åŠ 
                existing_data = torch.load(temp_data_path, map_location='cpu')
                existing_texts = existing_data.get('texts', [])
                existing_embeddings = existing_data.get('embeddings')

                # è¿½åŠ æ•°æ®
                all_texts = existing_texts + texts
                all_embeddings = torch.cat([existing_embeddings, embeddings_tensor], dim=0)
            else:
                # åˆ›å»ºæ–°æ–‡ä»¶
                all_texts = texts
                all_embeddings = embeddings_tensor

            # ä¿å­˜åˆ°æ–‡ä»¶
            torch.save({
                "texts": all_texts,
                "embeddings": all_embeddings
            }, temp_data_path)

            _log.info(f"ä¿å­˜äº† {len(texts)} ä¸ªæ¡ç›®çš„è®­ç»ƒæ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆæ€»è®¡ {len(all_texts)} ä¸ªæ¡ç›®ï¼‰")

        except Exception as e:
            _log.error(f"ä¿å­˜è®­ç»ƒæ•°æ®æ‰¹æ¬¡å¤±è´¥: {e}")
            raise

    def save_memory_embeddings_from_file(self, training_data_path: str):
        """
        ä»è®­ç»ƒæ•°æ®æ–‡ä»¶è¯»å–ç›‘ç£å‘é‡å¹¶ä¿å­˜åˆ°MemoryVectorDB

        Args:
            training_data_path: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        _log.info("ä»è®­ç»ƒæ•°æ®æ–‡ä»¶ä¿å­˜ç›‘ç£å‘é‡åˆ°MemoryVectorDB...")

        try:
            # åŠ è½½è®­ç»ƒæ•°æ®
            training_data = torch.load(training_data_path, map_location='cpu')
            embeddings = training_data.get('embeddings')

            if embeddings is None or len(embeddings) == 0:
                _log.warning("âš ï¸ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸­æ²¡æœ‰å‘é‡æ•°æ®")
                return

            # è®°å¿†æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            memory_db_path = os.path.join(self.memory_db_dir, "memory_embeddings.pt")

            # åˆ›å»ºMemoryVectorDBå¹¶åŠ è½½ç°æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            embedding_dim = embeddings.shape[-1]
            storage_device = "cpu"
            memory_db = MemoryVectorDB(embedding_dim=embedding_dim, device=storage_device)
            _log.info(f"MemoryVectorDBå°†åœ¨ {storage_device} ä¸Šæ‰§è¡Œä¿å­˜æ“ä½œï¼Œä»¥é¿å…GPUè®¾å¤‡ç¼–å·ä¸ä¸€è‡´é—®é¢˜")

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåŠ è½½ç°æœ‰æ•°æ®
            if os.path.exists(memory_db_path):
                try:
                    memory_db.load_from_pt(memory_db_path)
                    _log.info(f"åŠ è½½ç°æœ‰è®°å¿†æ•°æ®åº“ï¼Œå·²æœ‰ {memory_db.embeddings.shape[0]} ä¸ªå‘é‡")
                except Exception as e:
                    _log.warning(f"åŠ è½½ç°æœ‰è®°å¿†æ•°æ®åº“å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“")

            # è¿½åŠ æ–°çš„å‘é‡
            memory_db.add_vectors(embeddings)

            # ä¿å­˜åˆ°æ–‡ä»¶
            memory_db.save_to_pt(memory_db_path)

            _log.info(f"âœ… æˆåŠŸä¿å­˜ {len(embeddings)} ä¸ªæ–°çš„ç›‘ç£å‘é‡åˆ° {memory_db_path}ï¼ˆæ€»è®¡ {memory_db.embeddings.shape[0]} ä¸ªå‘é‡ï¼‰")

        except Exception as e:
            _log.error(f"ä»æ–‡ä»¶ä¿å­˜è®°å¿†å‘é‡å¤±è´¥: {e}")
            raise

    def _parse_memory_entries(self, generated_text: str) -> List[str]:
        """
        è§£ææ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¤šä¸ªç‹¬ç«‹çš„è®°å¿†æ¡ç›®
        
        Args:
            generated_text: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªè®°å¿†æ¡ç›®
        
        Returns:
            è®°å¿†æ¡ç›®æ–‡æœ¬åˆ—è¡¨
        """
        if not generated_text or not generated_text.strip():
            return []
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ— è®°å¿†æ¡ç›®
        if "æ— è®°å¿†æ¡ç›®" in generated_text or "æ— " in generated_text[:20]:
            return []
        
        entries = []
        
        # æ–¹æ³•1: æŒ‰è¡Œè§£æï¼ŒæŸ¥æ‰¾"æ¡ç›®X:"æ ¼å¼
        lines = generated_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # åŒ¹é…"æ¡ç›®1:", "æ¡ç›®2:", "1.", "2."ç­‰æ ¼å¼
            import re
            # åŒ¹é… "æ¡ç›®1:", "1.", "-" ç­‰å¼€å¤´
            match = re.match(r'^(?:æ¡ç›®\s*\d+|[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]|[-*])\s*[:ï¼š]?\s*(.+)', line)
            if match:
                entry_text = match.group(1).strip()
                if entry_text and len(entry_text) > 3:  # è‡³å°‘3ä¸ªå­—ç¬¦
                    entries.append(entry_text)
            elif line and not line.startswith('æ¡ç›®') and len(line) > 10:
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ¼å¼ï¼Œä½†å†…å®¹è¾ƒé•¿ï¼Œä¹Ÿå¯èƒ½æ˜¯è®°å¿†æ¡ç›®
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„å…³é”®è¯ï¼ˆå¦‚"å–œæ¬¢"ã€"æ˜¯"ã€"åœ¨"ç­‰ï¼‰
                if any(keyword in line for keyword in ['å–œæ¬¢', 'æ˜¯', 'åœ¨', 'æœ‰', 'çš„', 'äº†', 'ä¼š', 'è¦', 'å»', 'æ¥']):
                    entries.append(line)
        
        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ¼å¼åŒ–çš„æ¡ç›®ï¼Œå°è¯•æŒ‰å¥å·ã€æ¢è¡Œç­‰åˆ†å‰²
        if not entries:
            # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', generated_text)
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 5:  # è‡³å°‘5ä¸ªå­—ç¬¦
                    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯è®°å¿†æ¡ç›®çš„å†…å®¹ï¼ˆå¦‚"è¯·åˆ†æ"ã€"å¯¹è¯å†…å®¹"ç­‰ï¼‰
                    if not any(keyword in sentence for keyword in ['è¯·', 'åˆ†æ', 'å¯¹è¯', 'å†…å®¹', 'è®°å¿†æ¡ç›®', 'ä»¥ä¸‹']):
                        entries.append(sentence)
        
        # å»é‡å¹¶è¿‡æ»¤
        seen = set()
        unique_entries = []
        for entry in entries:
            # å½’ä¸€åŒ–ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
            normalized = ' '.join(entry.split())
            if normalized not in seen and len(normalized) > 3:
                seen.add(normalized)
                unique_entries.append(normalized)
        
        return unique_entries
    
    def save_memory_embeddings(self, memory_entries: List[Tuple[str, torch.Tensor]]):
        """
        ä¿å­˜ç›‘ç£å‘é‡åˆ°MemoryVectorDBï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        
        Args:
            memory_entries: (è®°å¿†æ–‡æœ¬, ç›‘ç£å‘é‡) çš„åˆ—è¡¨
        """
        _log.info("ä¿å­˜ç›‘ç£å‘é‡åˆ°MemoryVectorDB...")
        
        # æå–æ‰€æœ‰ç›‘ç£å‘é‡
        embeddings = torch.stack([entry[1] for entry in memory_entries])
        
        # è®°å¿†æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        memory_db_path = os.path.join(self.memory_db_dir, "memory_embeddings.pt")
        
        # åˆ›å»ºMemoryVectorDBå¹¶åŠ è½½ç°æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # æ³¨æ„ï¼šMemoryVectorDBä¸»è¦ç”¨äºå­˜å‚¨ï¼Œåº”è¯¥ä½¿ç”¨CPUä»¥é¿å…GPUè®¾å¤‡é—®é¢˜
        embedding_dim = embeddings.shape[-1]
        storage_device = "cpu"
        memory_db = MemoryVectorDB(embedding_dim=embedding_dim, device=storage_device)
        _log.debug(f"MemoryVectorDBå°†åœ¨ {storage_device} ä¸Šæ‰§è¡Œä¿å­˜æ“ä½œ")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåŠ è½½ç°æœ‰æ•°æ®
        if os.path.exists(memory_db_path):
            try:
                memory_db.load_from_pt(memory_db_path)
                _log.info(f"åŠ è½½ç°æœ‰è®°å¿†æ•°æ®åº“ï¼Œå·²æœ‰ {memory_db.embeddings.shape[0]} ä¸ªå‘é‡")
            except Exception as e:
                _log.warning(f"åŠ è½½ç°æœ‰è®°å¿†æ•°æ®åº“å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“")
        
        # è¿½åŠ æ–°çš„å‘é‡
        memory_db.add_vectors(embeddings)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        memory_db.save_to_pt(memory_db_path)

        _log.info(f"âœ… æˆåŠŸä¿å­˜ {len(memory_entries)} ä¸ªæ–°çš„ç›‘ç£å‘é‡åˆ° {memory_db_path}ï¼ˆæ€»è®¡ {memory_db.embeddings.shape[0]} ä¸ªå‘é‡ï¼‰")

        # æ³¨æ„ï¼šmemory_entriesæš‚æ—¶ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œç”¨äºåç»­è®­ç»ƒ
        # è®­ç»ƒå®Œæˆååœ¨cleanup_after_trainingä¸­ç»Ÿä¸€æ¸…ç†

    def load_training_model(self):
        """åŠ è½½ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹ï¼ˆç”¨äºè®°å¿†æå–å’Œè®­ç»ƒï¼‰"""
        _log.info(f"åŠ è½½è®­ç»ƒæ¨¡å‹: {self.base_model_path}")

        # ä½¿ç”¨ä¸initialize_modelç›¸åŒçš„åŠ è½½é€»è¾‘
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        model_path = self.base_model_path
        if not os.path.isabs(model_path):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            model_path = os.path.abspath(os.path.join(project_root, model_path))

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        is_local_path = os.path.exists(model_path) and os.path.isdir(model_path)

        try:
            # åŠ è½½processorï¼ˆä½¿ç”¨AutoProcessorè€Œä¸æ˜¯AutoTokenizerï¼Œå› ä¸ºéœ€è¦å¤„ç†å›¾ç‰‡å’Œè§†é¢‘ï¼‰
            # æ­£å¸¸æ¨ç†æ—¶ä½¿ç”¨AutoProcessorï¼Œè®­ç»ƒæ—¶ä¹Ÿåº”è¯¥ä½¿ç”¨AutoProcessor
            processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=is_local_path
            )

            # å‡†å¤‡åŠ è½½å‚æ•°
            load_kwargs = {
                "torch_dtype": torch.bfloat16,
                "trust_remote_code": True,
                "local_files_only": is_local_path
            }
            
            # æ ¹æ®è®¾å¤‡é…ç½®å†³å®šdevice_mapï¼ˆä½¿ç”¨ä¸TrainingModelContextç›¸åŒçš„é€»è¾‘ï¼‰
            multi_gpu_config = self.config.get("model", {}).get("multi_gpu", {})
            multi_gpu_enabled = multi_gpu_config.get("enabled", False)
            
            if isinstance(self.device, list) and multi_gpu_enabled:
                # å¤šGPUé…ç½®ï¼šä½¿ç”¨ä¼˜åŒ–çš„åˆ†é…ç­–ç•¥
                cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
                cuda_visible_set = bool(cuda_visible)
                max_memory_config = multi_gpu_config.get("max_memory", {})
                allocation = _optimize_multi_gpu_allocation(self.device, max_memory_config, cuda_visible_set=cuda_visible_set)
                load_kwargs["device_map"] = allocation["device_map"]
                if allocation["max_memory"]:
                    load_kwargs["max_memory"] = allocation["max_memory"]
                _log.info(f"ğŸ”§ è®­ç»ƒæ¨¡å‹: æŒ‡å®šè®¾å¤‡{self.device}ï¼Œä½¿ç”¨ä¼˜åŒ–çš„åˆ†é…ç­–ç•¥")
            elif isinstance(self.device, str) and self.device.startswith("cuda"):
                load_kwargs["device_map"] = {"": self.device}
                _log.info(f"ğŸ”§ è®­ç»ƒæ¨¡å‹: å•GPUæ¨¡å¼ï¼Œè®¾å¤‡æ˜ å°„åˆ° {self.device}")
            else:
                load_kwargs["device_map"] = "auto"
                _log.info("ğŸ”§ è®­ç»ƒæ¨¡å‹: ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")

            # åŠ è½½æ¨¡å‹ - æ³¨æ„è¿™é‡Œä½¿ç”¨Qwen3VLForConditionalGenerationï¼Œä¸æ˜¯AutoModelForCausalLM
            from transformers import Qwen3VLForConditionalGeneration
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_path,
                **load_kwargs
            )

            # æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
            # ä½¿ç”¨MemoryTokenManagerï¼Œä¸æ­£å¸¸æ¨ç†æ—¶ä¿æŒä¸€è‡´
            from memory.token_manager import MemoryTokenManager
            token_manager = MemoryTokenManager(model, processor.tokenizer)
            recall_token_ids = token_manager.check_and_add_tokens(perturbation_std=0.02)
            _log.info(f"âœ… ç‰¹æ®Štokenå¤„ç†å®Œæˆ: {recall_token_ids}")

            _log.info("âœ… è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, processor

        except Exception as e:
            _log.error(f"âŒ åŠ è½½è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def train_recall_token(self, training_data_path: str, model=None, processor=None, sft_vectors_path: Optional[str] = None) -> str:
        """
        ç¬¬ä¸€æ­¥è®­ç»ƒï¼šè®­ç»ƒ<recall> tokençš„embedding

        Args:
            training_data_path: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            è®­ç»ƒåçš„æ¨¡å‹è·¯å¾„
        """
        # å°è¯•ç¡®ä¿è®­ç»ƒæ¨¡å—å·²åŠ è½½
        if not _ensure_training_modules_loaded():
            raise ImportError("è®­ç»ƒæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œè®­ç»ƒã€‚è¯·æ£€æŸ¥ recall/ ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„è®­ç»ƒè„šæœ¬ã€‚")

        _log.info("å¼€å§‹ç¬¬ä¸€æ­¥è®­ç»ƒï¼š<recall> token embeddingè®­ç»ƒ...")

        trainer = None
        try:
            # ä»æ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®
            training_data = torch.load(training_data_path, map_location='cpu')
            texts = training_data.get('texts', [])
            embeddings = training_data.get('embeddings')

            if not texts or embeddings is None:
                raise ValueError("è®­ç»ƒæ•°æ®æ–‡ä»¶æ— æ•ˆæˆ–ä¸ºç©º")

            # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ å…¥é¢„åŠ è½½çš„æ¨¡å‹ï¼‰
            lora_r = self.lora_config.get("r", 8)
            lora_alpha = self.lora_config.get("lora_alpha", 32)
            lora_dropout = self.lora_config.get("lora_dropout", 0.1)
            # è·å–ç¬¬ä¸€æ­¥è®­ç»ƒçš„LoRAç›®æ ‡æ¨¡å—ï¼ˆå¦‚æœé…ç½®äº†ï¼Œåªä½¿ç”¨Qå’ŒVä»¥å‡å°‘æ˜¾å­˜ï¼‰
            step1_lora_target_modules = self.lora_config.get("step1_lora_target_modules", None)
            # è·å–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            gradient_accumulation_steps = self.config.get("model", {}).get("multi_gpu", {}).get("gradient_accumulation_steps", 1)
            # è·å–max_memoryé…ç½®
            max_memory = self.config.get("model", {}).get("multi_gpu", {}).get("max_memory")

            # è·å–ç¬¬ä¸€æ­¥è®­ç»ƒçš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            max_length_recall_training = self.config.get("model", {}).get("training", {}).get("training_config", {}).get("max_length_recall_training")
            if max_length_recall_training is None:
                max_length_recall_training = None  # æ˜ç¡®è®¾ç½®ä¸ºNone

            trainer = RecallMemoryTrainer(
                self.base_model_path,
                device=self.device,
                lora_r=lora_r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                original_device=self.original_device,
                preloaded_model=model,
                preloaded_tokenizer=processor,
                gradient_accumulation_steps=gradient_accumulation_steps,
                max_memory=max_memory,
                max_length=max_length_recall_training,
                lora_target_modules=step1_lora_target_modules
                # ç¬¬ä¸€æ­¥è®­ç»ƒä¸è®¾ç½®epoch_end_hookï¼Œä¸æ’å…¥SFT
            )

            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼šåˆ†åˆ«åŠ è½½è®°å¿†æ¡ç›®å’ŒSFTå‘é‡
            memory_data = None
            sft_data = None

            # åŠ è½½è®°å¿†æ¡ç›®æ•°æ®
            if os.path.exists(training_data_path):
                memory_data = torch.load(training_data_path, map_location='cpu')
                memory_count = len(memory_data.get('texts', []))
                _log.info(f"ğŸ“– åŠ è½½è®°å¿†æ¡ç›®æ•°æ®: {memory_count} æ¡")

            # åŠ è½½SFTå‘é‡æ•°æ®
            if sft_vectors_path and os.path.exists(sft_vectors_path):
                sft_data = torch.load(sft_vectors_path, map_location='cpu')
                sft_count = len(sft_data.get('texts', []))
                _log.info(f"ğŸ“– åŠ è½½SFTå‘é‡æ•°æ®: {sft_count} æ¡")

            # åˆ›å»ºè®­ç»ƒæ•°æ®ï¼šè®°å¿†æ¡ç›® + éšæœºæŠ½å–çš„SFTå‘é‡
            if memory_data and sft_data:
                memory_texts = memory_data.get('texts', [])
                memory_embeddings = memory_data.get('embeddings', torch.empty(0))
                sft_texts = sft_data.get('texts', [])
                sft_embeddings = sft_data.get('embeddings', torch.empty(0))

                memory_count = len(memory_texts)
                sft_total_count = len(sft_texts)

                # è®¡ç®—éœ€è¦çš„SFTå‘é‡æ•°é‡ï¼š1.5å€äºè®°å¿†æ¡ç›®æ•°é‡
                required_sft_count = int(memory_count * 1.5)
                actual_sft_count = min(required_sft_count, sft_total_count)

                # éšæœºæŠ½å–SFTå‘é‡
                if actual_sft_count < sft_total_count:
                    import random
                    random.seed(42)  # ç¡®ä¿å¯é‡ç°
                    selected_indices = random.sample(range(sft_total_count), actual_sft_count)
                    selected_sft_texts = [sft_texts[i] for i in selected_indices]
                    selected_sft_embeddings = sft_embeddings[selected_indices]
                else:
                    selected_sft_texts = sft_texts
                    selected_sft_embeddings = sft_embeddings

                # åˆå¹¶æ•°æ®
                combined_texts = memory_texts + selected_sft_texts
                combined_embeddings = torch.cat([memory_embeddings, selected_sft_embeddings], dim=0)

                # åˆ›å»ºåŒ…å«å…ƒä¿¡æ¯çš„è®­ç»ƒæ•°æ®
                training_data = {
                    'texts': combined_texts,
                    'embeddings': combined_embeddings,
                    'memory_count': memory_count,  # è®°å¿†æ¡ç›®æ•°é‡
                    'sft_count': actual_sft_count  # SFTå‘é‡æ•°é‡
                }

                temp_data_path = os.path.join(self.memory_db_dir, "temp_recall_training_data.pt")
                torch.save(training_data, temp_data_path)
                _log.info(f"âœ… å·²å‡†å¤‡è®­ç»ƒæ•°æ®: {memory_count} æ¡è®°å¿†æ¡ç›® + {actual_sft_count} æ¡SFTå‘é‡")

                # åˆ é™¤SFTå‘é‡æ–‡ä»¶
                try:
                    os.remove(sft_vectors_path)
                    _log.info("ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶SFTå‘é‡æ–‡ä»¶")
                except Exception as e:
                    _log.warning(f"âš ï¸ åˆ é™¤SFTå‘é‡æ–‡ä»¶å¤±è´¥: {e}")

            elif memory_data:
                # åªæœ‰è®°å¿†æ¡ç›®æ•°æ®
                temp_data_path = training_data_path
                _log.info("â„¹ï¸ åªæœ‰è®°å¿†æ¡ç›®æ•°æ®ï¼Œå°†ç›´æ¥ä½¿ç”¨")
            else:
                raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")

            # è®­ç»ƒ
            embedding_epochs = self.training_config.get("embedding_epochs", 10)
            batch_size = self.training_config.get("batch_size", 4)
            learning_rate = float(self.training_config.get("learning_rate", 1e-4))

            step1_save_path = os.path.join(self.trained_model_dir, "step1_recall_token_trained")
            self._prepare_output_dir(step1_save_path)

            # Step1 åªè®­ç»ƒç‰¹æ®Štokenï¼Œæ­¤é˜¶æ®µä¸æ’å…¥SFT
            self._current_epoch_sample_n = None
            res = trainer.train(
                pt_file_path=temp_data_path,
                num_epochs=embedding_epochs,
                batch_size=batch_size,
                learning_rate=learning_rate,
                save_path=step1_save_path
            )
            _ = res

            # åˆå¹¶LoRAå¹¶ä¿å­˜
            model_path = trainer.merge_and_save_model(step1_save_path)
            if self.export_save_full_vl_assets:
                self._ensure_full_vl_assets(model_path)

            # ä¿å­˜Processoré…ç½®
            self._save_processor_to_path(model_path)

            _log.info(f"ç¬¬ä¸€æ­¥è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {model_path}")
            return model_path

        except Exception as e:
            _log.error(f"ç¬¬ä¸€æ­¥è®­ç»ƒå¤±è´¥: {e}")
            raise
        finally:
            # æ¸…ç†è®­ç»ƒå™¨åˆ›å»ºçš„æ‰€æœ‰æ¨¡å‹å®ä¾‹
            if trainer is not None:
                trainer.cleanup()
                del trainer

            # åˆ é™¤ä¸´æ—¶åˆå¹¶çš„è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                temp_merge_path = os.path.join(self.memory_db_dir, "temp_recall_training_data.pt")
                if os.path.exists(temp_merge_path):
                    os.remove(temp_merge_path)
                    _log.info("ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶åˆå¹¶çš„è®­ç»ƒæ•°æ®æ–‡ä»¶")
            except Exception as e:
                _log.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
    
    def train_memory_decoding(self, training_data_path: str, model_path: str) -> str:
        """
        ç¬¬äºŒæ­¥è®­ç»ƒï¼šè®­ç»ƒè®°å¿†è§£ç èƒ½åŠ›

        Args:
            training_data_path: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            model_path: ç¬¬ä¸€æ­¥è®­ç»ƒåçš„æ¨¡å‹è·¯å¾„

        Returns:
            è®­ç»ƒåçš„æ¨¡å‹è·¯å¾„
        """
        # å°è¯•ç¡®ä¿è®­ç»ƒæ¨¡å—å·²åŠ è½½
        if not _ensure_training_modules_loaded():
            raise ImportError("è®­ç»ƒæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œè®­ç»ƒã€‚è¯·æ£€æŸ¥ recall/ ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„è®­ç»ƒè„šæœ¬ã€‚")

        _log.info("å¼€å§‹ç¬¬äºŒæ­¥è®­ç»ƒï¼šè®°å¿†è§£ç è®­ç»ƒ...")

        trainer = None
        try:
            # ä»æ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®
            training_data = torch.load(training_data_path, map_location='cpu')
            texts = training_data.get('texts', [])
            embeddings = training_data.get('embeddings')

            if not texts or embeddings is None:
                raise ValueError("è®­ç»ƒæ•°æ®æ–‡ä»¶æ— æ•ˆæˆ–ä¸ºç©º")

            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            temp_data_path = training_data_path

            # åœ¨åˆ›å»ºè®­ç»ƒå™¨ä¹‹å‰ï¼Œå…ˆç¡®ä¿æ¨¡å‹ä¸­çš„ç‰¹æ®Štokenå­˜åœ¨
            # ä½¿ç”¨MemoryTokenManageråŠ è½½å¹¶æ£€æŸ¥tokenï¼Œç„¶åå°†å¤„ç†è¿‡çš„æ¨¡å‹ä¼ é€’ç»™è®­ç»ƒå™¨
            _log.info(f"ğŸ”§ é¢„å¤„ç†æ¨¡å‹token: {model_path}")
            preloaded_model, preloaded_processor = TrainingModelContext.load_training_model(
                model_path, self.device, self.config.get("model", {}).get("multi_gpu", {})
            )
            _log.info("âœ… æ¨¡å‹tokené¢„å¤„ç†å®Œæˆï¼Œå·²æ·»åŠ ç‰¹æ®Štoken")

            # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ å…¥é¢„å¤„ç†è¿‡çš„æ¨¡å‹å’Œtokenizerï¼‰
            lora_r = self.lora_config.get("r", 8)
            lora_alpha = self.lora_config.get("lora_alpha", 32)
            lora_dropout = self.lora_config.get("lora_dropout", 0.1)
            # è·å–ç¬¬äºŒæ­¥è®­ç»ƒçš„LoRAç›®æ ‡æ¨¡å—ï¼ˆå¦‚æœé…ç½®äº†ï¼Œä½¿ç”¨å®Œæ•´é…ç½®ï¼‰
            step2_lora_target_modules = self.lora_config.get("step2_lora_target_modules", None)
            # è·å–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            gradient_accumulation_steps = self.config.get("model", {}).get("multi_gpu", {}).get("gradient_accumulation_steps", 1)
            # è·å–max_memoryé…ç½®
            max_memory = self.config.get("model", {}).get("multi_gpu", {}).get("max_memory")

            dataset_max_length = int(self.training_config.get("memory_dataset_max_length", 3000) or 3000)
            test_sample_count = int(self.training_config.get("memory_test_sample_count", 2) or 2)
            test_max_new_tokens = int(self.training_config.get("memory_test_max_new_tokens", 300) or 300)
            test_use_cache = bool(self.training_config.get("memory_test_use_cache", False))
            activation_prompts = self.guides_config.get("activation_prompts")
            end_prompts = self.guides_config.get("end_prompts")

            trainer = EnhancedTextMemoryTrainer(
                model_path,
                device=self.device,
                lora_r=lora_r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                original_device=self.original_device,
                preloaded_model=preloaded_model,  # ä¼ å…¥é¢„å¤„ç†è¿‡çš„æ¨¡å‹
                preloaded_tokenizer=preloaded_processor,  # ä¼ å…¥é¢„å¤„ç†è¿‡çš„tokenizer
                gradient_accumulation_steps=gradient_accumulation_steps,
                max_memory=max_memory,
                generation_config=self.config.get("generation", {}),
                epoch_end_hook=(lambda ep, tr: self._run_sft_one_epoch(tr, epoch=ep, epoch_sample_n=self._current_epoch_sample_n)),
                lora_target_modules=step2_lora_target_modules,
                dataset_max_length=dataset_max_length,
                test_sample_count=test_sample_count,
                test_max_new_tokens=test_max_new_tokens,
                test_use_cache=test_use_cache,
                activation_prompts=activation_prompts,
                end_prompts=end_prompts,
            )

            # åŠ è½½SFTæ•°æ®å¹¶æå–å®Œæ•´å†…å®¹ï¼ˆæˆªæ–­ç‚¹å°†åœ¨æ€è€ƒéƒ¨åˆ†å†…éƒ¨ï¼‰
            sft_full_texts = []
            if self.sft_enabled and self.sft_path:
                try:
                    # éœ€è¦processoræ¥å°†messagesè½¬æ¢ä¸ºæ–‡æœ¬
                    from transformers import AutoProcessor
                    processor = AutoProcessor.from_pretrained(
                        model_path,
                        trust_remote_code=True,
                        local_files_only=True
                    )
                    
                    sft_samples = self._load_sft_dataset()
                    for sample in sft_samples:
                        messages = self._standardize_sft_messages(sample)
                        if messages:
                            # ä½¿ç”¨processorå°†messagesè½¬æ¢ä¸ºå®Œæ•´æ–‡æœ¬ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ¶ˆæ¯ï¼‰
                            try:
                                # ä½¿ç”¨apply_chat_templateè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                                full_text = processor.apply_chat_template(
                                    messages,
                                    tokenize=False,
                                    add_generation_prompt=False
                                )
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ€è€ƒéƒ¨åˆ†
                                start_tag = "<think>"
                                end_tag = "</think>"
                                start_idx = full_text.find(start_tag)
                                end_idx = full_text.find(end_tag)
                                
                                if start_idx != -1 and end_idx != -1:
                                    # æ‰¾åˆ°æ€è€ƒéƒ¨åˆ†ï¼Œä¿å­˜å®Œæ•´æ–‡æœ¬å’Œæ€è€ƒéƒ¨åˆ†çš„èµ·æ­¢ä½ç½®
                                    # æ³¨æ„ï¼šè¿™é‡Œä¿å­˜çš„æ˜¯å®Œæ•´æ–‡æœ¬ï¼Œæˆªæ–­ä¼šåœ¨è®­ç»ƒæ—¶è¿›è¡Œ
                                    sft_full_texts.append({
                                        "full_text": full_text,
                                        "thinking_start": start_idx,
                                        "thinking_end": end_idx + len(end_tag)
                                    })
                            except Exception as e:
                                _log.debug(f"å¤„ç†SFTæ ·æœ¬å¤±è´¥: {e}")
                                continue
                    
                    _log.info(f"âœ… ä»SFTæ•°æ®ä¸­æå–äº† {len(sft_full_texts)} æ¡å®Œæ•´æ–‡æœ¬ï¼Œæˆªæ–­ç‚¹å°†æ§åˆ¶åœ¨æ€è€ƒéƒ¨åˆ†å†…éƒ¨")
                except Exception as e:
                    _log.warning(f"âš ï¸ åŠ è½½SFTæ•°æ®å¤±è´¥ï¼Œå°†ä½¿ç”¨è®°å¿†æ¡ç›®ä½œä¸ºä¸Šä¸‹æ–‡: {e}")

            # è®­ç»ƒ
            memory_epochs = self.training_config.get("memory_epochs", 20)
            batch_size = self.training_config.get("batch_size", 4)
            learning_rate = float(self.training_config.get("learning_rate", 1e-4))

            step2_save_path = os.path.join(self.trained_model_dir, "step2_memory_decoding_trained")
            self._prepare_output_dir(step2_save_path)

            # è®¾ç½®SFTæ¯epoché‡‡æ ·å‚è€ƒæ•°ï¼ˆä¸è®°å¿†æ¡ç›®æ•°é‡ç›¸åŒï¼‰
            training_data = torch.load(temp_data_path, map_location='cpu')
            memory_texts = training_data.get('texts', [])
            self._current_epoch_sample_n = len(memory_texts)
            res2 = trainer.train(
                pt_file_path=temp_data_path,
                num_epochs=memory_epochs,
                batch_size=batch_size,
                learning_rate=learning_rate,
                noise_std=0.01,
                save_path=step2_save_path,
                sft_full_texts=sft_full_texts if sft_full_texts else None
            )
            _ = res2

            # åˆå¹¶LoRAå¹¶ä¿å­˜
            final_model_path = trainer.merge_and_save_model(step2_save_path)
            if self.export_save_full_vl_assets:
                self._ensure_full_vl_assets(final_model_path)

            # ä¿å­˜Processoré…ç½®
            self._save_processor_to_path(final_model_path)

            _log.info(f"ç¬¬äºŒæ­¥è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {final_model_path}")
            return final_model_path

        except Exception as e:
            _log.error(f"ç¬¬äºŒæ­¥è®­ç»ƒå¤±è´¥: {e}")
            raise
        finally:
            # æ¸…ç†è®­ç»ƒå™¨åˆ›å»ºçš„æ‰€æœ‰æ¨¡å‹å®ä¾‹
            if trainer is not None:
                trainer.cleanup()
                del trainer

    def cleanup_after_training(self):
        """
        è®­ç»ƒå®Œæˆåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        æ³¨æ„ï¼š
        - JSONèŠå¤©è®°å½•æ–‡ä»¶ä¼šè¢«åˆ é™¤ï¼ˆå·²ç”¨äºè®­ç»ƒï¼Œä¸å†éœ€è¦ï¼‰
        - ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶ä¼šè¢«åˆ é™¤ï¼ˆè®­ç»ƒå®Œæˆåä¸å†éœ€è¦ï¼‰
        - å†…å­˜ä¸­çš„èŠå¤©ç¼“å­˜ä¼šè¢«æ¸…ç©ºï¼ˆè®­ç»ƒå®Œæˆåä¸å†éœ€è¦ï¼‰
        - è®°å¿†å‘é‡æ•°æ®åº“ï¼ˆmemory_embeddings.ptï¼‰ä¼šè¢«ä¿ç•™ï¼ˆè¿™æ˜¯è®­ç»ƒå¥½çš„è®°å¿†ï¼Œéœ€è¦ä¿ç•™ï¼‰
        """
        _log.info("æ¸…ç†è®­ç»ƒåçš„ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜...")
        
        # 1. æ¸…ç©ºJSONèŠå¤©è®°å½•æ–‡ä»¶ï¼ˆè®­ç»ƒå®Œæˆåä¸å†éœ€è¦ï¼‰
        if os.path.exists(self.chat_history_storage_dir):
            json_files = list(Path(self.chat_history_storage_dir).glob("*.json"))
            deleted_count = 0
            for json_file in json_files:
                try:
                    os.remove(json_file)
                    deleted_count += 1
                    _log.info(f"åˆ é™¤JSONæ–‡ä»¶: {json_file.name}")
                except Exception as e:
                    _log.warning(f"åˆ é™¤JSONæ–‡ä»¶å¤±è´¥ {json_file}: {e}")
            if deleted_count > 0:
                _log.info(f"âœ… å…±åˆ é™¤ {deleted_count} ä¸ªJSONèŠå¤©è®°å½•æ–‡ä»¶")
        
        # 2. åˆ é™¤ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶
        temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
        if os.path.exists(temp_data_path):
            try:
                os.remove(temp_data_path)
                _log.info(f"âœ… åˆ é™¤ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶: temp_training_data.pt")
            except Exception as e:
                _log.warning(f"åˆ é™¤ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        
        # 3. æ¸…ç©ºå†…å­˜ä¸­çš„èŠå¤©ç¼“å­˜
        # ä½¿ç”¨å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
        import importlib
        try:
            api_server = importlib.import_module('api_server_qwen3vl')
            group_chat_histories = getattr(api_server, 'group_chat_histories', {})
            private_chat_histories = getattr(api_server, 'private_chat_histories', {})
            
            group_count = len(group_chat_histories)
            private_count = len(private_chat_histories)
            
            group_chat_histories.clear()
            private_chat_histories.clear()
            
            _log.info(f"âœ… æ¸…ç©ºå†…å­˜ä¸­çš„èŠå¤©ç¼“å­˜ï¼ˆç¾¤èŠ: {group_count}, ç§èŠ: {private_count}ï¼‰")
        except Exception as e:
            _log.warning(f"æ¸…ç©ºå†…å­˜ç¼“å­˜å¤±è´¥: {e}")
        
        # 4. è®°å¿†å‘é‡æ•°æ®åº“ï¼ˆmemory_embeddings.ptï¼‰ä¼šè¢«ä¿ç•™ï¼Œä¸åˆ é™¤
        memory_db_path = os.path.join(self.memory_db_dir, "memory_embeddings.pt")
        if os.path.exists(memory_db_path):
            _log.info(f"ğŸ“Œ è®°å¿†å‘é‡æ•°æ®åº“å·²ä¿ç•™: {memory_db_path}ï¼ˆè¿™æ˜¯è®­ç»ƒå¥½çš„è®°å¿†ï¼Œä¸ä¼šè¢«åˆ é™¤ï¼‰")
        
        _log.info("âœ… è®­ç»ƒåçš„æ¸…ç†å®Œæˆ")

__all__ = ["MemoryTrainingService", "TrainingModelContext"]
